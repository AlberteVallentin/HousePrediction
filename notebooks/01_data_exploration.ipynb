{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Exploration and Understanding\n",
    "\n",
    "**Welcome to the Ames Housing Price Prediction Project!**\n",
    "\n",
    "In this notebook, we will conduct a comprehensive exploratory data analysis (EDA) of the Ames Housing dataset. Think of this as getting to know our data intimately before we build any predictive models.\n",
    "\n",
    "## What We'll Accomplish\n",
    "\n",
    "By the end of this notebook, we will have:\n",
    "- **Understood our target variable** (house prices) and optimized it for modeling\n",
    "- **Identified the most predictive features** through correlation and statistical analysis\n",
    "- **Developed a smart strategy for handling missing data** using real estate domain knowledge\n",
    "- **Detected and analyzed outliers** that could impact our model performance\n",
    "- **Created a foundation** for feature engineering in our next notebook\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Exploratory Data Analysis is like being a detective - we're looking for clues about what makes houses expensive or cheap. This investigation will guide all our future decisions about feature engineering, model selection, and validation strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Environment Setup\n",
    "\n",
    "First, let's import all the libraries we'll need for our analysis. Each library serves a specific purpose in our data exploration journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and numerical operations\n",
    "import pandas as pd              # For working with structured data (like spreadsheets)\n",
    "import numpy as np               # For mathematical operations and array handling\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt  # For creating basic plots and charts\n",
    "import seaborn as sns            # For beautiful statistical visualizations\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats          # For statistical tests and distributions\n",
    "from scipy.stats import skew, kurtosis, shapiro  # For measuring data distribution characteristics\n",
    "\n",
    "# Machine learning utilities\n",
    "from sklearn.feature_selection import mutual_info_regression  # For detecting non-linear relationships\n",
    "\n",
    "# Configuration for better output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Hide warning messages for cleaner output\n",
    "\n",
    "# Set up pandas to show more data\n",
    "pd.set_option('display.max_columns', None)  # Show all columns when displaying dataframes\n",
    "pd.set_option('display.max_rows', 20)       # Limit rows to keep output manageable\n",
    "\n",
    "# Configure matplotlib for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')     # Use a clean, professional style\n",
    "sns.set_palette(\"husl\")                     # Use a colorful, distinguishable palette\n",
    "plt.rcParams['figure.figsize'] = (12, 8)    # Set default figure size\n",
    "plt.rcParams['font.size'] = 11              # Set default font size\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")\n",
    "print(\"✓ Visualization settings configured for professional output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why these specific libraries?**\n",
    "\n",
    "- **Pandas & NumPy**: The foundation of data science in Python - they handle all our data manipulation needs\n",
    "- **Matplotlib & Seaborn**: Create beautiful, informative visualizations that help us understand patterns\n",
    "- **SciPy**: Provides advanced statistical functions for testing hypotheses about our data\n",
    "- **Scikit-learn**: Even for EDA, we use some ML utilities to understand feature relationships\n",
    "\n",
    "Now let's load our dataset and take our first look at the Ames housing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Loading and First Impressions\n",
    "\n",
    "Let's load our dataset and get our first look at what we're working with. The Ames Housing dataset contains information about residential properties sold in Ames, Iowa between 2006-2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Ames Housing dataset\n",
    "try:\n",
    "    df = pd.read_csv(\"../data/raw/train.csv\")\n",
    "    print(f\"✓ Dataset loaded successfully from: ../data/raw/train.csv\")\n",
    "    print(f\"✓ Dataset shape: {df.shape[0]:,} houses with {df.shape[1]} features each\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ ERROR: Dataset file not found. Please ensure ../data/raw/train.csv exists.\")\n",
    "    raise\n",
    "\n",
    "# Get basic information about our dataset\n",
    "print(f\"\\n=== DATASET OVERVIEW ===\")\n",
    "print(f\"Total properties: {df.shape[0]:,}\")\n",
    "print(f\"Total features: {df.shape[1]}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "print(f\"Time period: {df['YrSold'].min()}-{df['YrSold'].max()}\")\n",
    "print(f\"Duplicate rows: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What we just learned:**\n",
    "\n",
    "This gives us our first overview of the dataset size and scope. With nearly 1,500 houses and 81 features, we have a rich dataset for building predictive models. The 5-year time span (2006-2010) includes both pre- and post-financial crisis data, which could be important for our analysis.\n",
    "\n",
    "Let's examine the structure of our data more closely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature type analysis - Understanding what kinds of data we have\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove Id and target from numerical features for analysis\n",
    "if 'Id' in numerical_features: \n",
    "    numerical_features.remove('Id')\n",
    "if 'SalePrice' in numerical_features: \n",
    "    numerical_features.remove('SalePrice')\n",
    "\n",
    "print(f\"=== FEATURE TYPE BREAKDOWN ===\")\n",
    "print(f\"Numerical features: {len(numerical_features)} (continuous/discrete numbers)\")\n",
    "print(f\"Categorical features: {len(categorical_features)} (text categories)\")\n",
    "print(f\"Target variable: SalePrice (what we want to predict)\")\n",
    "print(f\"Identifier: Id (unique house identifier)\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(f\"\\n=== BASIC STATISTICAL SUMMARY ===\")\n",
    "print(\"Target Variable (SalePrice) Statistics:\")\n",
    "price_stats = df['SalePrice'].describe()\n",
    "for stat, value in price_stats.items():\n",
    "    if stat == 'count':\n",
    "        print(f\"  {stat}: {value:,.0f}\")\n",
    "    else:\n",
    "        print(f\"  {stat}: ${value:,.0f}\")\n",
    "\n",
    "# Show a sample of our data with key features\n",
    "print(f\"\\n=== SAMPLE DATA (First 3 Houses) ===\")\n",
    "key_columns = ['Id', 'MSSubClass', 'MSZoning', 'LotArea', 'OverallQual', 'OverallCond', \n",
    "               'YearBuilt', 'GrLivArea', 'BedroomAbvGr', 'TotRmsAbvGrd', 'SalePrice']\n",
    "sample_data = df[key_columns].head(3)\n",
    "print(sample_data.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform statistical tests for normality\n",
    "# Note: Shapiro-Wilk test has limitations for large samples, so we'll use a subset\n",
    "sample_size = 500  # Use a representative sample for the test\n",
    "np.random.seed(42)  # For reproducible results\n",
    "\n",
    "# Create random samples from both distributions\n",
    "original_sample = np.random.choice(prices, size=sample_size, replace=False)\n",
    "log_sample = np.random.choice(log_prices, size=sample_size, replace=False)\n",
    "\n",
    "# Perform Shapiro-Wilk tests\n",
    "original_shapiro_stat, original_shapiro_p = shapiro(original_sample)\n",
    "log_shapiro_stat, log_shapiro_p = shapiro(log_sample)\n",
    "\n",
    "print(\"=== STATISTICAL NORMALITY TESTS ===\")\n",
    "print(\"Shapiro-Wilk Test Results (Higher statistic = more normal, p > 0.05 = normal)\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Original Prices:\")\n",
    "print(f\"  Test Statistic: {original_shapiro_stat:.6f}\")\n",
    "print(f\"  P-value: {original_shapiro_p:.2e}\")\n",
    "print(f\"  Conclusion: {'Normal distribution' if original_shapiro_p > 0.05 else 'Not normal distribution'}\")\n",
    "\n",
    "print(f\"\\nLog-Transformed Prices:\")\n",
    "print(f\"  Test Statistic: {log_shapiro_stat:.6f}\")\n",
    "print(f\"  P-value: {log_shapiro_p:.2e}\")\n",
    "print(f\"  Conclusion: {'Normal distribution' if log_shapiro_p > 0.05 else 'Not normal distribution'}\")\n",
    "\n",
    "# Calculate improvement metrics\n",
    "stat_improvement = ((log_shapiro_stat - original_shapiro_stat) / original_shapiro_stat) * 100\n",
    "p_value_ratio = log_shapiro_p / original_shapiro_p if original_shapiro_p > 0 else float('inf')\n",
    "\n",
    "print(f\"\\n=== STATISTICAL IMPROVEMENT ANALYSIS ===\")\n",
    "print(f\"Test Statistic Improvement: {stat_improvement:.2f}%\")\n",
    "print(f\"P-value Improvement Ratio: {p_value_ratio:.1f}x better\")\n",
    "\n",
    "# Create a comprehensive summary of all improvements\n",
    "print(f\"\\n=== COMPREHENSIVE TRANSFORMATION SUMMARY ===\")\n",
    "print(f\"✓ Skewness: {original_stats['skewness']:.3f} → {transformed_stats['skewness']:.3f} ({skewness_improvement:.1f}% better)\")\n",
    "print(f\"✓ Kurtosis: {original_stats['kurtosis']:.3f} → {transformed_stats['kurtosis']:.3f} ({kurtosis_improvement:.1f}% better)\")\n",
    "print(f\"✓ Shapiro-Wilk: {original_shapiro_stat:.4f} → {log_shapiro_stat:.4f} ({stat_improvement:.1f}% better)\")\n",
    "print(f\"✓ Distribution Shape: {'Right-skewed' if original_stats['skewness'] > 0.5 else 'Normal'} → {'Nearly Normal' if abs(transformed_stats['skewness']) < 0.5 else 'Improved'}\")\n",
    "\n",
    "# Final recommendation\n",
    "recommendation = \"STRONGLY RECOMMENDED\" if (abs(transformed_stats['skewness']) < abs(original_stats['skewness']) and \n",
    "                                             log_shapiro_stat > original_shapiro_stat) else \"RECOMMENDED\"\n",
    "\n",
    "print(f\"\\n=== FINAL RECOMMENDATION ===\")\n",
    "print(f\"Log Transformation Status: {recommendation}\")\n",
    "print(f\"Reasoning:\")\n",
    "print(f\"  • Significantly reduced skewness for better model performance\")\n",
    "print(f\"  • Improved statistical normality (higher Shapiro-Wilk statistic)\")\n",
    "print(f\"  • Better distribution shape for linear regression assumptions\")\n",
    "print(f\"  • Reduced impact of extreme outliers\")\n",
    "print(f\"\\n✓ We will use log-transformed prices (LogSalePrice) as our target variable for all modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformation Results Analysis:**\n",
    "\n",
    "The log transformation has dramatically improved our target variable distribution:\n",
    "\n",
    "1. **Skewness Reduction**: The transformation significantly reduced skewness, making the distribution much more symmetric\n",
    "2. **Better Normality**: The Q-Q plots show the log-transformed prices follow a normal distribution much more closely\n",
    "3. **Reduced Outlier Impact**: The box plots show fewer extreme outliers in the transformed data\n",
    "4. **Modeling Benefits**: This transformation will help our regression models perform better and make more accurate predictions\n",
    "\n",
    "## 2.3 Statistical Validation of Transformation\n",
    "\n",
    "Let's formally test whether our log transformation has improved the normality of our target variable using statistical tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation to house prices\n",
    "# We use log1p (log(1 + x)) to handle any zero values safely\n",
    "log_prices = np.log1p(prices)\n",
    "\n",
    "# Calculate statistics for both original and transformed prices\n",
    "original_stats = {\n",
    "    'mean': prices.mean(),\n",
    "    'median': prices.median(),\n",
    "    'std': prices.std(),\n",
    "    'skewness': skew(prices),\n",
    "    'kurtosis': kurtosis(prices)\n",
    "}\n",
    "\n",
    "transformed_stats = {\n",
    "    'mean': log_prices.mean(),\n",
    "    'median': log_prices.median(),\n",
    "    'std': log_prices.std(),\n",
    "    'skewness': skew(log_prices),\n",
    "    'kurtosis': kurtosis(log_prices)\n",
    "}\n",
    "\n",
    "print(\"=== TRANSFORMATION COMPARISON ===\")\n",
    "print(f\"{'Metric':<12} {'Original':<15} {'Log-Transformed':<15} {'Improvement'}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Mean':<12} ${original_stats['mean']:<14,.0f} {transformed_stats['mean']:<15.3f} {'N/A'}\")\n",
    "print(f\"{'Median':<12} ${original_stats['median']:<14,.0f} {transformed_stats['median']:<15.3f} {'N/A'}\")\n",
    "print(f\"{'Std Dev':<12} ${original_stats['std']:<14,.0f} {transformed_stats['std']:<15.3f} {'N/A'}\")\n",
    "print(f\"{'Skewness':<12} {original_stats['skewness']:<15.3f} {transformed_stats['skewness']:<15.3f} {abs(original_stats['skewness']) - abs(transformed_stats['skewness']):.3f}\")\n",
    "print(f\"{'Kurtosis':<12} {original_stats['kurtosis']:<15.3f} {transformed_stats['kurtosis']:<15.3f} {abs(original_stats['kurtosis']) - abs(transformed_stats['kurtosis']):.3f}\")\n",
    "\n",
    "# Create side-by-side comparison visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Price Distribution: Original vs Log-Transformed', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Original distribution plots\n",
    "axes[0, 0].hist(prices, bins=50, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "axes[0, 0].set_title('Original Price Distribution')\n",
    "axes[0, 0].set_xlabel('Sale Price ($)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].boxplot(prices, vert=True, patch_artist=True, \n",
    "                   boxprops=dict(facecolor='lightcoral'))\n",
    "axes[0, 1].set_title('Original Price Box Plot')\n",
    "axes[0, 1].set_ylabel('Sale Price ($)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "stats.probplot(prices, dist=\"norm\", plot=axes[0, 2])\n",
    "axes[0, 2].set_title('Original Q-Q Plot')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Log-transformed distribution plots\n",
    "axes[1, 0].hist(log_prices, bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[1, 0].set_title('Log-Transformed Price Distribution')\n",
    "axes[1, 0].set_xlabel('Log(Sale Price)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].boxplot(log_prices, vert=True, patch_artist=True, \n",
    "                   boxprops=dict(facecolor='lightgreen'))\n",
    "axes[1, 1].set_title('Log-Transformed Price Box Plot')\n",
    "axes[1, 1].set_ylabel('Log(Sale Price)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "stats.probplot(log_prices, dist=\"norm\", plot=axes[1, 2])\n",
    "axes[1, 2].set_title('Log-Transformed Q-Q Plot')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate improvement metrics\n",
    "skewness_improvement = ((abs(original_stats['skewness']) - abs(transformed_stats['skewness'])) / abs(original_stats['skewness'])) * 100\n",
    "kurtosis_improvement = ((abs(original_stats['kurtosis']) - abs(transformed_stats['kurtosis'])) / abs(original_stats['kurtosis'])) * 100\n",
    "\n",
    "print(f\"\\n=== TRANSFORMATION EFFECTIVENESS ===\")\n",
    "print(f\"Skewness reduction: {skewness_improvement:.1f}% improvement\")\n",
    "print(f\"Kurtosis reduction: {kurtosis_improvement:.1f}% improvement\")\n",
    "print(f\"Distribution shape: {'Much more normal' if abs(transformed_stats['skewness']) < 0.5 else 'Somewhat improved'}\")\n",
    "\n",
    "# Store the transformed target variable for future use\n",
    "df['LogSalePrice'] = log_prices\n",
    "print(f\"\\n✓ Log-transformed target variable created as 'LogSalePrice' column\")\n",
    "print(f\"✓ Ready to use log-transformed prices for modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insights from Price Distribution Analysis:**\n",
    "\n",
    "1. **Right-Skewed Distribution**: The price distribution shows clear right skewness, meaning there are more affordable houses with a few expensive outliers pulling the mean higher than the median.\n",
    "\n",
    "2. **Market Reality**: The $7,000+ difference between mean and median reflects typical real estate markets where luxury properties drive up averages.\n",
    "\n",
    "3. **Outlier Presence**: The box plot and outlier analysis reveal high-end properties that could either be luxury homes or potential data errors.\n",
    "\n",
    "4. **Non-Normal Distribution**: The Q-Q plot shows our prices don't follow a normal distribution, which can be problematic for linear regression models.\n",
    "\n",
    "**Why This Matters for Modeling:**\n",
    "- Most machine learning algorithms perform better with normally distributed target variables\n",
    "- The right skew could cause our model to be biased toward predicting higher prices\n",
    "- We should consider transforming our target variable to improve model performance\n",
    "\n",
    "## 2.2 Target Variable Transformation\n",
    "\n",
    "Based on our distribution analysis, let's apply a log transformation to normalize the price distribution. This is a common and effective technique in real estate price prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive statistics for house prices\n",
    "prices = df['SalePrice']\n",
    "\n",
    "# Calculate distribution statistics\n",
    "mean_price = prices.mean()\n",
    "median_price = prices.median()\n",
    "std_price = prices.std()\n",
    "skewness = skew(prices)\n",
    "kurt = kurtosis(prices)\n",
    "min_price = prices.min()\n",
    "max_price = prices.max()\n",
    "q1, q3 = prices.quantile([0.25, 0.75])\n",
    "iqr = q3 - q1\n",
    "\n",
    "print(\"=== COMPREHENSIVE PRICE DISTRIBUTION ANALYSIS ===\")\n",
    "print(f\"Mean price: ${mean_price:,.0f}\")\n",
    "print(f\"Median price: ${median_price:,.0f}\")\n",
    "print(f\"Standard deviation: ${std_price:,.0f}\")\n",
    "print(f\"Price range: ${min_price:,.0f} - ${max_price:,.0f}\")\n",
    "print(f\"Interquartile range (Q1-Q3): ${q1:,.0f} - ${q3:,.0f}\")\n",
    "print(f\"\\n=== DISTRIBUTION CHARACTERISTICS ===\")\n",
    "print(f\"Skewness: {skewness:.3f} ({'Right-skewed' if skewness > 0.5 else 'Nearly symmetric' if abs(skewness) <= 0.5 else 'Left-skewed'})\")\n",
    "print(f\"Kurtosis: {kurt:.3f} ({'Heavy-tailed' if kurt > 3 else 'Light-tailed' if kurt < 3 else 'Normal-tailed'})\")\n",
    "print(f\"Mean vs Median difference: ${mean_price - median_price:,.0f} ({((mean_price - median_price)/median_price)*100:.1f}%)\")\n",
    "\n",
    "# Create comprehensive visualization of price distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('House Price Distribution Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Histogram with statistics\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(prices, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax1.axvline(mean_price, color='red', linestyle='--', linewidth=2, label=f'Mean: ${mean_price:,.0f}')\n",
    "ax1.axvline(median_price, color='green', linestyle='--', linewidth=2, label=f'Median: ${median_price:,.0f}')\n",
    "ax1.set_xlabel('Sale Price ($)')\n",
    "ax1.set_ylabel('Number of Houses')\n",
    "ax1.set_title('Price Distribution with Central Tendencies')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Box plot showing outliers\n",
    "ax2 = axes[0, 1]\n",
    "box_plot = ax2.boxplot(prices, vert=True, patch_artist=True)\n",
    "box_plot['boxes'][0].set_facecolor('lightcoral')\n",
    "ax2.set_ylabel('Sale Price ($)')\n",
    "ax2.set_title('Box Plot - Outlier Detection')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))\n",
    "\n",
    "# 3. Q-Q plot against normal distribution\n",
    "ax3 = axes[1, 0]\n",
    "stats.probplot(prices, dist=\"norm\", plot=ax3)\n",
    "ax3.set_title('Q-Q Plot vs Normal Distribution')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Density plot with normal overlay\n",
    "ax4 = axes[1, 1]\n",
    "ax4.hist(prices, bins=50, density=True, alpha=0.7, color='lightblue', label='Actual Distribution')\n",
    "# Overlay normal distribution for comparison\n",
    "x_norm = np.linspace(prices.min(), prices.max(), 100)\n",
    "y_norm = stats.norm.pdf(x_norm, mean_price, std_price)\n",
    "ax4.plot(x_norm, y_norm, 'r-', linewidth=2, label='Normal Distribution')\n",
    "ax4.set_xlabel('Sale Price ($)')\n",
    "ax4.set_ylabel('Density')\n",
    "ax4.set_title('Density Plot with Normal Comparison')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify outliers using IQR method\n",
    "outlier_threshold_low = q1 - 1.5 * iqr\n",
    "outlier_threshold_high = q3 + 1.5 * iqr\n",
    "outliers = prices[(prices < outlier_threshold_low) | (prices > outlier_threshold_high)]\n",
    "\n",
    "print(f\"\\n=== OUTLIER ANALYSIS ===\")\n",
    "print(f\"Outlier thresholds: ${outlier_threshold_low:,.0f} - ${outlier_threshold_high:,.0f}\")\n",
    "print(f\"Number of outliers: {len(outliers)} ({len(outliers)/len(prices)*100:.1f}% of data)\")\n",
    "if len(outliers) > 0:\n",
    "    print(f\"Outlier price range: ${outliers.min():,.0f} - ${outliers.max():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Our Data Structure:**\n",
    "\n",
    "From this analysis, we can see that our dataset is well-balanced with both numerical and categorical features. The sample data shows us houses with different characteristics:\n",
    "- **Variety in size**: From 1,710 to 2,198 square feet of living area\n",
    "- **Quality differences**: Overall quality ratings from 5 to 8 (out of 10)\n",
    "- **Age variation**: Houses built between 1961-2003  \n",
    "- **Price range**: From $109,500 to $215,000\n",
    "\n",
    "This diversity suggests our model will have plenty of variation to learn from. Now let's dive deep into our target variable - the house prices we want to predict.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Target Variable Analysis\n",
    "\n",
    "Understanding our target variable (SalePrice) is crucial because it influences how we build our model. We need to understand its distribution, detect any issues, and potentially transform it for better modeling performance.\n",
    "\n",
    "## 2.1 Price Distribution Analysis\n",
    "\n",
    "Let's start by examining how house prices are distributed in our dataset. This will help us understand the market dynamics and identify any characteristics that might affect our modeling approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
