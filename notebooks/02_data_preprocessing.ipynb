{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02: Data Preprocessing and Feature Engineering\n",
    "\n",
    "This notebook implements comprehensive preprocessing strategies based on the exploratory data analysis findings from Notebook 01, preparing the dataset for machine learning model development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Processing\n",
    "\n",
    "Load datasets and implement parser-guided missing data treatment strategies identified during exploratory analysis.\n",
    "\n",
    "### 1.1 Dataset Import and Validation\n",
    "\n",
    "Import training and test datasets, create combined dataset for consistent feature processing, and validate data integrity against Notebook 01 findings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Import Validation:\n",
      "Training data: (1460, 81)\n",
      "Test data: (1459, 80)\n",
      "Combined dataset: (2919, 81)\n",
      "Features to process: 80\n",
      "\n",
      "Missing data validation:\n",
      "Expected features with missing data: 34\n",
      "Actual features with missing data: 34\n",
      "Validation: ✓ PASS\n"
     ]
    }
   ],
   "source": [
    "# Load required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the datasets\n",
    "df_train = pd.read_csv('../data/raw/train.csv')\n",
    "df_test = pd.read_csv('../data/raw/test.csv')\n",
    "\n",
    "print(\"Dataset Import Validation:\")\n",
    "print(f\"Training data: {df_train.shape}\")\n",
    "print(f\"Test data: {df_test.shape}\")\n",
    "\n",
    "# Create combined dataset for consistent feature processing\n",
    "df_combined = pd.concat([\n",
    "    df_train.drop('SalePrice', axis=1),\n",
    "    df_test\n",
    "], ignore_index=True)\n",
    "df_combined['dataset_source'] = ['train']*len(df_train) + ['test']*len(df_test)\n",
    "\n",
    "print(f\"Combined dataset: {df_combined.shape}\")\n",
    "print(f\"Features to process: {df_combined.shape[1] - 1}\")\n",
    "\n",
    "# Validate against Notebook 01 findings\n",
    "expected_missing_features = 34\n",
    "actual_missing_features = df_combined.drop('dataset_source', axis=1).isnull().any().sum()\n",
    "print(f\"\\nMissing data validation:\")\n",
    "print(f\"Expected features with missing data: {expected_missing_features}\")\n",
    "print(f\"Actual features with missing data: {actual_missing_features}\")\n",
    "print(f\"Validation: {'✓ PASS' if actual_missing_features == expected_missing_features else '✗ FAIL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset validation successful - all expected characteristics confirmed from Notebook 01 analysis.\n",
    "\n",
    "### 1.2 Parser Integration Setup\n",
    "\n",
    "Initialize data description parser for domain-guided preprocessing decisions and feature engineering strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser Integration Setup:\n",
      "✓ Official real estate documentation loaded successfully\n",
      "✓ Categorical features identified: 46\n",
      "✓ Numerical features identified: 33\n",
      "\n",
      "Misclassified ordinal features to correct:\n",
      "  OverallQual: Parser=Categorical, Pandas=int64\n",
      "  OverallCond: Parser=Categorical, Pandas=int64\n",
      "  MSSubClass: Parser=Categorical, Pandas=int64\n"
     ]
    }
   ],
   "source": [
    "# Setup data description parser for domain knowledge\n",
    "from data_description_parser import (\n",
    "    load_feature_descriptions,\n",
    "    quick_feature_lookup,\n",
    "    display_summary_table,\n",
    "    get_categorical_features,\n",
    "    get_numerical_features\n",
    ")\n",
    "\n",
    "# Load official documentation\n",
    "feature_descriptions = load_feature_descriptions()\n",
    "print(\"Parser Integration Setup:\")\n",
    "print(\"✓ Official real estate documentation loaded successfully\")\n",
    "\n",
    "# Get feature classifications for preprocessing\n",
    "categorical_features = get_categorical_features(feature_descriptions)\n",
    "numerical_features = get_numerical_features(feature_descriptions)\n",
    "\n",
    "print(f\"✓ Categorical features identified: {len(categorical_features)}\")\n",
    "print(f\"✓ Numerical features identified: {len(numerical_features)}\")\n",
    "\n",
    "# Verify critical misclassified features from Notebook 01\n",
    "misclassified_features = ['OverallQual', 'OverallCond', 'MSSubClass']\n",
    "print(f\"\\nMisclassified ordinal features to correct:\")\n",
    "for feature in misclassified_features:\n",
    "    feature_type = 'Categorical' if feature in categorical_features else 'Numerical'\n",
    "    pandas_type = df_train[feature].dtype\n",
    "    print(f\"  {feature}: Parser={feature_type}, Pandas={pandas_type}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
