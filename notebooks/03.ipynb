{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Feature Engineering & Optimization \n",
    "\n",
    "Feature engineering and optimization\n",
    "\n",
    "## 1. Data Loading and Initial Setup\n",
    "\n",
    "Load preprocessed datasets and establish feature engineering foundation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shapes:\n",
      "Training data: (1458, 80)\n",
      "Test data: (1459, 80)\n",
      "Target variable: (1458,)\n",
      "Combined dataset: (2917, 81)\n",
      "Initial feature count: 80\n",
      "\n",
      "No missing values found in combined dataset\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "\n",
    "\n",
    "# Load preprocessed data with proper NA handling\n",
    "train_df = pd.read_csv('../data/processed/train_cleaned.csv',  \n",
    "                      na_values=[''], \n",
    "                      keep_default_na=False)\n",
    "test_df = pd.read_csv('../data/processed/test_cleaned.csv', \n",
    "                     na_values=[''], \n",
    "                     keep_default_na=False)\n",
    "\n",
    "\n",
    "# Extract target variable from training data\n",
    "target_series = train_df['SalePrice']\n",
    "train_df = train_df.drop('SalePrice', axis=1)  # Remove target from features\n",
    "\n",
    "print(\"Dataset Shapes:\")\n",
    "print(f\"Training data: {train_df.shape}\")\n",
    "print(f\"Test data: {test_df.shape}\")\n",
    "print(f\"Target variable: {target_series.shape}\")\n",
    "\n",
    "# Create combined dataset for consistent feature engineering\n",
    "df_combined = pd.concat([train_df, test_df], ignore_index=True)\n",
    "df_combined['dataset_source'] = ['train']*len(train_df) + ['test']*len(test_df)\n",
    "\n",
    "print(f\"Combined dataset: {df_combined.shape}\")\n",
    "print(f\"Initial feature count: {df_combined.shape[1] - 1}\")  # -1 for dataset_source\n",
    "\n",
    "#Check for missing values\n",
    "missing_values = df_combined.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "if not missing_values.empty:\n",
    "    print(\"\\nMissing Values in Combined Dataset:\")\n",
    "    print(missing_values)\n",
    "else:\n",
    "    print(\"\\nNo missing values found in combined dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Feature Type Analysis\n",
    "Analyze current feature distribution and identify categories for targeted engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Type Distribution:\n",
      "Categorical features: 43\n",
      "Numerical features: 37\n",
      "Number of floats:\n",
      "1\n",
      "Number of integers:\n",
      "36\n",
      "Total features: 80\n",
      "\n",
      "Categorical features: ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1']...\n",
      "Numerical features: ['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1']...\n"
     ]
    }
   ],
   "source": [
    "# Analyze feature types for engineering strategy\n",
    "categorical_features = df_combined.select_dtypes(include=['object']).columns.tolist()\n",
    "if 'dataset_source' in categorical_features:\n",
    "    categorical_features.remove('dataset_source')\n",
    "\n",
    "numerical_features = df_combined.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    " \n",
    "\n",
    "print(\"Feature Type Distribution:\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "print(f\"Numerical features: {len(numerical_features)}\")\n",
    "print(\"Number of floats:\")\n",
    "print(len(df_combined.select_dtypes(include=['float64']).columns))\n",
    "print(\"Number of integers:\")\n",
    "print(len(df_combined.select_dtypes(include=['int64']).columns))   \n",
    "print(f\"Total features: {len(categorical_features) + len(numerical_features)}\")\n",
    "\n",
    "print(f\"\\nCategorical features: {categorical_features[:10]}...\")\n",
    "print(f\"Numerical features: {numerical_features[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Systematic Variable Transformations\n",
    "\n",
    "### 2.1 Skewness Analysis and Log Transformations\n",
    "Analyze skewness of numerical features and apply log transformations where necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness Analysis for Numerical Features:\n",
      "============================================================\n",
      "Features with |skewness| ≥ 0.5: 28\n",
      "\n",
      "Top 10 most skewed features:\n",
      "          Feature   Skewness  Contains_Zero\n",
      "34        MiscVal  21.939672           True\n",
      "33       PoolArea  20.563597           True\n",
      "3         LotArea  13.109495          False\n",
      "15   LowQualFinSF  12.084539           True\n",
      "31      3SsnPorch  11.372080           True\n",
      "22   KitchenAbvGr   4.571363          False\n",
      "10     BsmtFinSF2   4.144503           True\n",
      "30  EnclosedPorch   4.002344           True\n",
      "32    ScreenPorch   3.945101           True\n",
      "18   BsmtHalfBath   3.929996           True\n"
     ]
    }
   ],
   "source": [
    "# Analyze skewness for all numerical features\n",
    "print(\"Skewness Analysis for Numerical Features:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "skewed_features = []\n",
    "skewness_data = []\n",
    "\n",
    "for feature in numerical_features:\n",
    "    if feature in df_combined.columns:\n",
    "        # Calculate skewness for non-null values\n",
    "        non_null_values = df_combined[feature].dropna()\n",
    "\n",
    "        if len(non_null_values) > 0 and non_null_values.var() > 0:  # Avoid constant features\n",
    "            skew_value = stats.skew(non_null_values)\n",
    "\n",
    "            skewness_data.append({\n",
    "                'Feature': feature,\n",
    "                'Skewness': skew_value,\n",
    "                'Abs_Skewness': abs(skew_value),\n",
    "                'Min_Value': non_null_values.min(),\n",
    "                'Contains_Zero': (non_null_values == 0).any()\n",
    "            })\n",
    "\n",
    "            # Identify features for log1p transformation (threshold ≥0.5)\n",
    "            if abs(skew_value) >= 0.5:\n",
    "                skewed_features.append(feature)\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "skewness_df = pd.DataFrame(skewness_data)\n",
    "skewness_df = skewness_df.sort_values('Abs_Skewness', ascending=False)\n",
    "\n",
    "print(f\"Features with |skewness| ≥ 0.5: {len(skewed_features)}\")\n",
    "print(f\"\\nTop 10 most skewed features:\")\n",
    "print(skewness_df.head(10)[['Feature', 'Skewness', 'Contains_Zero']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total skewed features (|skewness| ≥ 0.5): 28\n",
      "All numerical features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_3ef12\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_3ef12_level0_col0\" class=\"col_heading level0 col0\" >Feature</th>\n",
       "      <th id=\"T_3ef12_level0_col1\" class=\"col_heading level0 col1\" >Skewness</th>\n",
       "      <th id=\"T_3ef12_level0_col2\" class=\"col_heading level0 col2\" >Contains_Zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row0_col0\" class=\"data row0 col0\" >MiscVal</td>\n",
       "      <td id=\"T_3ef12_row0_col1\" class=\"data row0 col1\" >21.939672</td>\n",
       "      <td id=\"T_3ef12_row0_col2\" class=\"data row0 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row1_col0\" class=\"data row1 col0\" >PoolArea</td>\n",
       "      <td id=\"T_3ef12_row1_col1\" class=\"data row1 col1\" >20.563597</td>\n",
       "      <td id=\"T_3ef12_row1_col2\" class=\"data row1 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row2_col0\" class=\"data row2 col0\" >LotArea</td>\n",
       "      <td id=\"T_3ef12_row2_col1\" class=\"data row2 col1\" >13.109495</td>\n",
       "      <td id=\"T_3ef12_row2_col2\" class=\"data row2 col2\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row3_col0\" class=\"data row3 col0\" >LowQualFinSF</td>\n",
       "      <td id=\"T_3ef12_row3_col1\" class=\"data row3 col1\" >12.084539</td>\n",
       "      <td id=\"T_3ef12_row3_col2\" class=\"data row3 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row4_col0\" class=\"data row4 col0\" >3SsnPorch</td>\n",
       "      <td id=\"T_3ef12_row4_col1\" class=\"data row4 col1\" >11.372080</td>\n",
       "      <td id=\"T_3ef12_row4_col2\" class=\"data row4 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row5_col0\" class=\"data row5 col0\" >KitchenAbvGr</td>\n",
       "      <td id=\"T_3ef12_row5_col1\" class=\"data row5 col1\" >4.571363</td>\n",
       "      <td id=\"T_3ef12_row5_col2\" class=\"data row5 col2\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row6_col0\" class=\"data row6 col0\" >BsmtFinSF2</td>\n",
       "      <td id=\"T_3ef12_row6_col1\" class=\"data row6 col1\" >4.144503</td>\n",
       "      <td id=\"T_3ef12_row6_col2\" class=\"data row6 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row7_col0\" class=\"data row7 col0\" >EnclosedPorch</td>\n",
       "      <td id=\"T_3ef12_row7_col1\" class=\"data row7 col1\" >4.002344</td>\n",
       "      <td id=\"T_3ef12_row7_col2\" class=\"data row7 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row8_col0\" class=\"data row8 col0\" >ScreenPorch</td>\n",
       "      <td id=\"T_3ef12_row8_col1\" class=\"data row8 col1\" >3.945101</td>\n",
       "      <td id=\"T_3ef12_row8_col2\" class=\"data row8 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row9_col0\" class=\"data row9 col0\" >BsmtHalfBath</td>\n",
       "      <td id=\"T_3ef12_row9_col1\" class=\"data row9 col1\" >3.929996</td>\n",
       "      <td id=\"T_3ef12_row9_col2\" class=\"data row9 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row10_col0\" class=\"data row10 col0\" >GarageYrBlt</td>\n",
       "      <td id=\"T_3ef12_row10_col1\" class=\"data row10 col1\" >-3.919740</td>\n",
       "      <td id=\"T_3ef12_row10_col2\" class=\"data row10 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row11_col0\" class=\"data row11 col0\" >MasVnrArea</td>\n",
       "      <td id=\"T_3ef12_row11_col1\" class=\"data row11 col1\" >2.621719</td>\n",
       "      <td id=\"T_3ef12_row11_col2\" class=\"data row11 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row12_col0\" class=\"data row12 col0\" >OpenPorchSF</td>\n",
       "      <td id=\"T_3ef12_row12_col1\" class=\"data row12 col1\" >2.529358</td>\n",
       "      <td id=\"T_3ef12_row12_col2\" class=\"data row12 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row13_col0\" class=\"data row13 col0\" >WoodDeckSF</td>\n",
       "      <td id=\"T_3ef12_row13_col1\" class=\"data row13 col1\" >1.844792</td>\n",
       "      <td id=\"T_3ef12_row13_col2\" class=\"data row13 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row14_col0\" class=\"data row14 col0\" >MSSubClass</td>\n",
       "      <td id=\"T_3ef12_row14_col1\" class=\"data row14 col1\" >1.375131</td>\n",
       "      <td id=\"T_3ef12_row14_col2\" class=\"data row14 col2\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row15_col0\" class=\"data row15 col0\" >1stFlrSF</td>\n",
       "      <td id=\"T_3ef12_row15_col1\" class=\"data row15 col1\" >1.257286</td>\n",
       "      <td id=\"T_3ef12_row15_col2\" class=\"data row15 col2\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row16_col0\" class=\"data row16 col0\" >LotFrontage</td>\n",
       "      <td id=\"T_3ef12_row16_col1\" class=\"data row16 col1\" >1.102704</td>\n",
       "      <td id=\"T_3ef12_row16_col2\" class=\"data row16 col2\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row17_col0\" class=\"data row17 col0\" >GrLivArea</td>\n",
       "      <td id=\"T_3ef12_row17_col1\" class=\"data row17 col1\" >1.068750</td>\n",
       "      <td id=\"T_3ef12_row17_col2\" class=\"data row17 col2\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row18_col0\" class=\"data row18 col0\" >BsmtFinSF1</td>\n",
       "      <td id=\"T_3ef12_row18_col1\" class=\"data row18 col1\" >0.980645</td>\n",
       "      <td id=\"T_3ef12_row18_col2\" class=\"data row18 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row19_col0\" class=\"data row19 col0\" >BsmtUnfSF</td>\n",
       "      <td id=\"T_3ef12_row19_col1\" class=\"data row19 col1\" >0.919688</td>\n",
       "      <td id=\"T_3ef12_row19_col2\" class=\"data row19 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row20_col0\" class=\"data row20 col0\" >2ndFlrSF</td>\n",
       "      <td id=\"T_3ef12_row20_col1\" class=\"data row20 col1\" >0.861556</td>\n",
       "      <td id=\"T_3ef12_row20_col2\" class=\"data row20 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row21_col0\" class=\"data row21 col0\" >TotRmsAbvGrd</td>\n",
       "      <td id=\"T_3ef12_row21_col1\" class=\"data row21 col1\" >0.749232</td>\n",
       "      <td id=\"T_3ef12_row21_col2\" class=\"data row21 col2\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row22_col0\" class=\"data row22 col0\" >Fireplaces</td>\n",
       "      <td id=\"T_3ef12_row22_col1\" class=\"data row22 col1\" >0.725278</td>\n",
       "      <td id=\"T_3ef12_row22_col2\" class=\"data row22 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row23_col0\" class=\"data row23 col0\" >HalfBath</td>\n",
       "      <td id=\"T_3ef12_row23_col1\" class=\"data row23 col1\" >0.696666</td>\n",
       "      <td id=\"T_3ef12_row23_col2\" class=\"data row23 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row24_col0\" class=\"data row24 col0\" >TotalBsmtSF</td>\n",
       "      <td id=\"T_3ef12_row24_col1\" class=\"data row24 col1\" >0.671751</td>\n",
       "      <td id=\"T_3ef12_row24_col2\" class=\"data row24 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row25_col0\" class=\"data row25 col0\" >BsmtFullBath</td>\n",
       "      <td id=\"T_3ef12_row25_col1\" class=\"data row25 col1\" >0.622415</td>\n",
       "      <td id=\"T_3ef12_row25_col2\" class=\"data row25 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row26_col0\" class=\"data row26 col0\" >YearBuilt</td>\n",
       "      <td id=\"T_3ef12_row26_col1\" class=\"data row26 col1\" >-0.599194</td>\n",
       "      <td id=\"T_3ef12_row26_col2\" class=\"data row26 col2\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row27_col0\" class=\"data row27 col0\" >OverallCond</td>\n",
       "      <td id=\"T_3ef12_row27_col1\" class=\"data row27 col1\" >0.569314</td>\n",
       "      <td id=\"T_3ef12_row27_col2\" class=\"data row27 col2\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row28_col0\" class=\"data row28 col0\" >YearRemodAdd</td>\n",
       "      <td id=\"T_3ef12_row28_col1\" class=\"data row28 col1\" >-0.450131</td>\n",
       "      <td id=\"T_3ef12_row28_col2\" class=\"data row28 col2\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3ef12_row29_col0\" class=\"data row29 col0\" >BedroomAbvGr</td>\n",
       "      <td id=\"T_3ef12_row29_col1\" class=\"data row29 col1\" >0.326568</td>\n",
       "      <td id=\"T_3ef12_row29_col2\" class=\"data row29 col2\" >True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x13c7f7790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyze skewness for all numerical features\n",
    "skewness_data = []\n",
    "\n",
    "for feature in numerical_features:\n",
    "    if feature in df_combined.columns:\n",
    "        values = df_combined[feature].dropna()\n",
    "\n",
    "        if len(values) > 0 and values.var() > 0:  # Skip constant features\n",
    "            skew = stats.skew(values)\n",
    "            skewness_data.append({\n",
    "                'Feature': feature,\n",
    "                'Skewness': skew,\n",
    "                'Abs_Skewness': abs(skew),\n",
    "                'Contains_Zero': (values == 0).any()\n",
    "            })\n",
    "\n",
    "# Create summary DataFrame\n",
    "skewness_df = pd.DataFrame(skewness_data).sort_values('Abs_Skewness', ascending=False)\n",
    "\n",
    "# Identify highly skewed features (|skew| ≥ 0.5)\n",
    "skewed_features = skewness_df[skewness_df['Abs_Skewness'] >= 0.5]['Feature'].tolist()\n",
    "\n",
    "print(f\"Total skewed features (|skewness| ≥ 0.5): {len(skewed_features)}\")\n",
    "\n",
    "# All numerical features with skewness\n",
    "print(\"All numerical features:\")\n",
    "display(skewness_df[['Feature', 'Skewness', 'Contains_Zero']].head(30).style.hide(axis='index'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data validation confirms 230+ features with zero missing values from comprehensive preprocessing pipeline.\n",
    "Combined dataset structure enables consistent feature engineering across train/test splits.\n",
    "\n",
    "### 1.2 Baseline Feature Analysis and Correlation Discovery\n",
    "\n",
    "Implement systematic correlation analysis to identify feature engineering opportunities.\n",
    "Execute feature importance ranking pipeline to establish baseline for engineering optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = df_combined.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_features = [col for col in numerical_features if col not in ['Id', 'SalePrice', 'SalePrice_log', 'dataset_source']]\n",
    "\n",
    "# All categorical features one-hot encoded in preprocessing (notebook 02)\n",
    "categorical_features = df_combined.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "if 'dataset_source' in categorical_features:\n",
    "    categorical_features.remove('dataset_source')\n",
    "\n",
    "print(f\"Numerical features available: {len(numerical_features)}\")\n",
    "print(f\"Categorical features remaining (all one-hot encoded): {len(categorical_features)}\")\n",
    "print(f\"Total features from preprocessing: {len(numerical_features) + len(categorical_features)}\")\n",
    "\n",
    "# Baseline correlation analysis with target (train data only)\n",
    "target_col = 'SalePrice_log' if 'SalePrice_log' in df_train_clean.columns else 'SalePrice'\n",
    "if target_col in df_train_clean.columns:\n",
    "    # Filter numerical features that exist in train data (excluding Id and target)\n",
    "    train_numerical_features = [col for col in numerical_features if col in df_train_clean.columns]\n",
    "    # Also exclude Id and target variables from features list\n",
    "    train_numerical_features = [col for col in train_numerical_features if col not in ['Id', 'SalePrice', 'SalePrice_log']]\n",
    "\n",
    "    baseline_correlations = df_train_clean[train_numerical_features + [target_col]].corr()[target_col].sort_values(ascending=False)\n",
    "\n",
    "    print(f\"\\nTop 10 features correlated with {target_col}:\")\n",
    "    print(baseline_correlations.head(11)[1:])  # Exclude target itself\n",
    "\n",
    "    print(\"\\nLeast correlated features (potential for engineering):\")\n",
    "    print(baseline_correlations.tail(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation analysis identifies OverallQual (0.821) and GrLivArea_log (0.737) as dominant predictors establishing clear engineering priorities.\n",
    "Negative correlations in garage types (-0.389) and foundation materials (-0.338) reveal combination opportunities for feature optimization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Correlation-Driven Feature Combination Discovery\n",
    "\n",
    "Calculate individual component correlations first, then create combinations and measure improvement over best individual components.\n",
    "Apply systematic combination testing with documented correlation improvements following Kaggle best practices for feature engineering.\n",
    "\n",
    "### 2.1 Individual Component Baseline Analysis\n",
    "\n",
    "Calculate correlation for all individual features to establish baseline performance for combination comparison.\n",
    "Identify top performers in each category (area, quality, bathroom) for targeted combination testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate individual feature correlations by category for baseline comparison\n",
    "\n",
    "# Area-related features for combination testing\n",
    "area_features = ['GrLivArea', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GarageArea',\n",
    "                'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', 'ScreenPorch',\n",
    "                'MasVnrArea', 'LotArea', 'PoolArea', 'LowQualFinSF']\n",
    "\n",
    "# Quality-related features for combination testing\n",
    "quality_features = ['OverallQual', 'OverallCond', 'ExterQual', 'ExterCond',\n",
    "                   'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual',\n",
    "                   'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']\n",
    "\n",
    "# Bathroom and room features for combination testing\n",
    "bath_room_features = ['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath',\n",
    "                     'BedroomAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars']\n",
    "\n",
    "# Calculate baseline correlations for each category\n",
    "print(\"INDIVIDUAL COMPONENT BASELINE CORRELATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Area features baseline\n",
    "print(\"\\nAREA FEATURES:\")\n",
    "area_correlations = {}\n",
    "for feature in area_features:\n",
    "    if feature in df_train_clean.columns:\n",
    "        corr = df_train_clean[feature].corr(df_train_clean[target_col])\n",
    "        area_correlations[feature] = corr\n",
    "        print(f\"{feature}: {corr:.3f}\")\n",
    "\n",
    "# Quality features baseline\n",
    "print(\"\\nQUALITY FEATURES:\")\n",
    "quality_correlations = {}\n",
    "for feature in quality_features:\n",
    "    if feature in df_train_clean.columns:\n",
    "        corr = df_train_clean[feature].corr(df_train_clean[target_col])\n",
    "        quality_correlations[feature] = corr\n",
    "        print(f\"{feature}: {corr:.3f}\")\n",
    "\n",
    "# Bathroom/room features baseline\n",
    "print(\"\\nBATHROOM & ROOM FEATURES:\")\n",
    "bath_room_correlations = {}\n",
    "for feature in bath_room_features:\n",
    "    if feature in df_train_clean.columns:\n",
    "        corr = df_train_clean[feature].corr(df_train_clean[target_col])\n",
    "        bath_room_correlations[feature] = corr\n",
    "        print(f\"{feature}: {corr:.3f}\")\n",
    "\n",
    "# Identify top performers in each category\n",
    "print(\"\\nTOP PERFORMERS BY CATEGORY:\")\n",
    "print(f\"Best Area Feature: {max(area_correlations, key=area_correlations.get)} ({max(area_correlations.values()):.3f})\")\n",
    "print(f\"Best Quality Feature: {max(quality_correlations, key=quality_correlations.get)} ({max(quality_correlations.values()):.3f})\")\n",
    "print(f\"Best Bath/Room Feature: {max(bath_room_correlations, key=bath_room_correlations.get)} ({max(bath_room_correlations.values()):.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline analysis reveals OverallQual (0.821) dominates as category leader, with clear 0.65+ correlation clustering in basement/garage features.\n",
    "Quality features maintain consistent 0.6+ performance while area features span from negative to 0.725, establishing engineering improvement thresholds.\n",
    "\n",
    "### 2.2 Area Feature Combinations vs Individual Components\n",
    "\n",
    "Test area feature additions and ratios against individual component correlations to measure improvement.\n",
    "Focus on combinations that beat best individual component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_combinations = {}\n",
    "print(\"AREA FEATURE COMBINATIONS VS INDIVIDUAL COMPONENTS\")\n",
    "print(\"=\" * 55)\n",
    "print(\"Strategy: Compare each combination vs its best individual component\")\n",
    "print()\n",
    "\n",
    "# Test addition combinations\n",
    "print(\"ADDITION COMBINATIONS:\")\n",
    "area_pairs = [\n",
    "    ('TotalBsmtSF', '1stFlrSF'),\n",
    "    ('GrLivArea', 'TotalBsmtSF'),\n",
    "    ('GarageArea', 'TotalBsmtSF'),\n",
    "    ('1stFlrSF', '2ndFlrSF'),\n",
    "    ('MasVnrArea', 'TotalBsmtSF'),\n",
    "    ('WoodDeckSF', 'OpenPorchSF'),\n",
    "    ('GrLivArea', 'GarageArea')\n",
    "]\n",
    "\n",
    "for feat1, feat2 in area_pairs:\n",
    "    if feat1 in df_train_clean.columns and feat2 in df_train_clean.columns:\n",
    "        combination = df_train_clean[feat1] + df_train_clean[feat2]\n",
    "        corr = combination.corr(df_train_clean[target_col])\n",
    "        area_combinations[f\"{feat1}_add_{feat2}\"] = corr\n",
    "\n",
    "        # Compare to individual components\n",
    "        individual_best = max(area_correlations.get(feat1, 0), area_correlations.get(feat2, 0))\n",
    "        improvement = (corr - individual_best) / individual_best * 100\n",
    "\n",
    "        print(f\"{feat1}_add_{feat2}: {corr:.3f} (vs best individual {individual_best:.3f}, {improvement:+.1f}%)\")\n",
    "\n",
    "# Test ratio combinations\n",
    "print(\"\\nRATIO COMBINATIONS:\")\n",
    "ratio_pairs = [\n",
    "    ('GrLivArea', 'LotArea'),\n",
    "    ('TotalBsmtSF', 'GrLivArea'),\n",
    "    ('GarageArea', 'GarageCars'),\n",
    "    ('1stFlrSF', 'TotalBsmtSF'),\n",
    "    ('MasVnrArea', 'GrLivArea')\n",
    "]\n",
    "\n",
    "for feat1, feat2 in ratio_pairs:\n",
    "    if feat1 in df_train_clean.columns and feat2 in df_train_clean.columns:\n",
    "        # Avoid division by zero\n",
    "        mask = df_train_clean[feat2] > 0\n",
    "        if mask.sum() > 100:  # Ensure sufficient data\n",
    "            ratio = df_train_clean.loc[mask, feat1] / df_train_clean.loc[mask, feat2]\n",
    "            corr = ratio.corr(df_train_clean.loc[mask, target_col])\n",
    "            area_combinations[f\"{feat1}_ratio_{feat2}\"] = corr\n",
    "\n",
    "            # Compare to individual components\n",
    "            individual_best = max(area_correlations.get(feat1, 0), area_correlations.get(feat2, 0))\n",
    "            improvement = (corr - individual_best) / individual_best * 100\n",
    "\n",
    "            print(f\"{feat1}_ratio_{feat2}: {corr:.3f} (vs best individual {individual_best:.3f}, {improvement:+.1f}%)\")\n",
    "\n",
    "# Identify successful combinations that beat individual baselines\n",
    "print(\"\\nSUCCESSFUL AREA COMBINATIONS (beat individual components):\")\n",
    "successful_area = {}\n",
    "\n",
    "for name, corr in area_combinations.items():\n",
    "    # Extract feature names to find individual baselines\n",
    "    if '_add_' in name or '_ratio_' in name:\n",
    "        parts = name.replace('_add_', '|').replace('_ratio_', '|').split('|')\n",
    "        if len(parts) == 2:\n",
    "            feat1, feat2 = parts\n",
    "            # Get individual correlations\n",
    "            corr1 = area_correlations.get(feat1, 0)\n",
    "            corr2 = area_correlations.get(feat2, 0)\n",
    "            individual_best = max(corr1, corr2)\n",
    "            \n",
    "            # Check if combination beats individual best\n",
    "            if corr > individual_best:\n",
    "                improvement = (corr - individual_best) / individual_best * 100\n",
    "                successful_area[name] = (corr, improvement, individual_best)\n",
    "\n",
    "for name, (corr, improvement, individual_best) in sorted(successful_area.items(), key=lambda x: x[1][1], reverse=True):\n",
    "    print(f\"{name}: {corr:.3f} ({improvement:+.1f}% vs best individual {individual_best:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition strategy demonstrates consistent success with 7 combinations beating individual components (3.2% to 30.8% improvement).\n",
    "Ratio strategy fails systematically with all combinations showing severe correlation decline (-58.8% to -116.2%), validating absolute measurement approach over efficiency metrics.\n",
    "\n",
    "### 2.3 Quality Feature Combinations vs Individual Components\n",
    "\n",
    "Create quality interaction features and quality-area multiplications with individual correlation comparison.\n",
    "Document improvement percentages over individual quality and area features for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_combinations = {}\n",
    "\n",
    "print(\"QUALITY FEATURE COMBINATIONS VS INDIVIDUAL COMPONENTS\")\n",
    "print(\"=\" * 55)\n",
    "print(\"Strategy: Compare each combination vs its best individual component\")\n",
    "print()\n",
    "\n",
    "# Test quality multiplication with area features\n",
    "print(\"QUALITY × AREA COMBINATIONS:\")\n",
    "quality_area_pairs = [\n",
    "    ('OverallQual', 'GrLivArea'),\n",
    "    ('OverallQual', 'TotalBsmtSF'),\n",
    "    ('ExterQual', 'GrLivArea'),\n",
    "    ('KitchenQual', 'GrLivArea'),\n",
    "    ('ExterQual', 'TotalBsmtSF'),\n",
    "    ('BsmtQual', 'TotalBsmtSF'),\n",
    "    ('KitchenQual', 'TotalBsmtSF')\n",
    "]\n",
    "\n",
    "for qual_feat, area_feat in quality_area_pairs:\n",
    "    if qual_feat in df_train_clean.columns and area_feat in df_train_clean.columns:\n",
    "        combination = df_train_clean[qual_feat] * df_train_clean[area_feat]\n",
    "        corr = combination.corr(df_train_clean[target_col])\n",
    "        quality_combinations[f\"{qual_feat}_multiply_{area_feat}\"] = corr\n",
    "\n",
    "        # Compare to individual components\n",
    "        qual_corr = quality_correlations.get(qual_feat, 0)\n",
    "        area_corr = area_correlations.get(area_feat, 0)\n",
    "        individual_best = max(qual_corr, area_corr)\n",
    "        improvement = (corr - individual_best) / individual_best * 100\n",
    "\n",
    "        print(f\"{qual_feat}_multiply_{area_feat}: {corr:.3f} (vs best individual {individual_best:.3f}, {improvement:+.1f}%)\")\n",
    "\n",
    "# Test quality interaction combinations\n",
    "print(\"\\nQUALITY × QUALITY COMBINATIONS:\")\n",
    "quality_pairs = [\n",
    "    ('OverallQual', 'ExterQual'),\n",
    "    ('OverallQual', 'KitchenQual'),\n",
    "    ('ExterQual', 'KitchenQual'),\n",
    "    ('OverallQual', 'BsmtQual'),\n",
    "    ('ExterQual', 'BsmtQual'),\n",
    "    ('KitchenQual', 'BsmtQual')\n",
    "]\n",
    "\n",
    "for qual1, qual2 in quality_pairs:\n",
    "    if qual1 in df_train_clean.columns and qual2 in df_train_clean.columns:\n",
    "        combination = df_train_clean[qual1] * df_train_clean[qual2]\n",
    "        corr = combination.corr(df_train_clean[target_col])\n",
    "        quality_combinations[f\"{qual1}_multiply_{qual2}\"] = corr\n",
    "\n",
    "        # Compare to individual components\n",
    "        individual_best = max(quality_correlations.get(qual1, 0), quality_correlations.get(qual2, 0))\n",
    "        improvement = (corr - individual_best) / individual_best * 100\n",
    "\n",
    "        print(f\"{qual1}_multiply_{qual2}: {corr:.3f} (vs best individual {individual_best:.3f}, {improvement:+.1f}%)\")\n",
    "\n",
    "# Test quality addition combinations\n",
    "print(\"\\nQUALITY + QUALITY COMBINATIONS:\")\n",
    "for qual1, qual2 in quality_pairs:\n",
    "    if qual1 in df_train_clean.columns and qual2 in df_train_clean.columns:\n",
    "        combination = df_train_clean[qual1] + df_train_clean[qual2]\n",
    "        corr = combination.corr(df_train_clean[target_col])\n",
    "        quality_combinations[f\"{qual1}_add_{qual2}\"] = corr\n",
    "\n",
    "        # Compare to individual components\n",
    "        individual_best = max(quality_correlations.get(qual1, 0), quality_correlations.get(qual2, 0))\n",
    "        improvement = (corr - individual_best) / individual_best * 100\n",
    "\n",
    "        print(f\"{qual1}_add_{qual2}: {corr:.3f} (vs best individual {individual_best:.3f}, {improvement:+.1f}%)\")\n",
    "\n",
    "# Identify successful quality combinations that beat individual baselines\n",
    "print(\"\\nSUCCESSFUL QUALITY COMBINATIONS (beat individual components):\")\n",
    "successful_quality = {}\n",
    "\n",
    "for name, corr in quality_combinations.items():\n",
    "    # Extract feature names to find individual baselines\n",
    "    if '_multiply_' in name or '_add_' in name:\n",
    "        parts = name.replace('_multiply_', '|').replace('_add_', '|').split('|')\n",
    "        if len(parts) == 2:\n",
    "            feat1, feat2 = parts\n",
    "            # Get individual correlations\n",
    "            corr1 = quality_correlations.get(feat1, area_correlations.get(feat1, 0))\n",
    "            corr2 = quality_correlations.get(feat2, area_correlations.get(feat2, 0))\n",
    "            individual_best = max(corr1, corr2)\n",
    "            \n",
    "            # Check if combination beats individual best\n",
    "            if corr > individual_best:\n",
    "                improvement = (corr - individual_best) / individual_best * 100\n",
    "                successful_quality[name] = (corr, improvement, individual_best)\n",
    "\n",
    "for name, (corr, improvement, individual_best) in sorted(successful_quality.items(), key=lambda x: x[1][1], reverse=True):\n",
    "    print(f\"{name}: {corr:.3f} ({improvement:+.1f}% vs best individual {individual_best:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quality feature engineering achieves 14 successful combinations beating individual components with improvements ranging 1.0% to 15.0%.\n",
    "Quality-area multiplication strategy proves most effective (BsmtQual_multiply_TotalBsmtSF: +15.0%) while OverallQual combinations show modest gains due to high individual baseline (0.821).\n",
    "\n",
    "### 2.4 Bathroom and Room Engineering vs Individual Components\n",
    "\n",
    "Test bathroom combinations and room efficiency ratios against individual bathroom/room feature correlations.\n",
    "Calculate improvement over best individual components to validate engineering decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bath_room_combinations = {}\n",
    "print(\"BATHROOM & ROOM COMBINATIONS VS INDIVIDUAL COMPONENTS\")\n",
    "print(\"=\" * 55)\n",
    "print(\"Strategy: Compare each combination vs its best individual component\")\n",
    "print()\n",
    "\n",
    "# Test bathroom calculation combinations\n",
    "print(\"BATHROOM COMBINATIONS:\")\n",
    "# Standard real estate bathroom calculation: FullBath + 0.5*HalfBath\n",
    "if 'FullBath' in df_train_clean.columns and 'HalfBath' in df_train_clean.columns:\n",
    "    total_baths_standard = df_train_clean['FullBath'] + 0.5 * df_train_clean['HalfBath']\n",
    "    corr = total_baths_standard.corr(df_train_clean[target_col])\n",
    "    bath_room_combinations['TotalBaths_Standard'] = corr\n",
    "    individual_best = max(bath_room_correlations.get('FullBath', 0), bath_room_correlations.get('HalfBath', 0))\n",
    "    improvement = (corr - individual_best) / individual_best * 100\n",
    "    print(f\"TotalBaths_Standard: {corr:.3f} (vs best individual {individual_best:.3f}, {improvement:+.1f}%)\")\n",
    "\n",
    "# Include basement bathrooms\n",
    "if all(col in df_train_clean.columns for col in ['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath']):\n",
    "    total_baths_all = (df_train_clean['FullBath'] + df_train_clean['BsmtFullBath'] +\n",
    "                      0.5 * (df_train_clean['HalfBath'] + df_train_clean['BsmtHalfBath']))\n",
    "    corr = total_baths_all.corr(df_train_clean[target_col])\n",
    "    bath_room_combinations['TotalBaths_All'] = corr\n",
    "    individual_best = max(bath_room_correlations.get('FullBath', 0), bath_room_correlations.get('BsmtFullBath', 0))\n",
    "    improvement = (corr - individual_best) / individual_best * 100\n",
    "    print(f\"TotalBaths_All: {corr:.3f} (vs best individual {individual_best:.3f}, {improvement:+.1f}%)\")\n",
    "\n",
    "# Test room efficiency ratios\n",
    "print(\"\\nROOM EFFICIENCY COMBINATIONS:\")\n",
    "room_efficiency_pairs = [\n",
    "    ('GrLivArea', 'TotRmsAbvGrd'),\n",
    "    ('GrLivArea', 'BedroomAbvGr'),\n",
    "    ('TotalBsmtSF', 'BedroomAbvGr'),\n",
    "    ('GarageCars', 'GarageArea')\n",
    "]\n",
    "\n",
    "for area_feat, room_feat in room_efficiency_pairs:\n",
    "    if area_feat in df_train_clean.columns and room_feat in df_train_clean.columns:\n",
    "        # Calculate area per room (avoid division by zero)\n",
    "        mask = df_train_clean[room_feat] > 0\n",
    "        if mask.sum() > 100:\n",
    "            efficiency = df_train_clean.loc[mask, area_feat] / df_train_clean.loc[mask, room_feat]\n",
    "            corr = efficiency.corr(df_train_clean.loc[mask, target_col])\n",
    "            bath_room_combinations[f\"{area_feat}_per_{room_feat}\"] = corr\n",
    "\n",
    "            # Compare to individual components\n",
    "            area_corr = area_correlations.get(area_feat, 0) if area_feat in area_correlations else bath_room_correlations.get(area_feat, 0)\n",
    "            room_corr = bath_room_correlations.get(room_feat, 0)\n",
    "            individual_best = max(area_corr, room_corr)\n",
    "            improvement = (corr - individual_best) / individual_best * 100\n",
    "\n",
    "            print(f\"{area_feat}_per_{room_feat}: {corr:.3f} (vs best individual {individual_best:.3f}, {improvement:+.1f}%)\")\n",
    "\n",
    "# Test room count combinations\n",
    "print(\"\\nROOM COUNT COMBINATIONS:\")\n",
    "room_count_features = ['BedroomAbvGr', 'TotRmsAbvGrd', 'Fireplaces']\n",
    "if all(col in df_train_clean.columns for col in room_count_features):\n",
    "    total_rooms = df_train_clean['BedroomAbvGr'] + df_train_clean['TotRmsAbvGrd'] + df_train_clean['Fireplaces']\n",
    "    corr = total_rooms.corr(df_train_clean[target_col])\n",
    "    bath_room_combinations['TotalRooms_All'] = corr\n",
    "    individual_best = max([bath_room_correlations.get(feat, 0) for feat in room_count_features])\n",
    "    improvement = (corr - individual_best) / individual_best * 100\n",
    "    print(f\"TotalRooms_All: {corr:.3f} (vs best individual {individual_best:.3f}, {improvement:+.1f}%)\")\n",
    "\n",
    "# Test garage efficiency\n",
    "if 'GarageArea' in df_train_clean.columns and 'GarageCars' in df_train_clean.columns:\n",
    "    mask = df_train_clean['GarageCars'] > 0\n",
    "    if mask.sum() > 100:\n",
    "        garage_efficiency = df_train_clean.loc[mask, 'GarageArea'] / df_train_clean.loc[mask, 'GarageCars']\n",
    "        corr = garage_efficiency.corr(df_train_clean.loc[mask, target_col])\n",
    "        bath_room_combinations['GarageArea_per_Car'] = corr\n",
    "        individual_best = max(area_correlations.get('GarageArea', 0), bath_room_correlations.get('GarageCars', 0))\n",
    "        improvement = (corr - individual_best) / individual_best * 100\n",
    "        print(f\"GarageArea_per_Car: {corr:.3f} (vs best individual {individual_best:.3f}, {improvement:+.1f}%)\")\n",
    "\n",
    "# Identify successful bathroom/room combinations\n",
    "print(\"\\nSUCCESSFUL BATH/ROOM COMBINATIONS (beat individual components):\")\n",
    "successful_bath_room = {}\n",
    "\n",
    "for name, corr in bath_room_combinations.items():\n",
    "    # For bathroom/room combinations, find the best individual component\n",
    "    individual_best = 0\n",
    "    if 'TotalBaths' in name:\n",
    "        individual_best = max(bath_room_correlations.get('FullBath', 0), bath_room_correlations.get('HalfBath', 0))\n",
    "    elif 'Room' in name:\n",
    "        individual_best = max(bath_room_correlations.get('TotRmsAbvGrd', 0), bath_room_correlations.get('BedroomAbvGr', 0))\n",
    "    elif 'Garage' in name:\n",
    "        individual_best = max(bath_room_correlations.get('GarageArea', 0), bath_room_correlations.get('GarageCars', 0))\n",
    "    \n",
    "    # Check if combination beats individual best\n",
    "    if corr > individual_best and individual_best > 0:\n",
    "        improvement = (corr - individual_best) / individual_best * 100\n",
    "        successful_bath_room[name] = (corr, improvement, individual_best)\n",
    "\n",
    "for name, (corr, improvement, individual_best) in sorted(successful_bath_room.items(), key=lambda x: x[1][1], reverse=True):\n",
    "    print(f\"{name}: {corr:.3f} ({improvement:+.1f}% vs best individual {individual_best:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bathroom engineering achieves strong success with TotalBaths_All (+13.6%) and TotalBaths_Standard (+7.6%) beating individual components.\n",
    "Room efficiency ratios fail systematically (-21.5% to -104.7%) confirming ratio strategy ineffectiveness across all feature categories, while absolute measurement combinations succeed.\n",
    "\n",
    "### 2.5 Final Selection Based on Improvement Analysis\n",
    "\n",
    "Compile successful combinations from all categories that beat their individual component baselines.\n",
    "Create final engineered feature set with documented improvement percentages and systematic correlation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all successful combinations from sections 2.2-2.4\n",
    "print(\"FINAL FEATURE ENGINEERING SELECTION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Collect all combinations that beat individual components\n",
    "final_engineered_features = {}\n",
    "\n",
    "# Area combinations that beat individual components\n",
    "print(\"SUCCESSFUL AREA COMBINATIONS:\")\n",
    "if 'successful_area' in locals():\n",
    "    for name, (corr, improvement, individual_best) in sorted(successful_area.items(), key=lambda x: x[1][1], reverse=True):\n",
    "        final_engineered_features[name] = corr\n",
    "        print(f\"  {name}: {corr:.3f} ({improvement:+.1f}% vs individual {individual_best:.3f})\")\n",
    "\n",
    "# Quality combinations that beat individual components\n",
    "print(\"\\nSUCCESSFUL QUALITY COMBINATIONS:\")\n",
    "if 'successful_quality' in locals():\n",
    "    for name, (corr, improvement, individual_best) in sorted(successful_quality.items(), key=lambda x: x[1][1], reverse=True):\n",
    "        final_engineered_features[name] = corr\n",
    "        print(f\"  {name}: {corr:.3f} ({improvement:+.1f}% vs individual {individual_best:.3f})\")\n",
    "\n",
    "# Bathroom/room combinations that beat individual components\n",
    "print(\"\\nSUCCESSFUL BATHROOM/ROOM COMBINATIONS:\")\n",
    "if 'successful_bath_room' in locals():\n",
    "    for name, (corr, improvement, individual_best) in sorted(successful_bath_room.items(), key=lambda x: x[1][1], reverse=True):\n",
    "        final_engineered_features[name] = corr\n",
    "        print(f\"  {name}: {corr:.3f} ({improvement:+.1f}% vs individual {individual_best:.3f})\")\n",
    "\n",
    "# Engineering strategy analysis\n",
    "print(f\"\\nENGINEERING STRATEGY ANALYSIS:\")\n",
    "print(f\"Total engineered features selected: {len(final_engineered_features)}\")\n",
    "\n",
    "# Categorize by strategy type\n",
    "addition_features = [name for name in final_engineered_features.keys() if '_add_' in name]\n",
    "multiply_features = [name for name in final_engineered_features.keys() if '_multiply_' in name]\n",
    "standard_features = [name for name in final_engineered_features.keys() if 'Standard' in name or 'All' in name]\n",
    "\n",
    "print(f\"Addition strategy successes: {len(addition_features)}\")\n",
    "print(f\"Multiplication strategy successes: {len(multiply_features)}\")\n",
    "print(f\"Standard formula successes: {len(standard_features)}\")\n",
    "\n",
    "# Create final feature recommendations\n",
    "print(f\"\\nFINAL FEATURE RECOMMENDATIONS:\")\n",
    "print(f\"Top engineered features by correlation strength:\")\n",
    "sorted_features = sorted(final_engineered_features.items(), key=lambda x: x[1], reverse=True)\n",
    "for i, (name, corr) in enumerate(sorted_features, 1):\n",
    "    print(f\"  {i}. {name}: {corr:.3f}\")\n",
    "\n",
    "# Engineering methodology summary\n",
    "print(f\"\\nMETHODOLOGY VALIDATION:\")\n",
    "print(f\"✓ Addition strategy: Effective for area features\")\n",
    "print(f\"✓ Multiplication strategy: Effective for quality-area interactions\")\n",
    "print(f\"✓ Standard formulas: Effective for bathroom calculations\")\n",
    "print(f\"✗ Ratio strategy: Consistently ineffective across all categories\")\n",
    "\n",
    "# CRITICAL: Add successful engineered features to df_combined dataset\n",
    "print(f\"\\nADDING ENGINEERED FEATURES TO DATASET:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "features_added = 0\n",
    "features_skipped = 0\n",
    "\n",
    "# Recreate successful area combinations\n",
    "for name in addition_features:\n",
    "    if '_add_' in name:\n",
    "        parts = name.split('_add_')\n",
    "        if len(parts) == 2:\n",
    "            feat1, feat2 = parts\n",
    "            if feat1 in df_combined.columns and feat2 in df_combined.columns:\n",
    "                df_combined[name] = df_combined[feat1] + df_combined[feat2]\n",
    "                features_added += 1\n",
    "                print(f\"  Added: {name}\")\n",
    "            else:\n",
    "                features_skipped += 1\n",
    "                print(f\"  Skipped: {name} (missing components)\")\n",
    "\n",
    "# Recreate successful quality interactions\n",
    "for name in multiply_features:\n",
    "    if '_multiply_' in name:\n",
    "        parts = name.split('_multiply_')\n",
    "        if len(parts) == 2:\n",
    "            feat1, feat2 = parts\n",
    "            if feat1 in df_combined.columns and feat2 in df_combined.columns:\n",
    "                df_combined[name] = df_combined[feat1] * df_combined[feat2]\n",
    "                features_added += 1\n",
    "                print(f\"  Added: {name}\")\n",
    "            else:\n",
    "                features_skipped += 1\n",
    "                print(f\"  Skipped: {name} (missing components)\")\n",
    "\n",
    "# Recreate successful bathroom and room standard formulas\n",
    "for name in standard_features:\n",
    "    if 'TotalBaths_Standard' in name:\n",
    "        if all(col in df_combined.columns for col in ['FullBath', 'HalfBath']):\n",
    "            df_combined[name] = df_combined['FullBath'] + 0.5 * df_combined['HalfBath']\n",
    "            features_added += 1\n",
    "            print(f\"  Added: {name}\")\n",
    "        else:\n",
    "            features_skipped += 1\n",
    "            print(f\"  Skipped: {name} (missing bathroom components)\")\n",
    "    \n",
    "    elif 'TotalBaths_All' in name:\n",
    "        bathroom_cols = ['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath']\n",
    "        if all(col in df_combined.columns for col in bathroom_cols):\n",
    "            df_combined[name] = (df_combined['FullBath'] + df_combined['BsmtFullBath'] + \n",
    "                               0.5 * (df_combined['HalfBath'] + df_combined['BsmtHalfBath']))\n",
    "            features_added += 1\n",
    "            print(f\"  Added: {name}\")\n",
    "        else:\n",
    "            features_skipped += 1\n",
    "            print(f\"  Skipped: {name} (missing bathroom components)\")\n",
    "    \n",
    "    elif 'TotalRooms_All' in name:\n",
    "        room_cols = ['BedroomAbvGr', 'TotRmsAbvGrd', 'Fireplaces']\n",
    "        if all(col in df_combined.columns for col in room_cols):\n",
    "            df_combined[name] = df_combined['BedroomAbvGr'] + df_combined['TotRmsAbvGrd'] + df_combined['Fireplaces']\n",
    "            features_added += 1\n",
    "            print(f\"  Added: {name}\")\n",
    "        else:\n",
    "            features_skipped += 1\n",
    "            print(f\"  Skipped: {name} (missing room components)\")\n",
    "\n",
    "print(f\"\\nFEATURE ADDITION SUMMARY:\")\n",
    "print(f\"Features successfully added to dataset: {features_added}\")\n",
    "print(f\"Features skipped (missing components): {features_skipped}\")\n",
    "print(f\"Total dataset features: {len(df_combined.columns)}\")\n",
    "\n",
    "# Validate feature addition\n",
    "new_feature_count = len(df_combined.columns)\n",
    "print(f\"\\nValidation: Dataset now contains {new_feature_count} features including {features_added} new engineered features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Systematic feature engineering produces 24 successful combinations beating individual component baselines with improvements ranging 0.7% to 30.8%.\n",
    "Successful engineered features added to df_combined dataset ensuring availability for subsequent analysis sections, with strategy validation confirming addition (12 features) and multiplication (9 features) effectiveness while ratio approaches fail consistently.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Age-Based and Temporal Feature Engineering\n",
    "\n",
    "Calculate property age using sale year reference and create lifecycle-based features.\n",
    "Handle temporal anomalies and implement remodel recency calculations for property improvement analysis.\n",
    "\n",
    "### 3.1 Age Calculation Feature Creation\n",
    "\n",
    "Calculate property age using sale year reference and investigate temporal data quality issues.\n",
    "Implement systematic age feature creation with data-driven anomaly detection and correction pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic age feature creation using sale year reference\n",
    "age_features = {}\n",
    "\n",
    "# Create initial age features without assumptions\n",
    "if all(col in df_combined.columns for col in ['YrSold', 'YearBuilt']):\n",
    "    df_combined['PropertyAge'] = df_combined['YrSold'] - df_combined['YearBuilt']\n",
    "    age_features['PropertyAge'] = df_combined['PropertyAge']\n",
    "\n",
    "if all(col in df_combined.columns for col in ['YrSold', 'GarageYrBlt']):\n",
    "    # Fill missing garage years with 0 for calculation\n",
    "    df_combined['GarageYrBlt'].fillna(0, inplace=True)\n",
    "    df_combined['GarageAge'] = df_combined['YrSold'] - df_combined['GarageYrBlt']\n",
    "    age_features['GarageAge'] = df_combined['GarageAge']\n",
    "\n",
    "if all(col in df_combined.columns for col in ['YrSold', 'YearRemodAdd']):\n",
    "    df_combined['RemodAge'] = df_combined['YrSold'] - df_combined['YearRemodAdd']\n",
    "    age_features['RemodAge'] = df_combined['RemodAge']\n",
    "\n",
    "# Display initial ranges to show data quality issues\n",
    "print(\"Initial age feature ranges:\")\n",
    "for feature_name, feature_data in age_features.items():\n",
    "    print(f\"{feature_name}: {feature_data.min():.0f} to {feature_data.max():.0f} years\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age calculation produces negative values (-1 to -2 years) and extreme garage ages (2010 years) requiring investigation.\n",
    "Initial ranges: PropertyAge (-1 to 136), GarageAge (-200 to 2010), RemodAge (-2 to 60) indicating data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality investigation for temporal anomalies\n",
    "print(\"TEMPORAL DATA QUALITY INVESTIGATION:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. NEGATIVE PROPERTY AGE INVESTIGATION\n",
    "negative_property = df_combined['PropertyAge'] < 0\n",
    "print(f\"1. Properties with negative PropertyAge: {negative_property.sum()}\")\n",
    "if negative_property.any():\n",
    "    negative_cases = df_combined[negative_property][['Id', 'YrSold', 'YearBuilt', 'PropertyAge']]\n",
    "    print(\"   Future construction cases:\")\n",
    "    for _, row in negative_cases.iterrows():\n",
    "        print(f\"   ID {row['Id']}: Sale {row['YrSold']}, Built {row['YearBuilt']}, Age {row['PropertyAge']}\")\n",
    "\n",
    "# 2. NEGATIVE GARAGE AGE INVESTIGATION\n",
    "negative_garage = df_combined['GarageAge'] < 0\n",
    "print(f\"\\n2. Properties with negative GarageAge: {negative_garage.sum()}\")\n",
    "if negative_garage.any():\n",
    "    garage_cases = df_combined[negative_garage][['Id', 'YrSold', 'GarageYrBlt', 'GarageAge']]\n",
    "    print(\"   Future garage construction cases:\")\n",
    "    for _, row in garage_cases.iterrows():\n",
    "        print(f\"   ID {row['Id']}: Sale {row['YrSold']}, Garage {row['GarageYrBlt']}, Age {row['GarageAge']}\")\n",
    "\n",
    "# 3. NEGATIVE REMODEL AGE INVESTIGATION\n",
    "negative_remodel = df_combined['RemodAge'] < 0\n",
    "print(f\"\\n3. Properties with negative RemodAge: {negative_remodel.sum()}\")\n",
    "if negative_remodel.any():\n",
    "    remodel_cases = df_combined[negative_remodel][['Id', 'YrSold', 'YearRemodAdd', 'RemodAge']]\n",
    "    print(\"   Future remodel cases:\")\n",
    "    for _, row in remodel_cases.iterrows():\n",
    "        print(f\"   ID {row['Id']}: Sale {row['YrSold']}, Remodel {row['YearRemodAdd']}, Age {row['RemodAge']}\")\n",
    "\n",
    "# 4. TEMPORAL ANOMALY INVESTIGATION (Garages built before houses)\n",
    "if all(col in df_combined.columns for col in ['GarageYrBlt', 'YearBuilt']):\n",
    "    has_garage_year = df_combined['GarageYrBlt'] > 0\n",
    "    garage_before_house = (df_combined['GarageYrBlt'] < df_combined['YearBuilt']) & has_garage_year\n",
    "\n",
    "    print(f\"\\n4. Properties with garages built BEFORE houses: {garage_before_house.sum()}\")\n",
    "    if garage_before_house.any():\n",
    "        temporal_cases = df_combined[garage_before_house][['Id', 'YearBuilt', 'GarageYrBlt']].head(10)\n",
    "        print(\"   Temporal anomaly cases:\")\n",
    "        for _, row in temporal_cases.iterrows():\n",
    "            years_diff = row['YearBuilt'] - row['GarageYrBlt']\n",
    "            print(f\"   ID {row['Id']}: House {row['YearBuilt']}, Garage {row['GarageYrBlt']} ({years_diff} years difference)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigation reveals 1 future construction (ID 2550), 2 future remodels (IDs 2296, 2550), 1 data entry error (ID 2593: 2207→2007), and 18 garages built before houses.\n",
    "Most anomalies involve ID 2550 with multiple future dates, while garages-before-houses represent legitimate historical construction patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply data-driven corrections based on investigation findings\n",
    "print(\"\\nAPPLYING TARGETED CORRECTIONS:\")\n",
    "\n",
    "# First fix obvious data entry errors\n",
    "if 'GarageYrBlt' in df_combined.columns:\n",
    "    # Fix ID 2593: 2207 → 2007 (identified in investigation)\n",
    "    error_2207 = df_combined['GarageYrBlt'] == 2207\n",
    "    if error_2207.any():\n",
    "        df_combined.loc[error_2207, 'GarageYrBlt'] = 2007\n",
    "        print(f\"Fixed data entry error: 2207 → 2007 for {error_2207.sum()} properties\")\n",
    "\n",
    "# Test two approaches for \"no garage\" (GarageYrBlt=0) properties\n",
    "print(\"\\nTesting garage age calculation approaches:\")\n",
    "\n",
    "# Approach 1: GarageAge = PropertyAge for no garage\n",
    "df_test1 = df_combined.copy()\n",
    "missing_garage_1 = df_test1['GarageYrBlt'] == 0\n",
    "df_test1.loc[missing_garage_1, 'GarageYrBlt'] = df_test1.loc[missing_garage_1, 'YearBuilt']\n",
    "df_test1['GarageAge'] = df_test1['YrSold'] - df_test1['GarageYrBlt']\n",
    "\n",
    "# Approach 2: GarageAge = 0 for no garage\n",
    "df_test2 = df_combined.copy()\n",
    "df_test2['GarageAge'] = df_test2['YrSold'] - df_test2['GarageYrBlt']\n",
    "missing_garage_2 = df_test2['GarageYrBlt'] == 0\n",
    "df_test2.loc[missing_garage_2, 'GarageAge'] = 0\n",
    "\n",
    "# Compare correlations (train data only)\n",
    "train_mask = df_combined['dataset_source'] == 'train'\n",
    "target_col = 'SalePrice_log' if 'SalePrice_log' in df_train_clean.columns else 'SalePrice'\n",
    "\n",
    "if target_col in df_train_clean.columns and train_mask.sum() == len(df_train_clean):\n",
    "    target_data = df_train_clean[target_col]\n",
    "    corr1 = df_test1.loc[train_mask, 'GarageAge'].corr(target_data)\n",
    "    corr2 = df_test2.loc[train_mask, 'GarageAge'].corr(target_data)\n",
    "\n",
    "    print(f\"Approach 1 (GarageAge = PropertyAge for no garage): {corr1:.3f}\")\n",
    "    print(f\"Approach 2 (GarageAge = 0 for no garage): {corr2:.3f}\")\n",
    "\n",
    "    # Apply the better approach\n",
    "    if abs(corr1) > abs(corr2):\n",
    "        print(\"Using Approach 1: GarageAge = PropertyAge for no garage\")\n",
    "        df_combined = df_test1.copy()\n",
    "        chosen_approach = \"PropertyAge\"\n",
    "    else:\n",
    "        print(\"Using Approach 2: GarageAge = 0 for no garage\")\n",
    "        df_combined = df_test2.copy()\n",
    "        chosen_approach = \"Zero\"\n",
    "else:\n",
    "    print(\"Using default Approach 2: GarageAge = 0 for no garage\")\n",
    "    df_combined['GarageAge'] = df_combined['YrSold'] - df_combined['GarageYrBlt']\n",
    "    missing_garage = df_combined['GarageYrBlt'] == 0\n",
    "    df_combined.loc[missing_garage, 'GarageAge'] = 0\n",
    "    chosen_approach = \"Zero\"\n",
    "\n",
    "# Age clipping: handle negative ages with logical approach\n",
    "# Future construction/remodel treated as brand new (0 years old)\n",
    "clipped_property = (df_combined['PropertyAge'] < 0).sum()\n",
    "clipped_garage = (df_combined['GarageAge'] < 0).sum()\n",
    "clipped_remodel = (df_combined['RemodAge'] < 0).sum()\n",
    "\n",
    "df_combined['PropertyAge'] = df_combined['PropertyAge'].clip(lower=0)\n",
    "df_combined['GarageAge'] = df_combined['GarageAge'].clip(lower=0)\n",
    "df_combined['RemodAge'] = df_combined['RemodAge'].clip(lower=0)\n",
    "\n",
    "print(f\"Age clipping applied: {clipped_property} property, {clipped_garage} garage, {clipped_remodel} remodel\")\n",
    "\n",
    "# Fix missing garage year logic: no garage year = garage age equals property age\n",
    "missing_garage_mask = df_combined['GarageYrBlt'] == 0\n",
    "df_combined.loc[missing_garage_mask, 'GarageAge'] = df_combined.loc[missing_garage_mask, 'PropertyAge']\n",
    "\n",
    "# Create remodel indicator before dropping year features\n",
    "df_combined['IsRemodeled'] = (df_combined['YearRemodAdd'] != df_combined['YearBuilt']).astype(int)\n",
    "age_features['IsRemodeled'] = df_combined['IsRemodeled']\n",
    "\n",
    "# Drop original year features - age features provide better interpretability\n",
    "print(\"\\nReplacing year features with age features for better interpretability:\")\n",
    "print(\"Dropping: YearBuilt, GarageYrBlt, YearRemodAdd\")\n",
    "\n",
    "year_features_to_drop = ['YearBuilt', 'GarageYrBlt', 'YearRemodAdd']\n",
    "features_before = len(df_combined.columns)\n",
    "df_combined = df_combined.drop(columns=year_features_to_drop, errors='ignore')\n",
    "features_after = len(df_combined.columns)\n",
    "print(f\"Features: {features_before} → {features_after} (removed {features_before - features_after} year features)\")\n",
    "\n",
    "# Final age feature correlation analysis\n",
    "print(\"\\nFINAL AGE FEATURE PERFORMANCE:\")\n",
    "train_mask = df_combined['dataset_source'] == 'train'\n",
    "target_data = df_train_clean[target_col] if 'df_train_clean' in locals() else None\n",
    "\n",
    "if target_data is not None:\n",
    "    age_correlations = {}\n",
    "    for feature_name in ['PropertyAge', 'GarageAge', 'RemodAge', 'IsRemodeled']:\n",
    "        if feature_name in df_combined.columns:\n",
    "            train_feature = df_combined.loc[train_mask, feature_name]\n",
    "            if len(train_feature) == len(target_data):\n",
    "                corr = train_feature.corr(target_data)\n",
    "                age_correlations[feature_name] = corr\n",
    "                print(f\"{feature_name}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design-time redundancy prevention applied: original year features dropped after age feature creation for better interpretability.\n",
    "Age-based temporal features demonstrate strong depreciation patterns (PropertyAge: -0.588, GarageAge: -0.570, RemodAge: -0.569) while eliminating correlation redundancy through immediate feature replacement strategy.\n",
    "\n",
    "### 3.2 Remodel and Improvement Recency\n",
    "\n",
    "Calculate years since last remodel and create improvement recency features.\n",
    "Test interaction between property age and remodel recency for renovation impact analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remodel recency calculation (years since last remodel)\n",
    "remodel_features = {}\n",
    "\n",
    "# Years since last remodel (using existing RemodAge as base)\n",
    "if 'RemodAge' in df_combined.columns:\n",
    "    # RemodAge is years since remodel (YrSold - YearRemodAdd)\n",
    "    # Convert to positive \"years since improvement\" for better interpretation\n",
    "    df_combined['YearsSinceRemod'] = df_combined['RemodAge']\n",
    "    remodel_features['YearsSinceRemod'] = df_combined['YearsSinceRemod']\n",
    "\n",
    "# Remodel recency categories (recent vs old improvements)\n",
    "if 'YearsSinceRemod' in df_combined.columns:\n",
    "    # Recent remodel: within 10 years of sale\n",
    "    df_combined['RecentRemod'] = (df_combined['YearsSinceRemod'] <= 10).astype(int)\n",
    "    remodel_features['RecentRemod'] = df_combined['RecentRemod']\n",
    "\n",
    "    # Very recent remodel: within 5 years of sale\n",
    "    df_combined['VeryRecentRemod'] = (df_combined['YearsSinceRemod'] <= 5).astype(int)\n",
    "    remodel_features['VeryRecentRemod'] = df_combined['VeryRecentRemod']\n",
    "\n",
    "# Property improvement intensity (remodel frequency)\n",
    "# Calculate improvement gap using existing age features (PropertyAge - RemodAge)\n",
    "if all(col in df_combined.columns for col in ['PropertyAge', 'RemodAge']):\n",
    "    # Gap = time between construction and first remodel\n",
    "    df_combined['ImprovementGap'] = df_combined['PropertyAge'] - df_combined['RemodAge']\n",
    "    # Properties with gap=0 were never remodeled\n",
    "    remodel_features['ImprovementGap'] = df_combined['ImprovementGap']\n",
    "\n",
    "# Display remodel feature distributions\n",
    "print(\"REMODEL RECENCY FEATURE ANALYSIS:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for feature_name, feature_data in remodel_features.items():\n",
    "    print(f\"\\n{feature_name}:\")\n",
    "    if feature_data.dtype in ['int64', 'float64']:\n",
    "        print(f\"  Range: {feature_data.min():.0f} to {feature_data.max():.0f}\")\n",
    "        print(f\"  Mean: {feature_data.mean():.1f}\")\n",
    "    else:\n",
    "        print(f\"  Value counts: {feature_data.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remodel recency features reveal 40% of properties have recent improvements (≤10 years) with 25-year average since last renovation.\n",
    "ImprovementGap negative value indicates data quality issue requiring investigation before correlation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First investigate the negative ImprovementGap value\n",
    "print(\"IMPROVEMENT GAP INVESTIGATION:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "negative_gap = df_combined['ImprovementGap'] < 0\n",
    "print(f\"Properties with negative ImprovementGap: {negative_gap.sum()}\")\n",
    "if negative_gap.any():\n",
    "    gap_cases = df_combined[negative_gap][['Id', 'PropertyAge', 'RemodAge', 'ImprovementGap']].head(5)\n",
    "    print(\"Sample negative gap cases:\")\n",
    "    for _, row in gap_cases.iterrows():\n",
    "        print(f\"  ID {row['Id']}: PropertyAge {row['PropertyAge']}, RemodAge {row['RemodAge']}, Gap {row['ImprovementGap']}\")\n",
    "\n",
    "# Fix negative gaps (remodel before build year - data quality issue)\n",
    "df_combined['ImprovementGap'] = df_combined['ImprovementGap'].clip(lower=0)\n",
    "print(f\"Negative gaps clipped to 0, new range: {df_combined['ImprovementGap'].min()} to {df_combined['ImprovementGap'].max()}\")\n",
    "\n",
    "# Calculate remodel feature correlations with target (train data only)\n",
    "print(\"\\nREMODEL FEATURE CORRELATIONS:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "train_mask = df_combined['dataset_source'] == 'train'\n",
    "target_col = 'SalePrice_log' if 'SalePrice_log' in df_train_clean.columns else 'SalePrice'\n",
    "\n",
    "if target_col in df_train_clean.columns:\n",
    "    target_data = df_train_clean[target_col]\n",
    "    remodel_correlations = {}\n",
    "\n",
    "    for feature_name in ['YearsSinceRemod', 'RecentRemod', 'VeryRecentRemod', 'ImprovementGap']:\n",
    "        if feature_name in df_combined.columns:\n",
    "            train_feature = df_combined.loc[train_mask, feature_name]\n",
    "            if len(train_feature) == len(target_data):\n",
    "                corr = train_feature.corr(target_data)\n",
    "                remodel_correlations[feature_name] = corr\n",
    "                print(f\"{feature_name}: {corr:.3f}\")\n",
    "\n",
    "# Age-remodel interaction features\n",
    "print(\"\\nAGE-REMODEL INTERACTION FEATURES:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Property lifecycle stage: age vs remodel gap ratio\n",
    "if all(col in df_combined.columns for col in ['PropertyAge', 'ImprovementGap']):\n",
    "    # Avoid division by zero for properties never remodeled\n",
    "    mask = df_combined['ImprovementGap'] > 0\n",
    "    df_combined['LifecycleRatio'] = 0  # Default for never remodeled\n",
    "    df_combined.loc[mask, 'LifecycleRatio'] = (\n",
    "        df_combined.loc[mask, 'PropertyAge'] / df_combined.loc[mask, 'ImprovementGap']\n",
    "    )\n",
    "\n",
    "    # Calculate correlation for lifecycle ratio\n",
    "    if target_col in df_train_clean.columns:\n",
    "        train_lifecycle = df_combined.loc[train_mask, 'LifecycleRatio']\n",
    "        lifecycle_corr = train_lifecycle.corr(target_data)\n",
    "        print(f\"LifecycleRatio: {lifecycle_corr:.3f}\")\n",
    "\n",
    "# Renovation freshness: property age adjusted by recent improvements\n",
    "if all(col in df_combined.columns for col in ['PropertyAge', 'RecentRemod']):\n",
    "    # Effective age: reduced for recent renovations\n",
    "    df_combined['EffectiveAge'] = df_combined['PropertyAge'] * (1 - 0.3 * df_combined['RecentRemod'])\n",
    "\n",
    "    # Calculate correlation for effective age\n",
    "    if target_col in df_train_clean.columns:\n",
    "        train_effective = df_combined.loc[train_mask, 'EffectiveAge']\n",
    "        effective_corr = train_effective.corr(target_data)\n",
    "        print(f\"EffectiveAge: {effective_corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remodel correlation analysis confirms renovation timing significantly impacts property valuation with RecentRemod (0.426) and EffectiveAge (-0.600) showing strongest predictive relationships.\n",
    "Single data quality issue corrected while lifecycle interaction features demonstrate limited correlation potential for engineering applications.\n",
    "\n",
    "Age-based temporal engineering delivers strong correlation features with EffectiveAge (-0.600) outperforming individual PropertyAge (-0.588) through renovation adjustment logic.\n",
    "Systematic temporal anomaly correction preserves 100% data integrity while RecentRemod (0.426) validates renovation timing importance for property valuation accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Ratio and Efficiency Feature Creation\n",
    "\n",
    "Create garage efficiency features by dividing GarageArea by GarageCars to measure space per vehicle.\n",
    "Test area efficiency ratios (GrLivArea/TotRmsAbvGrd, TotalBsmtSF/BedroomAbvGr) to capture room size patterns.\n",
    "\n",
    "### 4.1 Efficiency Ratio Discovery and Creation\n",
    "\n",
    "Create garage efficiency features by dividing GarageArea by GarageCars to measure space per vehicle.\n",
    "Test area efficiency ratios (GrLivArea/TotRmsAbvGrd, TotalBsmtSF/BedroomAbvGr) to capture room size patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create efficiency ratio features focusing on space utilization metrics\n",
    "# Test garage, room, and area efficiency ratios with baseline comparison\n",
    "\n",
    "efficiency_features = {}\n",
    "\n",
    "print(\"RATIO AND EFFICIENCY FEATURE CREATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Garage efficiency: space per car\n",
    "print(\"\\n1. GARAGE EFFICIENCY RATIOS:\")\n",
    "if all(col in df_combined.columns for col in ['GarageArea', 'GarageCars']):\n",
    "    # Avoid division by zero - only calculate for properties with cars\n",
    "    car_mask = df_combined['GarageCars'] > 0\n",
    "    print(f\"Properties with garage cars: {car_mask.sum()} of {len(df_combined)}\")\n",
    "\n",
    "    if car_mask.sum() > 100:  # Ensure sufficient data\n",
    "        df_combined['GarageArea_per_Car'] = 0  # Default for no cars\n",
    "        df_combined.loc[car_mask, 'GarageArea_per_Car'] = (\n",
    "            df_combined.loc[car_mask, 'GarageArea'] / df_combined.loc[car_mask, 'GarageCars']\n",
    "        )\n",
    "        efficiency_features['GarageArea_per_Car'] = df_combined['GarageArea_per_Car']\n",
    "\n",
    "        # Display efficiency statistics\n",
    "        efficiency_stats = df_combined.loc[car_mask, 'GarageArea_per_Car']\n",
    "        print(f\"Garage efficiency range: {efficiency_stats.min():.0f} to {efficiency_stats.max():.0f} sq ft per car\")\n",
    "        print(f\"Average garage space per car: {efficiency_stats.mean():.0f} sq ft\")\n",
    "\n",
    "# Room efficiency: living area per room\n",
    "print(\"\\n2. ROOM EFFICIENCY RATIOS:\")\n",
    "room_ratios = [\n",
    "    ('GrLivArea', 'TotRmsAbvGrd', 'Living area per room'),\n",
    "    ('GrLivArea', 'BedroomAbvGr', 'Living area per bedroom'),\n",
    "    ('TotalBsmtSF', 'BedroomAbvGr', 'Basement per bedroom')\n",
    "]\n",
    "\n",
    "for area_col, room_col, description in room_ratios:\n",
    "    if all(col in df_combined.columns for col in [area_col, room_col]):\n",
    "        room_mask = df_combined[room_col] > 0\n",
    "        print(f\"\\n{description}:\")\n",
    "        print(f\"Properties with {room_col} > 0: {room_mask.sum()}\")\n",
    "\n",
    "        if room_mask.sum() > 100:\n",
    "            ratio_name = f\"{area_col}_per_{room_col}\"\n",
    "            df_combined[ratio_name] = 0  # Default for zero rooms\n",
    "            df_combined.loc[room_mask, ratio_name] = (\n",
    "                df_combined.loc[room_mask, area_col] / df_combined.loc[room_mask, room_col]\n",
    "            )\n",
    "            efficiency_features[ratio_name] = df_combined[ratio_name]\n",
    "\n",
    "            # Display ratio statistics\n",
    "            ratio_stats = df_combined.loc[room_mask, ratio_name]\n",
    "            print(f\"Range: {ratio_stats.min():.0f} to {ratio_stats.max():.0f}\")\n",
    "            print(f\"Average: {ratio_stats.mean():.0f}\")\n",
    "\n",
    "# Lot utilization: building footprint efficiency\n",
    "print(\"\\n3. LOT UTILIZATION RATIOS:\")\n",
    "lot_ratios = [\n",
    "    ('GrLivArea', 'LotArea', 'Living area utilization'),\n",
    "    ('1stFlrSF', 'LotArea', 'Footprint utilization'),\n",
    "    ('TotalBsmtSF', 'LotArea', 'Basement utilization')\n",
    "]\n",
    "\n",
    "for building_col, lot_col, description in lot_ratios:\n",
    "    if all(col in df_combined.columns for col in [building_col, lot_col]):\n",
    "        lot_mask = df_combined[lot_col] > 0\n",
    "        print(f\"\\n{description}:\")\n",
    "\n",
    "        if lot_mask.sum() > 100:\n",
    "            ratio_name = f\"{building_col}_per_{lot_col}\"\n",
    "            df_combined[ratio_name] = (\n",
    "                df_combined[building_col] / df_combined[lot_col]\n",
    "            )\n",
    "            efficiency_features[ratio_name] = df_combined[ratio_name]\n",
    "\n",
    "            # Display utilization statistics (as percentages)\n",
    "            ratio_stats = df_combined.loc[lot_mask, ratio_name] * 100\n",
    "            print(f\"Range: {ratio_stats.min():.2f}% to {ratio_stats.max():.2f}%\")\n",
    "            print(f\"Average: {ratio_stats.mean():.2f}%\")\n",
    "\n",
    "print(f\"\\nTotal efficiency features created: {len(efficiency_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficiency ratio creation reveals realistic property utilization patterns with 272 sq ft average garage space per car and 18% lot utilization.\n",
    "Room efficiency ratios span wide ranges indicating diverse property layouts requiring correlation analysis for predictive value assessment.\n",
    "\n",
    "### 4.2 Size Normalization Features\n",
    "\n",
    "Create per-square-foot value features and lot utilization ratios for property efficiency analysis.\n",
    "Test building footprint efficiency and land use optimization features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test efficiency ratio correlations with target variable\n",
    "# Compare ratio performance against individual component correlations\n",
    "\n",
    "print(\"EFFICIENCY RATIO CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "train_mask = df_combined['dataset_source'] == 'train'\n",
    "target_col = 'SalePrice_log' if 'SalePrice_log' in df_train_clean.columns else 'SalePrice'\n",
    "\n",
    "if target_col in df_train_clean.columns:\n",
    "    target_data = df_train_clean[target_col]\n",
    "    efficiency_correlations = {}\n",
    "\n",
    "    print(\"\\nEFFICIENCY RATIO CORRELATIONS:\")\n",
    "    for feature_name in efficiency_features.keys():\n",
    "        if feature_name in df_combined.columns:\n",
    "            train_feature = df_combined.loc[train_mask, feature_name]\n",
    "            if len(train_feature) == len(target_data):\n",
    "                corr = train_feature.corr(target_data)\n",
    "                efficiency_correlations[feature_name] = corr\n",
    "                print(f\"{feature_name}: {corr:.3f}\")\n",
    "\n",
    "    # Compare against baseline individual features\n",
    "    print(\"\\nCOMPARISON WITH INDIVIDUAL COMPONENTS:\")\n",
    "    print(\"(Previous correlations from section 2.1)\")\n",
    "    baseline_comparisons = {\n",
    "        'GarageArea_per_Car': ('GarageArea', 'GarageCars'),\n",
    "        'GrLivArea_per_TotRmsAbvGrd': ('GrLivArea', 'TotRmsAbvGrd'),\n",
    "        'GrLivArea_per_BedroomAbvGr': ('GrLivArea', 'BedroomAbvGr'),\n",
    "        'TotalBsmtSF_per_BedroomAbvGr': ('TotalBsmtSF', 'BedroomAbvGr')\n",
    "    }\n",
    "\n",
    "    for ratio_name, (comp1, comp2) in baseline_comparisons.items():\n",
    "        if ratio_name in efficiency_correlations:\n",
    "            ratio_corr = efficiency_correlations[ratio_name]\n",
    "            # Get individual correlations from previous analysis\n",
    "            comp1_corr = area_correlations.get(comp1, bath_room_correlations.get(comp1, 0))\n",
    "            comp2_corr = bath_room_correlations.get(comp2, area_correlations.get(comp2, 0))\n",
    "            best_individual = max(abs(comp1_corr), abs(comp2_corr))\n",
    "\n",
    "            print(f\"\\n{ratio_name}: {ratio_corr:.3f}\")\n",
    "            print(f\"  vs {comp1}: {comp1_corr:.3f}\")\n",
    "            print(f\"  vs {comp2}: {comp2_corr:.3f}\")\n",
    "            print(f\"  Best individual: {best_individual:.3f}\")\n",
    "\n",
    "            if abs(ratio_corr) > best_individual:\n",
    "                improvement = ((abs(ratio_corr) - best_individual) / best_individual) * 100\n",
    "                print(f\"  ✓ Ratio improves by {improvement:.1f}%\")\n",
    "            else:\n",
    "                decline = ((best_individual - abs(ratio_corr)) / best_individual) * 100\n",
    "                print(f\"  ✗ Ratio declines by {decline:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficiency ratio analysis confirms systematic underperformance with all ratios declining 21-69% below individual component correlations.\n",
    "GrLivArea_per_TotRmsAbvGrd (0.569) shows least decline at 21.5% while garage efficiency drops 69.2%, validating ratio strategy ineffectiveness for property valuation modeling.\n",
    "\n",
    "Ratio-based feature engineering demonstrates comprehensive failure across garage, room, and lot utilization categories with 21-69% correlation decline from individual components.\n",
    "Systematic ratio ineffectiveness validates addition and multiplication strategy superiority while confirming space efficiency metrics inadequacy for property valuation prediction accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Distribution Analysis and Transformation Strategy\n",
    "\n",
    "Analyze distribution characteristics of all engineered features and identify transformation requirements.\n",
    "Apply log transformation to highly skewed engineered features for improved model performance.\n",
    "\n",
    "### 5.1 Skewness Detection and Analysis\n",
    "\n",
    "Calculate skewness for all engineered features and identify transformation candidates.\n",
    "Implement systematic transformation testing for features with |skewness| > 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ENGINEERED FEATURE DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Collect all engineered features created in previous sections\n",
    "engineered_feature_list = []\n",
    "\n",
    "# Section 2 features: successful combinations from final_engineered_features\n",
    "if 'final_engineered_features' in locals():\n",
    "    engineered_feature_list.extend(final_engineered_features.keys())\n",
    "    print(f\"Section 2 engineered features: {len(final_engineered_features)}\")\n",
    "else:\n",
    "    print(\"Section 2 engineered features: 0 (run Section 2.5 first to populate final_engineered_features)\")\n",
    "\n",
    "# Section 3 features: collect all age and remodel features created in sections 3.1-3.2\n",
    "print(\"Collecting age and remodel features from Section 3...\")\n",
    "section3_features = []\n",
    "\n",
    "# Collect from age_features (Section 3.1)\n",
    "if 'age_features' in locals():\n",
    "    section3_features.extend(age_features.keys())\n",
    "    \n",
    "# Collect from remodel_features (Section 3.2)\n",
    "if 'remodel_features' in locals():\n",
    "    section3_features.extend(remodel_features.keys())\n",
    "\n",
    "# Add interaction features from Section 3.2\n",
    "interaction_features = ['LifecycleRatio', 'EffectiveAge']\n",
    "section3_features.extend([f for f in interaction_features if f in df_combined.columns])\n",
    "\n",
    "# Filter to only existing features in dataset\n",
    "section3_existing = [f for f in section3_features if f in df_combined.columns]\n",
    "engineered_feature_list.extend(section3_existing)\n",
    "print(f\"Section 3 age/remodel features: {len(section3_existing)}\")\n",
    "\n",
    "# Section 4 features: efficiency ratios\n",
    "if 'efficiency_features' in locals():\n",
    "    engineered_feature_list.extend(efficiency_features.keys())\n",
    "    print(f\"Section 4 efficiency features: {len(efficiency_features)}\")\n",
    "\n",
    "print(f\"\\nTotal engineered features for analysis: {len(engineered_feature_list)}\")\n",
    "\n",
    "# Calculate skewness for all engineered features\n",
    "feature_skewness = {}\n",
    "transformation_candidates = {}\n",
    "\n",
    "print(f\"\\nSKEWNESS ANALYSIS:\")\n",
    "print(f\"Feature Name{' '*25} | Skewness | Status\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for feature_name in engineered_feature_list:\n",
    "    if feature_name in df_combined.columns:\n",
    "        feature_data = df_combined[feature_name]\n",
    "\n",
    "        # Calculate skewness (exclude zero values for ratio features)\n",
    "        if feature_data.nunique() > 1:  # Avoid constant features\n",
    "            skew_value = stats.skew(feature_data.dropna())\n",
    "            feature_skewness[feature_name] = skew_value\n",
    "\n",
    "            # Determine transformation requirement\n",
    "            if abs(skew_value) > 0.5:\n",
    "                status = \"TRANSFORM\" if abs(skew_value) > 1.0 else \"CONSIDER\"\n",
    "                transformation_candidates[feature_name] = skew_value\n",
    "            else:\n",
    "                status = \"NORMAL\"\n",
    "\n",
    "            print(f\"{feature_name:<35} | {skew_value:>8.3f} | {status}\")\n",
    "\n",
    "print(f\"\\nTRANSFORMATION SUMMARY:\")\n",
    "print(f\"Features requiring transformation (|skew| > 1.0): {len([f for f, s in transformation_candidates.items() if abs(s) > 1.0])}\")\n",
    "print(f\"Features to consider (0.5 < |skew| ≤ 1.0): {len([f for f, s in transformation_candidates.items() if 0.5 < abs(s) <= 1.0])}\")\n",
    "print(f\"Normal distribution features (|skew| ≤ 0.5): {len(feature_skewness) - len(transformation_candidates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution analysis reveals 7 features requiring transformation with extreme skewness (LifecycleRatio: 5.597, GrLivArea_per_BedroomAbvGr: 3.039) primarily affecting efficiency ratios.\n",
    "Age-based features show mild skewness while binary indicators maintain normal distributions, establishing clear transformation priorities for modeling optimization.\n",
    "\n",
    "### 5.2 Feature Transformation Implementation\n",
    "\n",
    "Apply log transformation to skewed engineered features and validate distribution improvements.\n",
    "Create final transformed feature set with normality improvements documented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation to highly skewed features\n",
    "# Focus on features with |skewness| > 1.0 for normalization\n",
    "\n",
    "print(\"FEATURE TRANSFORMATION IMPLEMENTATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Identify features requiring transformation (|skew| > 1.0)\n",
    "transform_features = [f for f, s in transformation_candidates.items() if abs(s) > 1.0]\n",
    "print(f\"Features to transform: {len(transform_features)}\")\n",
    "\n",
    "transformation_results = {}\n",
    "transformed_features = {}\n",
    "\n",
    "print(f\"\\nTRANSFORMATION RESULTS:\")\n",
    "print(f\"Feature Name{' '*25} | Before | After  | Status\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for feature_name in transform_features:\n",
    "    if feature_name in df_combined.columns:\n",
    "        original_data = df_combined[feature_name].copy()\n",
    "        original_skew = transformation_candidates[feature_name]\n",
    "\n",
    "        # Apply log transformation (handle zeros by adding small constant)\n",
    "        # Use log1p for features that might contain zeros\n",
    "        if (original_data >= 0).all():\n",
    "            # For non-negative features, use log1p (log(1+x))\n",
    "            transformed_data = np.log1p(original_data)\n",
    "            transform_method = \"log1p\"\n",
    "        else:\n",
    "            # For features with negative values, shift then transform\n",
    "            min_val = original_data.min()\n",
    "            shifted_data = original_data - min_val + 1\n",
    "            transformed_data = np.log(shifted_data)\n",
    "            transform_method = f\"log(x-{min_val:.1f}+1)\"\n",
    "\n",
    "        # Calculate new skewness\n",
    "        new_skew = stats.skew(transformed_data.dropna())\n",
    "\n",
    "        # Create transformed feature name\n",
    "        transformed_name = f\"{feature_name}_log\"\n",
    "        df_combined[transformed_name] = transformed_data\n",
    "        transformed_features[transformed_name] = transformed_data\n",
    "\n",
    "        # Store transformation results\n",
    "        transformation_results[feature_name] = {\n",
    "            'original_skew': original_skew,\n",
    "            'transformed_skew': new_skew,\n",
    "            'improvement': abs(original_skew) - abs(new_skew),\n",
    "            'method': transform_method\n",
    "        }\n",
    "\n",
    "        # Determine transformation success\n",
    "        if abs(new_skew) < abs(original_skew):\n",
    "            status = \"IMPROVED\" if abs(new_skew) < 0.5 else \"BETTER\"\n",
    "        else:\n",
    "            status = \"WORSE\"\n",
    "\n",
    "        print(f\"{feature_name:<35} | {original_skew:>6.2f} | {new_skew:>6.2f} | {status}\")\n",
    "\n",
    "# Transformation effectiveness summary\n",
    "print(f\"\\nTRANSFORMATION EFFECTIVENESS:\")\n",
    "successful = sum(1 for r in transformation_results.values() if r['improvement'] > 0)\n",
    "normalized = sum(1 for r in transformation_results.values() if abs(r['transformed_skew']) < 0.5)\n",
    "\n",
    "print(f\"Features improved: {successful}/{len(transform_features)}\")\n",
    "print(f\"Features normalized (|skew| < 0.5): {normalized}/{len(transform_features)}\")\n",
    "\n",
    "# Test correlation preservation for transformed features\n",
    "print(f\"\\nCORRELATION PRESERVATION ANALYSIS:\")\n",
    "train_mask = df_combined['dataset_source'] == 'train'\n",
    "target_col = 'SalePrice_log' if 'SalePrice_log' in df_train_clean.columns else 'SalePrice'\n",
    "\n",
    "if target_col in df_train_clean.columns:\n",
    "    target_data = df_train_clean[target_col]\n",
    "\n",
    "    for feature_name in transform_features:\n",
    "        if feature_name in df_combined.columns:\n",
    "            # Original correlation\n",
    "            original_feature = df_combined.loc[train_mask, feature_name]\n",
    "            original_corr = original_feature.corr(target_data)\n",
    "\n",
    "            # Transformed correlation\n",
    "            transformed_name = f\"{feature_name}_log\"\n",
    "            if transformed_name in df_combined.columns:\n",
    "                transformed_feature = df_combined.loc[train_mask, transformed_name]\n",
    "                transformed_corr = transformed_feature.corr(target_data)\n",
    "\n",
    "                corr_change = abs(transformed_corr) - abs(original_corr)\n",
    "                status = \"PRESERVED\" if abs(corr_change) < 0.05 else (\"IMPROVED\" if corr_change > 0 else \"DECLINED\")\n",
    "\n",
    "                print(f\"{feature_name:<35}: {original_corr:>6.3f} → {transformed_corr:>6.3f} ({status})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log transformation achieves partial success with 5/7 features improved but none reaching full normalization, while correlation preservation maintains predictive relationships.\n",
    "Selective transformation strategy proves superior with bedroom-based efficiency ratios showing correlation decline, validating targeted normalization over universal application for modeling optimization.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Advanced Feature Selection and Dimensionality Management\n",
    "\n",
    "Analyze correlation matrix for feature redundancy and implement variance-based filtering.\n",
    "Create final optimized feature set balancing predictive power with model complexity.\n",
    "\n",
    "### 6.1 Continuous Feature Correlation Analysis for Redundancy Detection\n",
    "\n",
    "Calculate continuous feature redundancy through correlation analysis excluding one-hot encoded categorical features.\n",
    "Identify high-correlation pairs (>0.95) and establish target correlation framework for redundancy removal decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CONTINUOUS FEATURE CORRELATION ANALYSIS FOR REDUNDANCY\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Get all numerical features for initial filtering\n",
    "all_features = df_combined.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# Remove ID and target-related columns\n",
    "base_features = [col for col in all_features if col not in ['Id', 'SalePrice', 'SalePrice_log', 'dataset_source']]\n",
    "\n",
    "# Filter out one-hot encoded features (binary 0/1 features from categorical encoding)\n",
    "print(\"Filtering one-hot encoded features from correlation analysis...\")\n",
    "continuous_features = []\n",
    "oneshot_features = []\n",
    "\n",
    "for feature in base_features:\n",
    "    feature_data = df_combined[feature]\n",
    "    unique_vals = set(feature_data.dropna().unique())\n",
    "\n",
    "    # Check if feature is binary (one-hot encoded) vs continuous\n",
    "    if unique_vals.issubset({0, 1}) and len(unique_vals) == 2:\n",
    "        oneshot_features.append(feature)\n",
    "    else:\n",
    "        continuous_features.append(feature)\n",
    "\n",
    "print(f\"Total numerical features: {len(base_features)}\")\n",
    "print(f\"One-hot encoded features (excluded): {len(oneshot_features)}\")\n",
    "print(f\"Continuous features for analysis: {len(continuous_features)}\")\n",
    "\n",
    "# Use only continuous features for meaningful correlation analysis\n",
    "analysis_features = continuous_features\n",
    "\n",
    "print(f\"Total features for correlation analysis: {len(analysis_features)}\")\n",
    "\n",
    "# Calculate correlation matrix (using training data only for consistency)\n",
    "train_mask = df_combined['dataset_source'] == 'train'\n",
    "correlation_data = df_combined.loc[train_mask, analysis_features]\n",
    "\n",
    "print(f\"Correlation matrix shape: {correlation_data.shape}\")\n",
    "print(\"Computing pairwise correlations...\")\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = correlation_data.corr()\n",
    "\n",
    "# Find high-correlation pairs (>0.95 threshold for redundancy)\n",
    "high_corr_threshold = 0.95\n",
    "high_corr_pairs = []\n",
    "\n",
    "# Iterate through upper triangle to avoid duplicates\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_value) > high_corr_threshold:\n",
    "            feature1 = correlation_matrix.columns[i]\n",
    "            feature2 = correlation_matrix.columns[j]\n",
    "            high_corr_pairs.append((feature1, feature2, corr_value))\n",
    "\n",
    "print(f\"\\nHigh correlation pairs (|r| > {high_corr_threshold}):\")\n",
    "print(f\"Found {len(high_corr_pairs)} redundant feature pairs\")\n",
    "\n",
    "if high_corr_pairs:\n",
    "    for feature1, feature2, corr_val in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True):\n",
    "        print(f\"  {feature1} ↔ {feature2}: {corr_val:.3f}\")\n",
    "else:\n",
    "    print(\"  No feature pairs exceed redundancy threshold\")\n",
    "\n",
    "# Complete correlation matrix visualization\n",
    "print(f\"\\nVisualizing complete correlation matrix ({len(analysis_features)} features)...\")\n",
    "\n",
    "# Create large correlation matrix plot\n",
    "plt.figure(figsize=(20, 18))\n",
    "sns.heatmap(correlation_matrix,\n",
    "           annot=False,  # Too many features for annotations\n",
    "           cmap='RdBu_r',\n",
    "           center=0,\n",
    "           square=True,\n",
    "           linewidths=0,\n",
    "           cbar_kws={'shrink': 0.6})\n",
    "plt.title(f'Complete Feature Correlation Matrix ({len(analysis_features)} features)')\n",
    "plt.xticks(rotation=90, fontsize=6)\n",
    "plt.yticks(rotation=0, fontsize=6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional focused visualization for high-correlation pairs if they exist\n",
    "if len(high_corr_pairs) > 0:\n",
    "    print(f\"\\nFocused view: High correlation pairs (|r| > {high_corr_threshold})\")\n",
    "\n",
    "    # Extract unique features from high correlation pairs\n",
    "    unique_features = set()\n",
    "    for feat1, feat2, _ in high_corr_pairs:\n",
    "        unique_features.add(feat1)\n",
    "        unique_features.add(feat2)\n",
    "\n",
    "    if len(unique_features) > 1:\n",
    "        unique_features_list = sorted(list(unique_features))\n",
    "        subset_corr = correlation_matrix.loc[unique_features_list, unique_features_list]\n",
    "\n",
    "        plt.figure(figsize=(16, 14))\n",
    "        sns.heatmap(subset_corr,\n",
    "                   annot=False,\n",
    "                   cmap='RdBu_r',\n",
    "                   center=0,\n",
    "                   fmt='.2f',\n",
    "                   square=True,\n",
    "                   linewidths=0.5,\n",
    "                   cbar_kws={'shrink': 0.8})\n",
    "        plt.title(f'High Correlation Features Detail View (|r| > {high_corr_threshold})')\n",
    "        plt.xticks(rotation=45, ha='right', fontsize=9)\n",
    "        plt.yticks(rotation=0, fontsize=9)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"\\nNo high-correlation pairs found - feature set shows good diversity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous feature filtering reduces analysis from 250 total features to 87 continuous features, excluding 163 one-hot encoded categorical features from redundancy detection.\n",
    "Correlation matrix analysis identifies 11 high-correlation pairs (>0.95) including perfect temporal duplication (RemodAge ↔ YearsSinceRemod: 1.000), log transformation pairs (0.972-0.999), and quality feature correlation (GarageQual ↔ GarageCond: 0.959)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target correlation analysis for redundancy removal strategy\n",
    "print(f\"\\nTARGET CORRELATION ANALYSIS FOR REDUNDANCY REMOVAL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "target_col = 'SalePrice_log' if 'SalePrice_log' in df_train_clean.columns else 'SalePrice'\n",
    "if target_col in df_train_clean.columns and len(high_corr_pairs) > 0:\n",
    "    target_data = df_train_clean[target_col]\n",
    "    removal_candidates = []\n",
    "\n",
    "    print(f\"Analyzing {len(high_corr_pairs)} redundant pairs for removal strategy:\")\n",
    "    print(f\"Target variable: {target_col}\")\n",
    "    print()\n",
    "\n",
    "    for feature1, feature2, corr_val in high_corr_pairs:\n",
    "        # Get target correlations for both features\n",
    "        if feature1 in df_combined.columns and feature2 in df_combined.columns:\n",
    "            train_mask = df_combined['dataset_source'] == 'train'\n",
    "\n",
    "            feat1_data = df_combined.loc[train_mask, feature1]\n",
    "            feat2_data = df_combined.loc[train_mask, feature2]\n",
    "\n",
    "            if len(feat1_data) == len(target_data):\n",
    "                corr1 = feat1_data.corr(target_data)\n",
    "                corr2 = feat2_data.corr(target_data)\n",
    "\n",
    "                # Determine which feature to remove (keep stronger target correlation)\n",
    "                if abs(corr1) >= abs(corr2):\n",
    "                    keep_feature = feature1\n",
    "                    remove_feature = feature2\n",
    "                    keep_corr = corr1\n",
    "                    remove_corr = corr2\n",
    "                else:\n",
    "                    keep_feature = feature2\n",
    "                    remove_feature = feature1\n",
    "                    keep_corr = corr2\n",
    "                    remove_corr = corr1\n",
    "\n",
    "                removal_candidates.append(remove_feature)\n",
    "\n",
    "                print(f\"Pair: {feature1} ↔ {feature2} (r={corr_val:.3f})\")\n",
    "                print(f\"  {feature1}: {corr1:.3f} target correlation\")\n",
    "                print(f\"  {feature2}: {corr2:.3f} target correlation\")\n",
    "                print(f\"  → Remove: {remove_feature} (keep {keep_feature})\")\n",
    "                print()\n",
    "\n",
    "    # Create final removal list (remove duplicates)\n",
    "    features_to_remove = list(set(removal_candidates))\n",
    "\n",
    "    print(f\"REDUNDANCY REMOVAL SUMMARY:\")\n",
    "    print(f\"Original features: {len(analysis_features)}\")\n",
    "    print(f\"Features to remove: {len(features_to_remove)}\")\n",
    "    print(f\"Final feature count: {len(analysis_features) - len(features_to_remove)}\")\n",
    "    print()\n",
    "    print(\"Features marked for removal:\")\n",
    "    for i, feature in enumerate(sorted(features_to_remove), 1):\n",
    "        print(f\"  {i:2d}. {feature}\")\n",
    "\n",
    "# Additional hierarchical redundancy detection for engineered features\n",
    "print(f\"\\nHIERARCHICAL REDUNDANCY ANALYSIS:\")\n",
    "print(\"=\" * 35)\n",
    "print(\"Comparing engineered features vs individual components...\")\n",
    "\n",
    "hierarchical_removals = []\n",
    "\n",
    "# Bathroom hierarchy: TotalBaths features vs individual bathroom components\n",
    "bathroom_engineered = ['TotalBaths_Standard', 'TotalBaths_All']\n",
    "bathroom_individuals = ['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath']\n",
    "\n",
    "# Check if engineered bathroom features exist and get their correlations\n",
    "bathroom_eng_corrs = {}\n",
    "for feat in bathroom_engineered:\n",
    "    if feat in df_combined.columns:\n",
    "        train_mask = df_combined['dataset_source'] == 'train'\n",
    "        feat_data = df_combined.loc[train_mask, feat]\n",
    "        if len(feat_data) == len(target_data):\n",
    "            corr = feat_data.corr(target_data)\n",
    "            bathroom_eng_corrs[feat] = corr\n",
    "\n",
    "# Get individual bathroom correlations\n",
    "bathroom_ind_corrs = {}\n",
    "for feat in bathroom_individuals:\n",
    "    if feat in df_combined.columns:\n",
    "        train_mask = df_combined['dataset_source'] == 'train'\n",
    "        feat_data = df_combined.loc[train_mask, feat]\n",
    "        if len(feat_data) == len(target_data):\n",
    "            corr = feat_data.corr(target_data)\n",
    "            bathroom_ind_corrs[feat] = corr\n",
    "\n",
    "if bathroom_eng_corrs and bathroom_ind_corrs:\n",
    "    best_engineered = max(bathroom_eng_corrs.values())\n",
    "    best_individual = max(bathroom_ind_corrs.values())\n",
    "\n",
    "    print(f\"\\nBATHROOM FEATURE HIERARCHY:\")\n",
    "    print(f\"Best engineered bathroom: {best_engineered:.3f} (TotalBaths)\")\n",
    "    print(f\"Best individual bathroom: {best_individual:.3f}\")\n",
    "\n",
    "    if best_engineered > best_individual:\n",
    "        print(f\"→ Engineered bathroom features outperform individual components\")\n",
    "        print(f\"→ Marking individual bathroom features for removal:\")\n",
    "        for feat, corr in bathroom_ind_corrs.items():\n",
    "            if feat not in features_to_remove:  # Don't duplicate removals\n",
    "                hierarchical_removals.append(feat)\n",
    "                print(f\"  Remove {feat} (correlation: {corr:.3f})\")\n",
    "\n",
    "# Update final removal list\n",
    "if hierarchical_removals:\n",
    "    total_removals = len(features_to_remove) + len(hierarchical_removals)\n",
    "    print(f\"\\nHIERARCHICAL REMOVAL SUMMARY:\")\n",
    "    print(f\"Additional features to remove: {len(hierarchical_removals)}\")\n",
    "    print(f\"Total features to remove: {total_removals}\")\n",
    "    print(f\"Final feature count: {len(analysis_features) - total_removals}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous feature correlation analysis successfully identifies 11 high-correlation pairs requiring removal including perfect temporal duplication (RemodAge ↔ YearsSinceRemod), 7 log transformation pairs (0.972-0.999 correlation), and quality feature redundancy (GarageQual ↔ GarageCond).\n",
    "Target correlation analysis systematically removes 11 redundant features (87→76 features) with key decisions: keep GrLivArea_log (0.737) over GrLivArea (0.725), EffectiveAge (-0.600) over PropertyAge (-0.588), and RemodAge over identical YearsSinceRemod, optimizing predictive power while eliminating redundancy.\n",
    "\n",
    "### 6.2 Hierarchical Feature Redundancy Analysis\n",
    "\n",
    "Systematically compare engineered features against their original components through correlation analysis.\n",
    "Determine which individual components can be removed when engineered combinations outperform for feature optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HIERARCHICAL FEATURE REDUNDANCY ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Initialize analysis framework\n",
    "train_mask = df_combined['dataset_source'] == 'train'\n",
    "target_col = 'SalePrice_log' if 'SalePrice_log' in df_train_clean.columns else 'SalePrice'\n",
    "target_data = df_train_clean[target_col]\n",
    "\n",
    "hierarchical_removals = []\n",
    "removal_analysis = {}\n",
    "\n",
    "# 1. BATHROOM FEATURE HIERARCHY ANALYSIS\n",
    "print(\"\\n1. BATHROOM FEATURE HIERARCHY:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "bathroom_engineered = ['TotalBaths_Standard', 'TotalBaths_All']\n",
    "bathroom_individuals = ['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath']\n",
    "\n",
    "bathroom_eng_corrs = {}\n",
    "bathroom_ind_corrs = {}\n",
    "\n",
    "# Get engineered bathroom correlations (train data only)\n",
    "for feat in bathroom_engineered:\n",
    "    if feat in df_combined.columns:\n",
    "        feat_data = df_combined.loc[train_mask, feat]\n",
    "        if len(feat_data) == len(target_data):\n",
    "            corr = feat_data.corr(target_data)\n",
    "            bathroom_eng_corrs[feat] = corr\n",
    "            print(f\"Engineered: {feat}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(f\"Missing engineered feature: {feat} (not found in dataset)\")\n",
    "\n",
    "# Get individual bathroom correlations (train data only)\n",
    "for feat in bathroom_individuals:\n",
    "    if feat in df_combined.columns:\n",
    "        feat_data = df_combined.loc[train_mask, feat]\n",
    "        if len(feat_data) == len(target_data):\n",
    "            corr = feat_data.corr(target_data)\n",
    "            bathroom_ind_corrs[feat] = corr\n",
    "            print(f\"Individual: {feat}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(f\"Missing individual feature: {feat} (not found in dataset)\")\n",
    "\n",
    "# Bathroom hierarchy decision analysis\n",
    "if bathroom_eng_corrs and bathroom_ind_corrs:\n",
    "    best_engineered = max(bathroom_eng_corrs.values())\n",
    "    best_engineered_name = max(bathroom_eng_corrs, key=bathroom_eng_corrs.get)\n",
    "    best_individual = max(bathroom_ind_corrs.values())\n",
    "    \n",
    "    improvement_pct = (best_engineered - best_individual) / best_individual * 100\n",
    "    print(f\"\\nBest engineered bathroom feature: {best_engineered_name} ({best_engineered:.3f})\")\n",
    "    print(f\"Best individual bathroom feature: {best_individual:.3f}\")\n",
    "    print(f\"Improvement: {improvement_pct:+.1f}%\")\n",
    "\n",
    "    # Data-driven decision based on performance gap\n",
    "    if best_engineered > best_individual and improvement_pct > 10:\n",
    "        print(\"→ Analysis: Superior engineered bathroom feature identified\")\n",
    "        print(\"→ Decision: Remove individual bathroom features and inferior engineered feature\")\n",
    "        \n",
    "        # Remove individual bathroom features\n",
    "        for feat in bathroom_individuals:\n",
    "            if feat in df_combined.columns:\n",
    "                hierarchical_removals.append(feat)\n",
    "                removal_analysis[feat] = f\"Replaced by {best_engineered_name} ({best_engineered:.3f} vs {bathroom_ind_corrs.get(feat, 0):.3f})\"\n",
    "                print(f\"  Remove: {feat} (correlation: {bathroom_ind_corrs.get(feat, 0):.3f})\")\n",
    "        \n",
    "        # Remove inferior engineered bathroom feature\n",
    "        for eng_feat, eng_corr in bathroom_eng_corrs.items():\n",
    "            if eng_feat != best_engineered_name:\n",
    "                hierarchical_removals.append(eng_feat)\n",
    "                removal_analysis[eng_feat] = f\"Replaced by superior engineered feature {best_engineered_name} ({best_engineered:.3f} vs {eng_corr:.3f})\"\n",
    "                print(f\"  Remove: {eng_feat} (correlation: {eng_corr:.3f})\")\n",
    "    else:\n",
    "        print(\"→ Decision: Keep individual bathroom features\")\n",
    "\n",
    "# 2. AREA COMBINATION ANALYSIS\n",
    "print(f\"\\n2. AREA COMBINATION ANALYSIS:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Check major area combinations vs their components\n",
    "area_combinations = [\n",
    "    ('GrLivArea_add_TotalBsmtSF', ['GrLivArea', 'TotalBsmtSF']),\n",
    "    ('WoodDeckSF_add_OpenPorchSF', ['WoodDeckSF', 'OpenPorchSF']),\n",
    "    ('GarageArea_add_TotalBsmtSF', ['GarageArea', 'TotalBsmtSF'])\n",
    "]\n",
    "\n",
    "for combo_name, components in area_combinations:\n",
    "    if combo_name in df_combined.columns:\n",
    "        combo_data = df_combined.loc[train_mask, combo_name]\n",
    "        combo_corr = combo_data.corr(target_data)\n",
    "\n",
    "        component_corrs = {}\n",
    "        for comp in components:\n",
    "            if comp in df_combined.columns:\n",
    "                comp_data = df_combined.loc[train_mask, comp]\n",
    "                comp_corr = comp_data.corr(target_data)\n",
    "                component_corrs[comp] = comp_corr\n",
    "\n",
    "        best_component = max(component_corrs.values()) if component_corrs else 0\n",
    "\n",
    "        print(f\"\\nCombination: {combo_name}: {combo_corr:.3f}\")\n",
    "        for comp, corr in component_corrs.items():\n",
    "            print(f\"  Component: {comp}: {corr:.3f}\")\n",
    "\n",
    "        print(f\"Best component: {best_component:.3f}\")\n",
    "        improvement = (combo_corr - best_component) / best_component * 100 if best_component > 0 else 0\n",
    "        print(f\"Improvement: {improvement:+.1f}%\")\n",
    "\n",
    "        # Decision: Keep components for area features (they provide complementary information)\n",
    "        print(\"→ Decision: Keep individual area components (complementary information)\")\n",
    "\n",
    "# 3. QUALITY INTERACTION ANALYSIS\n",
    "print(f\"\\n3. QUALITY INTERACTION ANALYSIS:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Check top quality interactions vs their components\n",
    "quality_interactions = [\n",
    "    ('OverallQual_multiply_GrLivArea', ['OverallQual', 'GrLivArea']),\n",
    "    ('ExterQual_multiply_GrLivArea', ['ExterQual', 'GrLivArea']),\n",
    "    ('BsmtQual_multiply_TotalBsmtSF', ['BsmtQual', 'TotalBsmtSF'])\n",
    "]\n",
    "\n",
    "for interaction_name, components in quality_interactions:\n",
    "    if interaction_name in df_combined.columns:\n",
    "        interaction_data = df_combined.loc[train_mask, interaction_name]\n",
    "        interaction_corr = interaction_data.corr(target_data)\n",
    "\n",
    "        component_corrs = {}\n",
    "        for comp in components:\n",
    "            if comp in df_combined.columns:\n",
    "                comp_data = df_combined.loc[train_mask, comp]\n",
    "                comp_corr = comp_data.corr(target_data)\n",
    "                component_corrs[comp] = comp_corr\n",
    "\n",
    "        best_component = max(component_corrs.values()) if component_corrs else 0\n",
    "\n",
    "        print(f\"\\nInteraction: {interaction_name}: {interaction_corr:.3f}\")\n",
    "        for comp, corr in component_corrs.items():\n",
    "            print(f\"  Component: {comp}: {corr:.3f}\")\n",
    "\n",
    "        improvement = (interaction_corr - best_component) / best_component * 100 if best_component > 0 else 0\n",
    "        print(f\"Improvement: {improvement:+.1f}%\")\n",
    "\n",
    "        # Decision: Keep components for quality features (interpretability + model flexibility)\n",
    "        print(\"→ Decision: Keep individual quality components (interpretability + flexibility)\")\n",
    "\n",
    "# 4. FAILED FEATURE REMOVAL\n",
    "print(f\"\\n4. FAILED FEATURE REMOVAL:\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Remove all efficiency ratio features (already proven to fail)\n",
    "efficiency_ratios = [\n",
    "    'GarageArea_per_Car', 'GrLivArea_per_TotRmsAbvGrd', 'GrLivArea_per_BedroomAbvGr',\n",
    "    'TotalBsmtSF_per_BedroomAbvGr', 'GrLivArea_per_LotArea', '1stFlrSF_per_LotArea'\n",
    "]\n",
    "\n",
    "print(\"Removing failed efficiency ratio features:\")\n",
    "for feat in efficiency_ratios:\n",
    "    if feat in df_combined.columns:\n",
    "        hierarchical_removals.append(feat)\n",
    "        removal_analysis[feat] = \"Systematic underperformance (21-69% correlation decline)\"\n",
    "        print(f\"  Remove: {feat} (ratio strategy failure)\")\n",
    "\n",
    "# FINAL REMOVAL SUMMARY\n",
    "print(f\"\\nHIERARCHICAL REDUNDANCY REMOVAL SUMMARY:\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"Features marked for removal: {len(hierarchical_removals)}\")\n",
    "print(\"\\nRemoval analysis results:\")\n",
    "for feat in hierarchical_removals:\n",
    "    reason = removal_analysis.get(feat, \"Replaced by superior engineered feature\")\n",
    "    print(f\"  {feat}: {reason}\")\n",
    "\n",
    "# Store removal list for Section 7 implementation\n",
    "print(f\"\\nFeature removal list prepared for systematic implementation.\")\n",
    "print(f\"Total features to remove: {len(hierarchical_removals)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical feature redundancy analysis removes 11 features: TotalBaths_All (0.677) replaces 4 individual bathroom components plus TotalBaths_Standard (0.641), while 6 failed efficiency ratio features removed due to systematic underperformance.\n",
    "Area and quality combinations preserved alongside individual components for model interpretability despite strong improvements (13-31% and up to 15% respectively).\n",
    "\n",
    "\n",
    "Systematic removal of 6 failed efficiency ratio features confirms ratio strategy ineffectiveness, with final hierarchical analysis removing 11 redundant features (5 bathroom + 6 efficiency ratios) while preserving optimal engineered combinations.\n",
    "\n",
    "### 6.3 Variance Filtering and Final Selection\n",
    "\n",
    "Apply variance threshold filtering to remove near-constant features from cleaned continuous feature set.\n",
    "Create optimized feature set ready for model-driven selection and performance testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features with minimal variance that provide little predictive value\n",
    "print(\"VARIANCE THRESHOLD FILTERING\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Use conservative threshold to remove only truly constant features\n",
    "variance_threshold = 0.01\n",
    "variance_filter = VarianceThreshold(threshold=variance_threshold)\n",
    "\n",
    "# Fit variance filter on training data only for proper ML workflow\n",
    "train_mask = df_combined['dataset_source'] == 'train'\n",
    "training_features = df_combined.loc[train_mask, analysis_features]\n",
    "\n",
    "variance_filter.fit(training_features)\n",
    "\n",
    "# Get feature variance scores\n",
    "feature_variances = variance_filter.variances_\n",
    "low_variance_features = []\n",
    "\n",
    "print(f\"Variance threshold: {variance_threshold}\")\n",
    "print(f\"Features analyzed: {len(analysis_features)}\")\n",
    "\n",
    "# Identify features below threshold\n",
    "for i, (feature, variance) in enumerate(zip(analysis_features, feature_variances)):\n",
    "    if variance <= variance_threshold:\n",
    "        low_variance_features.append(feature)\n",
    "        print(f\"Low variance: {feature} (variance: {variance:.4f})\")\n",
    "\n",
    "print(f\"\\nLow variance features found: {len(low_variance_features)}\")\n",
    "\n",
    "# Create final optimized feature set\n",
    "optimized_features = [f for f in analysis_features if f not in low_variance_features]\n",
    "print(f\"Optimized feature count: {len(optimized_features)}\")\n",
    "print(f\"Variance filtering reduction: {len(analysis_features)} → {len(optimized_features)} ({len(low_variance_features)} removed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance filtering identifies 6 near-constant features with variance < 0.01, primarily log-transformed lot utilization ratios (GrLivArea_per_LotArea_log: 0.0073) and kitchen features (KitchenAbvGr_log: 0.0080). Statistical variance analysis reduces feature set from 123 to 117 features, removing low-information variables while preserving predictive capacity for model optimization.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Apply Feature Analysis Results and Dataset Export\n",
    "\n",
    "Apply systematic feature analysis results from Section 6 to create final optimized feature set.\n",
    "Export model-ready datasets with complete feature engineering documentation for model development pipeline.\n",
    "\n",
    "### 7.1 Apply Section 6 Analysis Results\n",
    "\n",
    "Implement hierarchical feature removals and variance filtering results from systematic analysis.\n",
    "Create final optimized feature set based on domain-knowledge-driven feature selection methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement hierarchical removals and variance filtering for final feature set\n",
    "print(\"APPLYING SECTION 6 FEATURE ANALYSIS RESULTS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Start with ALL features from preprocessing (numerical + one-hot encoded)\n",
    "all_feature_columns = [col for col in df_combined.columns \n",
    "                      if col not in ['dataset_source', 'Id', 'SalePrice', 'SalePrice_log']]\n",
    "\n",
    "print(f\"All features from preprocessing: {len(all_feature_columns)}\")\n",
    "\n",
    "# Note: Section 6.3 variance filtering was only applied to numerical features\n",
    "# We keep ALL one-hot encoded categorical features (they have binary variance)\n",
    "numerical_features_clean = [f for f in all_feature_columns if f in optimized_features] if 'optimized_features' in locals() else []\n",
    "categorical_features = [f for f in all_feature_columns if f not in numerical_features_clean]\n",
    "\n",
    "print(f\"Numerical features (after variance filtering): {len(numerical_features_clean)}\")\n",
    "print(f\"Categorical one-hot encoded features: {len(categorical_features)}\")\n",
    "\n",
    "# Combine numerical (post-variance-filter) + all categorical features\n",
    "final_features = numerical_features_clean + categorical_features\n",
    "print(f\"Starting with combined feature set: {len(final_features)}\")\n",
    "print(\"(117 numerical + categorical one-hot encoded features)\")\n",
    "\n",
    "# Apply hierarchical feature removals from Section 6.2 analysis\n",
    "if 'hierarchical_removals' in locals() and len(hierarchical_removals) > 0:\n",
    "    print(f\"\\nApplying hierarchical component removals:\")\n",
    "    print(f\"Features identified for removal: {len(hierarchical_removals)}\")\n",
    "    \n",
    "    # Show removal decisions with reasoning\n",
    "    removal_summary = []\n",
    "    for feature in hierarchical_removals:\n",
    "        if feature in final_features:\n",
    "            reason = removal_analysis.get(feature, \"Component replaced by superior engineered feature\")\n",
    "            removal_summary.append(f\"  {feature}: {reason}\")\n",
    "            final_features.remove(feature)\n",
    "    \n",
    "    print(\"Hierarchical removal decisions:\")\n",
    "    for removal in removal_summary:\n",
    "        print(removal)\n",
    "    \n",
    "    print(f\"\\nFeatures after hierarchical removal: {len(final_features)}\")\n",
    "else:\n",
    "    print(\"\\nNo hierarchical removals to apply (variable not found)\")\n",
    "\n",
    "# Validate final feature set consistency\n",
    "print(f\"\\nFINAL FEATURE SET VALIDATION:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Check all features exist in dataset\n",
    "missing_features = [f for f in final_features if f not in df_combined.columns]\n",
    "if missing_features:\n",
    "    print(f\"WARNING: {len(missing_features)} features missing from dataset:\")\n",
    "    for feat in missing_features[:5]:\n",
    "        print(f\"  - {feat}\")\n",
    "    final_features = [f for f in final_features if f in df_combined.columns]\n",
    "\n",
    "print(f\"Final validated feature count: {len(final_features)}\")\n",
    "print(f\"Feature validation: {'PASSED' if len(missing_features) == 0 else 'ADJUSTED'}\")\n",
    "\n",
    "# Quick performance validation to ensure removals don't harm baseline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "print(f\"\\nPerformance validation with final feature set:\")\n",
    "train_mask = df_combined['dataset_source'] == 'train'\n",
    "target_col = 'SalePrice_log' if 'SalePrice_log' in df_train_clean.columns else 'SalePrice'\n",
    "y_train = df_train_clean[target_col].values\n",
    "\n",
    "# Quick validation with RandomForest\n",
    "X_final = df_combined.loc[train_mask, final_features].values\n",
    "rf_model = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "cv_scores = cross_val_score(rf_model, X_final, y_train, cv=3, \n",
    "                           scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "final_rmse = np.sqrt(-cv_scores.mean())\n",
    "\n",
    "print(f\"Final feature set performance: {final_rmse:.4f} RMSE\")\n",
    "print(f\"Performance validation: {'ACCEPTABLE' if final_rmse < 0.20 else 'REVIEW NEEDED'}\")\n",
    "\n",
    "# Summary of feature reduction pipeline\n",
    "print(f\"\\nFEATURE REDUCTION SUMMARY:\")\n",
    "print(\"=\" * 25)\n",
    "original_count = len(numerical_features) if 'numerical_features' in locals() else 'unknown'\n",
    "variance_count = len(optimized_features) if 'optimized_features' in locals() else 'unknown'\n",
    "hierarchical_removed = len(hierarchical_removals) if 'hierarchical_removals' in locals() else 0\n",
    "\n",
    "print(f\"Original features (preprocessed): {original_count}\")\n",
    "print(f\"After variance filtering: {variance_count}\")\n",
    "print(f\"Hierarchical removals applied: {hierarchical_removed}\")\n",
    "print(f\"Final feature count: {len(final_features)}\")\n",
    "print(f\"Total reduction: {len(all_feature_columns)} → {len(final_features)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature integration combines 117 variance-filtered numerical features with 169 categorical one-hot encoded features from preprocessing pipeline. Hierarchical removal implementation successfully eliminates 11 redundant features (5 bathroom components + 6 efficiency ratios) with final validation confirming acceptable performance (0.1385 RMSE), reducing feature set from 286 to 275 features for model development.\n",
    "\n",
    "### 7.2 Final Dataset Validation and Export\n",
    "\n",
    "Validate final feature set consistency and export model-ready datasets for development pipeline.\n",
    "Create comprehensive feature engineering documentation with complete transformation summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model-ready datasets with comprehensive feature engineering documentation\n",
    "print(f\"\\nFINAL DATASET EXPORT AND DOCUMENTATION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Prepare final train and test datasets with optimized features\n",
    "train_mask = df_combined['dataset_source'] == 'train'\n",
    "test_mask = df_combined['dataset_source'] == 'test'\n",
    "\n",
    "# Create final feature matrices\n",
    "df_train_final = df_combined.loc[train_mask, final_features].reset_index(drop=True)\n",
    "df_test_final = df_combined.loc[test_mask, final_features].reset_index(drop=True)\n",
    "\n",
    "# Add target variables to training dataset\n",
    "df_train_final['SalePrice'] = df_train_clean['SalePrice'].values\n",
    "if 'SalePrice_log' in df_train_clean.columns:\n",
    "    df_train_final['SalePrice_log'] = df_train_clean['SalePrice_log'].values\n",
    "\n",
    "# Add Id columns for tracking\n",
    "df_train_final['Id'] = df_train_clean['Id'].values\n",
    "df_test_final['Id'] = df_test_clean['Id'].values\n",
    "\n",
    "print(f\"Final dataset shapes:\")\n",
    "print(f\"Train: {df_train_final.shape} (features + target + Id)\")\n",
    "print(f\"Test: {df_test_final.shape} (features + Id)\")\n",
    "\n",
    "# Export final datasets\n",
    "import os\n",
    "output_dir = '../data/processed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "train_output_path = f'{output_dir}/train_feature_engineered.csv'\n",
    "test_output_path = f'{output_dir}/test_feature_engineered.csv'\n",
    "\n",
    "df_train_final.to_csv(train_output_path, index=False)\n",
    "df_test_final.to_csv(test_output_path, index=False)\n",
    "\n",
    "print(f\"\\nDatasets exported:\")\n",
    "print(f\"✓ {train_output_path}\")\n",
    "print(f\"✓ {test_output_path}\")\n",
    "\n",
    "# Create comprehensive feature engineering documentation\n",
    "feature_engineering_doc = {\n",
    "    'feature_engineering_summary': {\n",
    "        'total_features_preprocessed': len(numerical_features) if 'numerical_features' in locals() else 'unknown',\n",
    "        'engineered_features_created': len(all_engineered_features) if 'all_engineered_features' in locals() else 'unknown',\n",
    "        'features_after_variance_filter': len(optimized_features) if 'optimized_features' in locals() else 'unknown',\n",
    "        'final_optimized_features': len(final_features),\n",
    "        'feature_reduction_ratio': f\"{len(final_features)}/{len(numerical_features) if 'numerical_features' in locals() else 'unknown'}\",\n",
    "        'final_performance_rmse': final_rmse\n",
    "    },\n",
    "    'feature_removal_summary': {\n",
    "        'hierarchical_removals': hierarchical_removals if 'hierarchical_removals' in locals() else [],\n",
    "        'variance_filter_removals': low_variance_features if 'low_variance_features' in locals() else []\n",
    "    },\n",
    "    'final_feature_list': final_features\n",
    "}\n",
    "\n",
    "# Export feature engineering documentation\n",
    "import json\n",
    "doc_output_path = f'{output_dir}/feature_engineering_documentation.json'\n",
    "with open(doc_output_path, 'w') as f:\n",
    "    json.dump(feature_engineering_doc, f, indent=2, default=str)\n",
    "\n",
    "print(f\"✓ Feature documentation: {doc_output_path}\")\n",
    "\n",
    "# Summary of feature engineering achievements\n",
    "print(f\"\\nFEATURE ENGINEERING ACHIEVEMENTS:\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"Starting features (preprocessed): {feature_engineering_doc['feature_engineering_summary']['total_features_preprocessed']}\")\n",
    "print(f\"Features after variance filtering: {feature_engineering_doc['feature_engineering_summary']['features_after_variance_filter']}\")\n",
    "print(f\"Final optimized feature count: {len(final_features)}\")\n",
    "print(f\"Final model performance: {final_rmse:.4f} RMSE\")\n",
    "print(f\"Feature reduction: {len(final_features)} features ready for model development\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final dataset export creates model-ready train_feature_engineered.csv and test_feature_engineered.csv with comprehensive feature engineering documentation. Systematic feature optimization provides validated feature set for model development phase with preserved predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dxcoilacg4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data to check for missing values\n",
    "df = pd.read_csv('../data/processed/train_preprocessed.csv')\n",
    "\n",
    "# Check the specific columns you're trying to convert\n",
    "integer_features = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'GarageCars']\n",
    "\n",
    "print(\"MISSING VALUES CHECK:\")\n",
    "print(\"=\" * 30)\n",
    "for col in integer_features:\n",
    "    if col in df.columns:\n",
    "        missing_count = df[col].isna().sum()\n",
    "        unique_vals = df[col].unique()[:10]  # Show first 10 unique values\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  Missing values: {missing_count}\")\n",
    "        print(f\"  Data type: {df[col].dtype}\")\n",
    "        print(f\"  Sample values: {unique_vals}\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HousePrediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
