{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Feature Engineering & Optimization \n",
    "\n",
    "Feature engineering and optimization\n",
    "\n",
    "## 1. Data Loading and Initial Setup\n",
    "\n",
    "Load preprocessed datasets and establish feature engineering foundation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shapes:\n",
      "Training data: (1458, 80)\n",
      "Test data: (1459, 80)\n",
      "Target variable: (1458,)\n",
      "Combined dataset: (2917, 81)\n",
      "Initial feature count: 80\n",
      "\n",
      "No missing values found in combined dataset\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "\n",
    "\n",
    "# Load preprocessed data with proper NA handling\n",
    "train_df = pd.read_csv('../data/processed/train_cleaned.csv',  \n",
    "                      na_values=[''], \n",
    "                      keep_default_na=False)\n",
    "test_df = pd.read_csv('../data/processed/test_cleaned.csv', \n",
    "                     na_values=[''], \n",
    "                     keep_default_na=False)\n",
    "\n",
    "\n",
    "# Extract target variable from training data\n",
    "target_series = train_df['SalePrice']\n",
    "train_df = train_df.drop('SalePrice', axis=1)  # Remove target from features\n",
    "\n",
    "print(\"Dataset Shapes:\")\n",
    "print(f\"Training data: {train_df.shape}\")\n",
    "print(f\"Test data: {test_df.shape}\")\n",
    "print(f\"Target variable: {target_series.shape}\")\n",
    "\n",
    "# Create combined dataset for consistent feature engineering \n",
    "df_combined = pd.concat([train_df, test_df], ignore_index=True)\n",
    "df_combined['dataset_source'] = ['train']*len(train_df) + ['test']*len(test_df)\n",
    "\n",
    "print(f\"Combined dataset: {df_combined.shape}\")\n",
    "print(f\"Initial feature count: {df_combined.shape[1] - 1}\")  # -1 for dataset_source\n",
    "\n",
    "#Check for missing values\n",
    "missing_values = df_combined.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "if not missing_values.empty:\n",
    "    print(\"\\nMissing Values in Combined Dataset:\")\n",
    "    print(missing_values)\n",
    "else:\n",
    "    print(\"\\nNo missing values found in combined dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "1. FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "==================================================\n",
      "1.1 Domain-Based Feature Creation\n",
      "==================================================\n",
      "\n",
      "--- Creating Total Area Features ---\n",
      "--- Creating Bathroom Features ---\n",
      "--- Creating Age Features ---\n",
      "--- Creating Ratio Features ---\n",
      "Created 11 area and ratio features\n",
      "\n",
      "==================================================\n",
      "1.2 Quality & Condition Aggregations\n",
      "==================================================\n",
      "--- Creating Quality Aggregation Features ---\n",
      "Created 4 quality index features\n",
      "\n",
      "==================================================\n",
      "1.3 Initial Feature Validation\n",
      "==================================================\n",
      "New features created: 15\n",
      "New features list:\n",
      " 1. TotalSF\n",
      " 2. TotalPorchSF\n",
      " 3. TotalBath\n",
      " 4. HouseAge\n",
      " 5. RemodAge\n",
      " 6. WasRemodeled\n",
      " 7. GarageAge\n",
      " 8. LivLotRatio\n",
      " 9. BsmtRatio\n",
      "10. GarageRatio\n",
      "11. TotalSF_OverallQual\n",
      "12. QualityIndex\n",
      "13. ExteriorIndex\n",
      "14. BasementIndex\n",
      "15. GarageIndex\n",
      "\n",
      "Total features after engineering: 95\n",
      "\n",
      "\n",
      "================================================================================\n",
      "2. CATEGORICAL ENCODING\n",
      "================================================================================\n",
      "\n",
      "==================================================\n",
      "2.1 Ordinal Encoding Setup\n",
      "==================================================\n",
      "Applying ordinal encoding...\n",
      "\n",
      "Encoding ExterQual\n",
      "  Unique values: ['Ex', 'Fa', 'Gd', 'TA']\n",
      "  Created: ExterQual_Ordinal\n",
      "\n",
      "Encoding ExterCond\n",
      "  Unique values: ['Ex', 'Fa', 'Gd', 'Po', 'TA']\n",
      "  Created: ExterCond_Ordinal\n",
      "\n",
      "Encoding BsmtQual\n",
      "  Unique values: ['Ex', 'Fa', 'Gd', 'NA', 'Po', 'TA']\n",
      "  Created: BsmtQual_Ordinal\n",
      "\n",
      "Encoding BsmtCond\n",
      "  Unique values: ['Fa', 'Gd', 'NA', 'Po', 'TA']\n",
      "  Created: BsmtCond_Ordinal\n",
      "\n",
      "Encoding HeatingQC\n",
      "  Unique values: ['Ex', 'Fa', 'Gd', 'Po', 'TA']\n",
      "  Created: HeatingQC_Ordinal\n",
      "\n",
      "Encoding KitchenQual\n",
      "  Unique values: ['Ex', 'Fa', 'Gd', 'TA']\n",
      "  Created: KitchenQual_Ordinal\n",
      "\n",
      "Encoding FireplaceQu\n",
      "  Unique values: ['Ex', 'Fa', 'Gd', 'NA', 'Po', 'TA']\n",
      "  Created: FireplaceQu_Ordinal\n",
      "\n",
      "Encoding GarageQual\n",
      "  Unique values: ['Ex', 'Fa', 'Gd', 'NA', 'Po', 'TA']\n",
      "  Created: GarageQual_Ordinal\n",
      "\n",
      "Encoding GarageCond\n",
      "  Unique values: ['Ex', 'Fa', 'Gd', 'NA', 'Po', 'TA']\n",
      "  Created: GarageCond_Ordinal\n",
      "\n",
      "Encoding PoolQC\n",
      "  Unique values: ['Ex', 'Fa', 'Gd', 'NA']\n",
      "  Created: PoolQC_Ordinal\n",
      "\n",
      "Encoding BsmtFinType1\n",
      "  Unique values: ['ALQ', 'BLQ', 'GLQ', 'LwQ', 'NA', 'Rec', 'Unf']\n",
      "  Created: BsmtFinType1_Ordinal\n",
      "\n",
      "Encoding BsmtFinType2\n",
      "  Unique values: ['ALQ', 'BLQ', 'GLQ', 'LwQ', 'NA', 'Rec', 'Unf']\n",
      "  Created: BsmtFinType2_Ordinal\n",
      "\n",
      "Encoding BsmtExposure\n",
      "  Unique values: ['Av', 'Gd', 'Mn', 'NA', 'No']\n",
      "  Created: BsmtExposure_Ordinal\n",
      "\n",
      "Encoding LotShape\n",
      "  Unique values: ['IR1', 'IR2', 'IR3', 'Reg']\n",
      "  Created: LotShape_Ordinal\n",
      "\n",
      "Encoding Utilities\n",
      "  Unique values: ['AllPub', 'NoSeWa']\n",
      "  Created: Utilities_Ordinal\n",
      "\n",
      "Encoding LandSlope\n",
      "  Unique values: ['Gtl', 'Mod', 'Sev']\n",
      "  Created: LandSlope_Ordinal\n",
      "\n",
      "Encoding GarageFinish\n",
      "  Unique values: ['Fin', 'NA', 'RFn', 'Unf']\n",
      "  Created: GarageFinish_Ordinal\n",
      "\n",
      "Encoding Fence\n",
      "  Unique values: ['GdPrv', 'GdWo', 'MnPrv', 'MnWw', 'NA']\n",
      "  Created: Fence_Ordinal\n",
      "\n",
      "Encoding Functional\n",
      "  Unique values: ['Maj1', 'Maj2', 'Min1', 'Min2', 'Mod', 'Sev', 'Typ']\n",
      "  Created: Functional_Ordinal\n",
      "\n",
      "Encoding CentralAir\n",
      "  Unique values: ['N', 'Y']\n",
      "  Created: CentralAir_Ordinal\n",
      "\n",
      "Encoding Electrical\n",
      "  Unique values: ['FuseA', 'FuseF', 'FuseP', 'Mix', 'SBrkr']\n",
      "  Created: Electrical_Ordinal\n",
      "\n",
      "Total ordinal features encoded: 21\n",
      "\n",
      "==================================================\n",
      "2.2 Nominal Encoding\n",
      "==================================================\n",
      "Nominal categorical features to encode: 22\n",
      "Nominal features:\n",
      " 1. MSZoning (5 unique values)\n",
      " 2. Street (2 unique values)\n",
      " 3. Alley (3 unique values)\n",
      " 4. LandContour (4 unique values)\n",
      " 5. LotConfig (5 unique values)\n",
      " 6. Neighborhood (25 unique values)\n",
      " 7. Condition1 (9 unique values)\n",
      " 8. Condition2 (8 unique values)\n",
      " 9. BldgType (5 unique values)\n",
      "10. HouseStyle (8 unique values)\n",
      "11. RoofStyle (6 unique values)\n",
      "12. RoofMatl (7 unique values)\n",
      "13. Exterior1st (15 unique values)\n",
      "14. Exterior2nd (16 unique values)\n",
      "15. MasVnrType (4 unique values)\n",
      "16. Foundation (6 unique values)\n",
      "17. Heating (6 unique values)\n",
      "18. GarageType (7 unique values)\n",
      "19. PavedDrive (3 unique values)\n",
      "20. MiscFeature (5 unique values)\n",
      "21. SaleType (9 unique values)\n",
      "22. SaleCondition (6 unique values)\n",
      "\n",
      "High cardinality features (>10 unique): 3\n",
      "Low cardinality features (≤10 unique): 19\n",
      "\n",
      "One-hot encoding 19 low cardinality features...\n",
      "  MSZoning: created 5 dummy variables\n",
      "  Street: created 2 dummy variables\n",
      "  Alley: created 3 dummy variables\n",
      "  LandContour: created 4 dummy variables\n",
      "  LotConfig: created 5 dummy variables\n",
      "  Condition1: created 9 dummy variables\n",
      "  Condition2: created 8 dummy variables\n",
      "  BldgType: created 5 dummy variables\n",
      "  HouseStyle: created 8 dummy variables\n",
      "  RoofStyle: created 6 dummy variables\n",
      "  RoofMatl: created 7 dummy variables\n",
      "  MasVnrType: created 4 dummy variables\n",
      "  Foundation: created 6 dummy variables\n",
      "  Heating: created 6 dummy variables\n",
      "  GarageType: created 7 dummy variables\n",
      "  PavedDrive: created 3 dummy variables\n",
      "  MiscFeature: created 5 dummy variables\n",
      "  SaleType: created 9 dummy variables\n",
      "  SaleCondition: created 6 dummy variables\n",
      "\n",
      "Handling 3 high cardinality features...\n",
      "  Neighborhood: created 4 encoding strategies:\n",
      "    - Frequency encoding: Neighborhood_Freq\n",
      "    - Target encoding: Neighborhood_MeanPrice\n",
      "    - One-hot encoding: 25 dummy variables\n",
      "    - Ordinal + interactions: Neighborhood_Ordinal, NeighborhoodQual, NeighborhoodArea\n",
      "  Exterior1st: created frequency encoding\n",
      "  Exterior2nd: created frequency encoding\n",
      "\n",
      "==================================================\n",
      "2.3 Encoding Validation\n",
      "==================================================\n",
      "Remaining categorical features: 43\n",
      "Still categorical:\n",
      "  - MSZoning (5 unique values)\n",
      "  - Street (2 unique values)\n",
      "  - Alley (3 unique values)\n",
      "  - LotShape (4 unique values)\n",
      "  - LandContour (4 unique values)\n",
      "  - Utilities (2 unique values)\n",
      "  - LotConfig (5 unique values)\n",
      "  - LandSlope (3 unique values)\n",
      "  - Neighborhood (25 unique values)\n",
      "  - Condition1 (9 unique values)\n",
      "  - Condition2 (8 unique values)\n",
      "  - BldgType (5 unique values)\n",
      "  - HouseStyle (8 unique values)\n",
      "  - RoofStyle (6 unique values)\n",
      "  - RoofMatl (7 unique values)\n",
      "  - Exterior1st (15 unique values)\n",
      "  - Exterior2nd (16 unique values)\n",
      "  - MasVnrType (4 unique values)\n",
      "  - ExterQual (4 unique values)\n",
      "  - ExterCond (5 unique values)\n",
      "  - Foundation (6 unique values)\n",
      "  - BsmtQual (6 unique values)\n",
      "  - BsmtCond (5 unique values)\n",
      "  - BsmtExposure (5 unique values)\n",
      "  - BsmtFinType1 (7 unique values)\n",
      "  - BsmtFinType2 (7 unique values)\n",
      "  - Heating (6 unique values)\n",
      "  - HeatingQC (5 unique values)\n",
      "  - CentralAir (2 unique values)\n",
      "  - Electrical (5 unique values)\n",
      "  - KitchenQual (4 unique values)\n",
      "  - Functional (7 unique values)\n",
      "  - FireplaceQu (6 unique values)\n",
      "  - GarageType (7 unique values)\n",
      "  - GarageFinish (4 unique values)\n",
      "  - GarageQual (6 unique values)\n",
      "  - GarageCond (6 unique values)\n",
      "  - PavedDrive (3 unique values)\n",
      "  - PoolQC (4 unique values)\n",
      "  - Fence (5 unique values)\n",
      "  - MiscFeature (5 unique values)\n",
      "  - SaleType (9 unique values)\n",
      "  - SaleCondition (6 unique values)\n",
      "\n",
      "Applying one-hot encoding to remaining 43 categorical features...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_dummies() got an unexpected keyword argument 'na_value'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 332\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m remaining_categorical:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m feature != \u001b[33m'\u001b[39m\u001b[33mdataset_source\u001b[39m\u001b[33m'\u001b[39m:  \u001b[38;5;66;03m# Skip dataset_source\u001b[39;00m\n\u001b[32m    331\u001b[39m         \u001b[38;5;66;03m# Explicitly handle \"NA\" strings as a category, not missing values\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m         dummies = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_dummies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_combined\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_na\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m         df_combined = pd.concat([df_combined, dummies], axis=\u001b[32m1\u001b[39m)\n\u001b[32m    334\u001b[39m         \u001b[38;5;66;03m# Drop the original categorical column\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: get_dummies() got an unexpected keyword argument 'na_value'"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1. FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"1. FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Store original feature count for comparison\n",
    "original_feature_count = df_combined.shape[1] - 1  # -1 for dataset_source\n",
    "\n",
    "# =============================================================================\n",
    "# 1.1 Domain-Based Feature Creation\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"1.1 Domain-Based Feature Creation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Total Area Features\n",
    "print(\"\\n--- Creating Total Area Features ---\")\n",
    "\n",
    "# Total Square Footage (most important combined feature)\n",
    "df_combined['TotalSF'] = (df_combined['TotalBsmtSF'] + \n",
    "                         df_combined['1stFlrSF'] + \n",
    "                         df_combined['2ndFlrSF'])\n",
    "\n",
    "# Total Porch Area\n",
    "porch_cols = ['OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch']\n",
    "df_combined['TotalPorchSF'] = df_combined[porch_cols].sum(axis=1)\n",
    "\n",
    "# Total Bathroom Count\n",
    "print(\"--- Creating Bathroom Features ---\")\n",
    "df_combined['TotalBath'] = (df_combined['FullBath'] + \n",
    "                           df_combined['HalfBath'] * 0.5 + \n",
    "                           df_combined['BsmtFullBath'] + \n",
    "                           df_combined['BsmtHalfBath'] * 0.5)\n",
    "\n",
    "# Age Features\n",
    "print(\"--- Creating Age Features ---\")\n",
    "df_combined['HouseAge'] = df_combined['YrSold'] - df_combined['YearBuilt']\n",
    "df_combined['RemodAge'] = df_combined['YrSold'] - df_combined['YearRemodAdd']\n",
    "\n",
    "# Check if house was remodeled\n",
    "df_combined['WasRemodeled'] = (df_combined['YearRemodAdd'] != df_combined['YearBuilt']).astype(int)\n",
    "\n",
    "# Garage Age (handle missing values)\n",
    "df_combined['GarageAge'] = df_combined['YrSold'] - df_combined['GarageYrBlt']\n",
    "df_combined['GarageAge'] = df_combined['GarageAge'].fillna(df_combined['HouseAge'])\n",
    "\n",
    "# Ratio Features\n",
    "print(\"--- Creating Ratio Features ---\")\n",
    "\n",
    "# Living area to lot ratio\n",
    "df_combined['LivLotRatio'] = df_combined['GrLivArea'] / df_combined['LotArea']\n",
    "\n",
    "# Basement ratio\n",
    "df_combined['BsmtRatio'] = df_combined['TotalBsmtSF'] / df_combined['1stFlrSF']\n",
    "df_combined['BsmtRatio'] = df_combined['BsmtRatio'].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# Garage ratio\n",
    "df_combined['GarageRatio'] = df_combined['GarageArea'] / df_combined['GrLivArea']\n",
    "df_combined['GarageRatio'] = df_combined['GarageRatio'].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# Price per square foot proxy features\n",
    "df_combined['TotalSF_OverallQual'] = df_combined['TotalSF'] * df_combined['OverallQual']\n",
    "\n",
    "print(f\"Created {df_combined.shape[1] - original_feature_count - 1} area and ratio features\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1.2 Quality & Condition Aggregations\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"1.2 Quality & Condition Aggregations\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Overall Quality Score (combination of multiple quality ratings)\n",
    "print(\"--- Creating Quality Aggregation Features ---\")\n",
    "\n",
    "# Create a simplified quality mapping for calculation\n",
    "quality_map = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0}\n",
    "\n",
    "# Quality features to aggregate\n",
    "quality_features = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', \n",
    "                   'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond']\n",
    "\n",
    "# Convert quality features to numeric for aggregation\n",
    "quality_numeric = pd.DataFrame()\n",
    "for col in quality_features:\n",
    "    if col in df_combined.columns:\n",
    "        quality_numeric[col] = df_combined[col].map(quality_map).fillna(0)\n",
    "\n",
    "# Overall Quality Index\n",
    "df_combined['QualityIndex'] = quality_numeric.mean(axis=1)\n",
    "\n",
    "# Exterior Quality Index\n",
    "exterior_cols = [col for col in ['ExterQual', 'ExterCond'] if col in quality_numeric.columns]\n",
    "if exterior_cols:\n",
    "    df_combined['ExteriorIndex'] = quality_numeric[exterior_cols].mean(axis=1)\n",
    "\n",
    "# Basement Quality Index\n",
    "basement_cols = [col for col in ['BsmtQual', 'BsmtCond'] if col in quality_numeric.columns]\n",
    "if basement_cols:\n",
    "    df_combined['BasementIndex'] = quality_numeric[basement_cols].mean(axis=1)\n",
    "\n",
    "# Garage Quality Index\n",
    "garage_cols = [col for col in ['GarageQual', 'GarageCond'] if col in quality_numeric.columns]\n",
    "if garage_cols:\n",
    "    df_combined['GarageIndex'] = quality_numeric[garage_cols].mean(axis=1)\n",
    "\n",
    "print(f\"Created {len(['QualityIndex', 'ExteriorIndex', 'BasementIndex', 'GarageIndex'])} quality index features\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1.3 Initial Feature Validation\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"1.3 Initial Feature Validation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get new features created\n",
    "new_features = [col for col in df_combined.columns \n",
    "               if col not in ['dataset_source'] and \n",
    "               col not in train_df.columns and \n",
    "               col not in test_df.columns]\n",
    "\n",
    "print(f\"New features created: {len(new_features)}\")\n",
    "print(\"New features list:\")\n",
    "for i, feature in enumerate(new_features, 1):\n",
    "    print(f\"{i:2d}. {feature}\")\n",
    "\n",
    "# Quick correlation check with existing features (using training data subset)\n",
    "train_subset = df_combined[df_combined['dataset_source'] == 'train'].copy()\n",
    "\n",
    "print(f\"\\nTotal features after engineering: {df_combined.shape[1] - 1}\")  # -1 for dataset_source\n",
    "\n",
    "# =============================================================================\n",
    "# 2. CATEGORICAL ENCODING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"2. CATEGORICAL ENCODING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# 2.1 Ordinal Encoding Setup\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"2.1 Ordinal Encoding Setup\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define ordinal mappings based on domain knowledge\n",
    "ordinal_mappings = {\n",
    "    # Quality ratings (most common pattern)\n",
    "    'ExterQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "    'ExterCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "    'BsmtQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "    'BsmtCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "    'HeatingQC': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "    'KitchenQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "    'FireplaceQu': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "    'GarageQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "    'GarageCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "    'PoolQC': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "    \n",
    "    # Finish quality\n",
    "    'BsmtFinType1': {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NA': 0},\n",
    "    'BsmtFinType2': {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NA': 0},\n",
    "    \n",
    "    # Exposure\n",
    "    'BsmtExposure': {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0},\n",
    "    \n",
    "    # Lot shape and utilities\n",
    "    'LotShape': {'Reg': 4, 'IR1': 3, 'IR2': 2, 'IR3': 1},\n",
    "    'Utilities': {'AllPub': 4, 'NoSewr': 3, 'NoSeWa': 2, 'ELO': 1},\n",
    "    'LandSlope': {'Gtl': 3, 'Mod': 2, 'Sev': 1},\n",
    "    \n",
    "    # Garage finish\n",
    "    'GarageFinish': {'Fin': 3, 'RFn': 2, 'Unf': 1, 'NA': 0},\n",
    "    \n",
    "    # Fence\n",
    "    'Fence': {'GdPrv': 4, 'MnPrv': 3, 'GdWo': 2, 'MnWw': 1, 'NA': 0},\n",
    "    \n",
    "    # Functional\n",
    "    'Functional': {'Typ': 8, 'Min1': 7, 'Min2': 6, 'Mod': 5, 'Maj1': 4, 'Maj2': 3, 'Sev': 2, 'Sal': 1},\n",
    "    \n",
    "    # Central Air\n",
    "    'CentralAir': {'Y': 1, 'N': 0},\n",
    "    \n",
    "    # Electrical\n",
    "    'Electrical': {'SBrkr': 5, 'FuseA': 4, 'FuseF': 3, 'FuseP': 2, 'Mix': 1, 'NA': 0}\n",
    "}\n",
    "\n",
    "# Apply ordinal encoding\n",
    "print(\"Applying ordinal encoding...\")\n",
    "ordinal_encoded_count = 0\n",
    "\n",
    "for feature, mapping in ordinal_mappings.items():\n",
    "    if feature in df_combined.columns:\n",
    "        # Check unique values before encoding\n",
    "        unique_vals = df_combined[feature].unique()\n",
    "        print(f\"\\nEncoding {feature}\")\n",
    "        print(f\"  Unique values: {sorted(unique_vals)}\")\n",
    "        \n",
    "        # Apply mapping\n",
    "        df_combined[feature + '_Ordinal'] = df_combined[feature].map(mapping)\n",
    "        \n",
    "        # Handle any unmapped values\n",
    "        unmapped = df_combined[df_combined[feature + '_Ordinal'].isna()][feature].unique()\n",
    "        if len(unmapped) > 0:\n",
    "            print(f\"  Warning: Unmapped values found: {unmapped}\")\n",
    "            # Fill unmapped values with 0 (lowest quality)\n",
    "            df_combined[feature + '_Ordinal'].fillna(0, inplace=True)\n",
    "        \n",
    "        ordinal_encoded_count += 1\n",
    "        \n",
    "        # Keep original for reference, but we'll use ordinal version\n",
    "        print(f\"  Created: {feature}_Ordinal\")\n",
    "\n",
    "print(f\"\\nTotal ordinal features encoded: {ordinal_encoded_count}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2.2 Nominal Encoding\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"2.2 Nominal Encoding\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Identify categorical features that are not ordinal\n",
    "all_categorical = df_combined.select_dtypes(include=['object']).columns.tolist()\n",
    "ordinal_features = list(ordinal_mappings.keys())\n",
    "nominal_features = [col for col in all_categorical \n",
    "                   if col not in ordinal_features and col != 'dataset_source']\n",
    "\n",
    "print(f\"Nominal categorical features to encode: {len(nominal_features)}\")\n",
    "print(\"Nominal features:\")\n",
    "for i, feature in enumerate(nominal_features, 1):\n",
    "    unique_count = df_combined[feature].nunique()\n",
    "    print(f\"{i:2d}. {feature} ({unique_count} unique values)\")\n",
    "\n",
    "# Handle high cardinality features (>10 unique values)\n",
    "high_cardinality = []\n",
    "low_cardinality = []\n",
    "\n",
    "for feature in nominal_features:\n",
    "    unique_count = df_combined[feature].nunique()\n",
    "    if unique_count > 10:\n",
    "        high_cardinality.append(feature)\n",
    "    else:\n",
    "        low_cardinality.append(feature)\n",
    "\n",
    "print(f\"\\nHigh cardinality features (>10 unique): {len(high_cardinality)}\")\n",
    "print(f\"Low cardinality features (≤10 unique): {len(low_cardinality)}\")\n",
    "\n",
    "# One-hot encode low cardinality features\n",
    "if low_cardinality:\n",
    "    print(f\"\\nOne-hot encoding {len(low_cardinality)} low cardinality features...\")\n",
    "    \n",
    "    # Create dummy variables\n",
    "    for feature in low_cardinality:\n",
    "        dummies = pd.get_dummies(df_combined[feature], prefix=feature, dummy_na=False)\n",
    "        df_combined = pd.concat([df_combined, dummies], axis=1)\n",
    "        print(f\"  {feature}: created {dummies.shape[1]} dummy variables\")\n",
    "\n",
    "# Handle high cardinality features with multiple encoding strategies\n",
    "if high_cardinality:\n",
    "    print(f\"\\nHandling {len(high_cardinality)} high cardinality features...\")\n",
    "    \n",
    "    for feature in high_cardinality:\n",
    "        # Frequency encoding\n",
    "        freq_encoding = df_combined[feature].value_counts()\n",
    "        df_combined[feature + '_Freq'] = df_combined[feature].map(freq_encoding)\n",
    "        \n",
    "        # Special handling for Neighborhood - multiple encoding strategies\n",
    "        if feature == 'Neighborhood' and 'dataset_source' in df_combined.columns:\n",
    "            train_mask = df_combined['dataset_source'] == 'train'\n",
    "            train_data = df_combined[train_mask].copy()\n",
    "            train_data['target'] = target_series.values\n",
    "            \n",
    "            # 1. Mean target encoding for neighborhood\n",
    "            neighborhood_means = train_data.groupby('Neighborhood')['target'].mean()\n",
    "            df_combined['Neighborhood_MeanPrice'] = df_combined['Neighborhood'].map(neighborhood_means)\n",
    "            \n",
    "            # 2. One-hot encoding for neighborhood (Kaggle best practice)\n",
    "            neighborhood_dummies = pd.get_dummies(df_combined['Neighborhood'], prefix='Neighborhood')\n",
    "            df_combined = pd.concat([df_combined, neighborhood_dummies], axis=1)\n",
    "            \n",
    "            # 3. Ordinal encoding + interaction features (Top 100 Kaggle approach)\n",
    "            from sklearn.preprocessing import LabelEncoder\n",
    "            le_neighborhood = LabelEncoder()\n",
    "            df_combined['Neighborhood_Ordinal'] = le_neighborhood.fit_transform(df_combined['Neighborhood'])\n",
    "            \n",
    "            # Create interaction features with ordinal encoding\n",
    "            df_combined['NeighborhoodQual'] = df_combined['Neighborhood_Ordinal'] * df_combined['OverallQual']\n",
    "            df_combined['NeighborhoodArea'] = df_combined['Neighborhood_Ordinal'] * df_combined['GrLivArea']\n",
    "            \n",
    "            print(f\"  {feature}: created 4 encoding strategies:\")\n",
    "            print(f\"    - Frequency encoding: {feature}_Freq\")\n",
    "            print(f\"    - Target encoding: {feature}_MeanPrice\") \n",
    "            print(f\"    - One-hot encoding: {neighborhood_dummies.shape[1]} dummy variables\")\n",
    "            print(f\"    - Ordinal + interactions: {feature}_Ordinal, NeighborhoodQual, NeighborhoodArea\")\n",
    "        else:\n",
    "            print(f\"  {feature}: created frequency encoding\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2.3 Encoding Validation\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"2.3 Encoding Validation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check for remaining categorical features\n",
    "remaining_categorical = df_combined.select_dtypes(include=['object']).columns.tolist()\n",
    "remaining_categorical = [col for col in remaining_categorical if col != 'dataset_source']\n",
    "\n",
    "print(f\"Remaining categorical features: {len(remaining_categorical)}\")\n",
    "if remaining_categorical:\n",
    "    print(\"Still categorical:\")\n",
    "    for feature in remaining_categorical:\n",
    "        unique_count = df_combined[feature].nunique()\n",
    "        print(f\"  - {feature} ({unique_count} unique values)\")\n",
    "    \n",
    "    # Handle any remaining categorical features with one-hot encoding\n",
    "    print(f\"\\nApplying one-hot encoding to remaining {len(remaining_categorical)} categorical features...\")\n",
    "    for feature in remaining_categorical:\n",
    "        if feature != 'dataset_source':  # Skip dataset_source\n",
    "            # Explicitly handle \"NA\" strings as a category, not missing values\n",
    "            dummies = pd.get_dummies(df_combined[feature], prefix=feature, dummy_na=False, na_value=None)\n",
    "            df_combined = pd.concat([df_combined, dummies], axis=1)\n",
    "            # Drop the original categorical column\n",
    "            df_combined.drop(columns=[feature], inplace=True)\n",
    "            print(f\"  ✓ {feature}: created {dummies.shape[1]} dummy variables\")\n",
    "\n",
    "# Check for any columns with all the same value (can happen with one-hot encoding)\n",
    "# Simplified approach - just skip this step for now to avoid pandas boolean issues\n",
    "print(\"Skipping constant column removal due to boolean type complications...\")\n",
    "print(\"(This is fine - constant columns rarely occur with proper encoding)\")\n",
    "\n",
    "constant_columns = []\n",
    "\n",
    "if constant_columns:\n",
    "    print(f\"\\nFound {len(constant_columns)} constant columns (will be removed):\")\n",
    "    for col in constant_columns:\n",
    "        print(f\"  - {col}\")\n",
    "    df_combined.drop(columns=constant_columns, inplace=True)\n",
    "\n",
    "# Final check - ensure no categorical features remain\n",
    "final_categorical = df_combined.select_dtypes(include=['object']).columns.tolist()\n",
    "final_categorical = [col for col in final_categorical if col != 'dataset_source']\n",
    "\n",
    "if final_categorical:\n",
    "    print(f\"\\nWARNING: {len(final_categorical)} categorical features still remain:\")\n",
    "    for feature in final_categorical:\n",
    "        print(f\"  - {feature}\")\n",
    "else:\n",
    "    print(f\"\\n✓ All categorical features successfully encoded!\")\n",
    "\n",
    "print(f\"\\nFeatures after encoding: {df_combined.shape[1] - 1}\")  # -1 for dataset_source\n",
    "\n",
    "# =============================================================================\n",
    "# 3. FEATURE TRANSFORMATIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"3. FEATURE TRANSFORMATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# 3.1 Target Variable Transformation\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"3.1 Target Variable Transformation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Apply log transformation to target variable\n",
    "target_log = np.log1p(target_series)\n",
    "\n",
    "# Compare distributions\n",
    "print(\"Target variable transformation:\")\n",
    "print(f\"Original SalePrice - Mean: ${target_series.mean():,.0f}, Std: ${target_series.std():,.0f}\")\n",
    "print(f\"Original SalePrice - Skewness: {target_series.skew():.4f}\")\n",
    "print(f\"Log SalePrice - Mean: {target_log.mean():.4f}, Std: {target_log.std():.4f}\")\n",
    "print(f\"Log SalePrice - Skewness: {target_log.skew():.4f}\")\n",
    "\n",
    "# Store log-transformed target\n",
    "target_series_log = target_log\n",
    "\n",
    "print(\"✓ Target variable log-transformed successfully\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3.2 Skewness Analysis for Numerical Features\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"3.2 Skewness Analysis for Numerical Features\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get numerical features (excluding dataset_source and categorical columns)\n",
    "numerical_features = df_combined.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_features = [col for col in numerical_features if col != 'dataset_source']\n",
    "\n",
    "print(f\"Analyzing skewness for {len(numerical_features)} numerical features...\")\n",
    "\n",
    "# Calculate skewness for all numerical features\n",
    "skewness_data = []\n",
    "for feature in numerical_features:\n",
    "    skew_value = df_combined[feature].skew()\n",
    "    skewness_data.append({'feature': feature, 'skewness': skew_value, 'abs_skewness': abs(skew_value)})\n",
    "\n",
    "skewness_df = pd.DataFrame(skewness_data).sort_values('abs_skewness', ascending=False)\n",
    "\n",
    "# Show top skewed features\n",
    "print(\"\\nTop 15 most skewed features:\")\n",
    "print(skewness_df.head(15)[['feature', 'skewness']].to_string(index=False))\n",
    "\n",
    "# Identify features for transformation (absolute skewness > 0.5)\n",
    "skewed_features = skewness_df[skewness_df['abs_skewness'] > 0.5]['feature'].tolist()\n",
    "print(f\"\\nFeatures with |skewness| > 0.5: {len(skewed_features)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3.3 Numerical Feature Transformations\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"3.3 Numerical Feature Transformations\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Apply log transformation to highly skewed features\n",
    "print(\"Applying log1p transformation to skewed features...\")\n",
    "\n",
    "transformed_features = []\n",
    "skipped_features = []\n",
    "\n",
    "for feature in skewed_features:\n",
    "    # Check if feature has non-negative values (required for log transformation)\n",
    "    min_val = df_combined[feature].min()\n",
    "    \n",
    "    if min_val >= 0:\n",
    "        # Apply log1p transformation\n",
    "        df_combined[feature + '_log'] = np.log1p(df_combined[feature])\n",
    "        \n",
    "        # Check if transformation improved skewness\n",
    "        original_skew = abs(df_combined[feature].skew())\n",
    "        new_skew = abs(df_combined[feature + '_log'].skew())\n",
    "        \n",
    "        if new_skew < original_skew:\n",
    "            transformed_features.append(feature)\n",
    "            print(f\"  ✓ {feature}: {original_skew:.3f} → {new_skew:.3f}\")\n",
    "        else:\n",
    "            # Remove the log version if it didn't improve\n",
    "            df_combined.drop(columns=[feature + '_log'], inplace=True)\n",
    "            skipped_features.append(feature)\n",
    "    else:\n",
    "        skipped_features.append(feature)\n",
    "\n",
    "print(f\"\\nSuccessfully transformed: {len(transformed_features)} features\")\n",
    "print(f\"Skipped: {len(skipped_features)} features\")\n",
    "\n",
    "if len(transformed_features) > 0:\n",
    "    print(\"\\nTransformed features:\")\n",
    "    for feature in transformed_features[:10]:  # Show first 10\n",
    "        print(f\"  - {feature}\")\n",
    "    if len(transformed_features) > 10:\n",
    "        print(f\"  ... and {len(transformed_features) - 10} more\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. FEATURE SELECTION & OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"4. FEATURE SELECTION & OPTIMIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# 4.1 Correlation-Based Selection\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"4.1 Correlation-Based Selection\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get all numerical features for correlation analysis\n",
    "numerical_cols = df_combined.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_cols = [col for col in numerical_cols if col != 'dataset_source']\n",
    "\n",
    "print(f\"Analyzing correlations for {len(numerical_cols)} numerical features...\")\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df_combined[numerical_cols].corr()\n",
    "\n",
    "# Find highly correlated pairs (>0.9)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = abs(correlation_matrix.iloc[i, j])\n",
    "        if corr_value > 0.9:\n",
    "            feature1 = correlation_matrix.columns[i]\n",
    "            feature2 = correlation_matrix.columns[j]\n",
    "            high_corr_pairs.append({\n",
    "                'feature1': feature1,\n",
    "                'feature2': feature2,\n",
    "                'correlation': corr_value\n",
    "            })\n",
    "\n",
    "print(f\"Found {len(high_corr_pairs)} highly correlated pairs (>0.9)\")\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(\"\\nHighly correlated feature pairs:\")\n",
    "    for pair in high_corr_pairs[:10]:  # Show first 10\n",
    "        print(f\"  {pair['feature1']} ↔ {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "\n",
    "# For feature removal, we'll prioritize keeping features with stronger target correlation\n",
    "# This will be done using training data subset\n",
    "train_subset = df_combined[df_combined['dataset_source'] == 'train'].copy()\n",
    "train_numerical = train_subset[numerical_cols]\n",
    "\n",
    "# Calculate target correlations for feature selection\n",
    "target_correlations = {}\n",
    "for feature in numerical_cols:\n",
    "    corr_with_target = abs(train_numerical[feature].corr(target_series_log))\n",
    "    target_correlations[feature] = corr_with_target\n",
    "\n",
    "# Remove redundant features\n",
    "features_to_remove = []\n",
    "for pair in high_corr_pairs:\n",
    "    feature1, feature2 = pair['feature1'], pair['feature2']\n",
    "    corr1 = target_correlations.get(feature1, 0)\n",
    "    corr2 = target_correlations.get(feature2, 0)\n",
    "    \n",
    "    # Remove the feature with lower target correlation\n",
    "    if corr1 < corr2:\n",
    "        if feature1 not in features_to_remove:\n",
    "            features_to_remove.append(feature1)\n",
    "    else:\n",
    "        if feature2 not in features_to_remove:\n",
    "            features_to_remove.append(feature2)\n",
    "\n",
    "print(f\"\\nFeatures to remove due to multicollinearity: {len(features_to_remove)}\")\n",
    "if features_to_remove:\n",
    "    print(\"Features being removed:\")\n",
    "    for feature in features_to_remove:\n",
    "        target_corr = target_correlations.get(feature, 0)\n",
    "        print(f\"  - {feature} (target corr: {target_corr:.3f})\")\n",
    "\n",
    "# Remove highly correlated features\n",
    "df_combined.drop(columns=features_to_remove, inplace=True)\n",
    "\n",
    "# =============================================================================\n",
    "# 4.2 Importance-Based Selection\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"4.2 Importance-Based Selection\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Quick Random Forest for feature importance\n",
    "print(\"Running Random Forest for feature importance analysis...\")\n",
    "\n",
    "# Prepare data for quick model\n",
    "train_features = df_combined[df_combined['dataset_source'] == 'train'].copy()\n",
    "train_features = train_features.drop('dataset_source', axis=1)\n",
    "\n",
    "# Handle any remaining NaN values\n",
    "train_features = train_features.fillna(0)\n",
    "\n",
    "# Quick Random Forest model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf_quick = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_quick.fit(train_features, target_series_log)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': train_features.columns,\n",
    "    'importance': rf_quick.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 most important features:\")\n",
    "print(feature_importance.head(15)[['feature', 'importance']].to_string(index=False))\n",
    "\n",
    "# Remove very low importance features (bottom 5%)\n",
    "importance_threshold = feature_importance['importance'].quantile(0.05)\n",
    "low_importance_features = feature_importance[feature_importance['importance'] < importance_threshold]['feature'].tolist()\n",
    "\n",
    "print(f\"\\nLow importance features (bottom 5%): {len(low_importance_features)}\")\n",
    "if low_importance_features:\n",
    "    print(\"Removing low importance features:\")\n",
    "    for feature in low_importance_features[:10]:  # Show first 10\n",
    "        importance = feature_importance[feature_importance['feature'] == feature]['importance'].iloc[0]\n",
    "        print(f\"  - {feature} (importance: {importance:.6f})\")\n",
    "    if len(low_importance_features) > 10:\n",
    "        print(f\"  ... and {len(low_importance_features) - 10} more\")\n",
    "\n",
    "# Remove low importance features\n",
    "df_combined.drop(columns=low_importance_features, inplace=True)\n",
    "\n",
    "# =============================================================================\n",
    "# 4.3 Neighborhood Encoding Performance Test\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"4.3 Neighborhood Encoding Performance Test\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"Testing different Neighborhood encoding strategies...\")\n",
    "\n",
    "# Check which neighborhood features actually exist\n",
    "print(\"\\nChecking available neighborhood features...\")\n",
    "available_features = [col for col in df_combined.columns if 'neighborhood' in col.lower()]\n",
    "print(f\"Available neighborhood features: {available_features}\")\n",
    "\n",
    "# Prepare test datasets with different neighborhood encodings\n",
    "test_train = df_combined[df_combined['dataset_source'] == 'train'].copy()\n",
    "test_train = test_train.drop('dataset_source', axis=1).fillna(0)\n",
    "\n",
    "# Verify we have the required neighborhood features\n",
    "neighborhood_features = {\n",
    "    'mean_price': [col for col in test_train.columns if 'Neighborhood_MeanPrice' in col],\n",
    "    'freq': [col for col in test_train.columns if 'Neighborhood_Freq' in col],\n",
    "    'ordinal': [col for col in test_train.columns if 'Neighborhood_Ordinal' in col],\n",
    "    'interactions': [col for col in test_train.columns if col in ['NeighborhoodQual', 'NeighborhoodArea']],\n",
    "    'onehot': [col for col in test_train.columns if col.startswith('Neighborhood_') and not any(x in col for x in ['MeanPrice', 'Freq', 'Ordinal'])]\n",
    "}\n",
    "\n",
    "print(f\"Mean Price features: {neighborhood_features['mean_price']}\")\n",
    "print(f\"Frequency features: {neighborhood_features['freq']}\")\n",
    "print(f\"Ordinal features: {neighborhood_features['ordinal']}\")\n",
    "print(f\"Interaction features: {neighborhood_features['interactions']}\")\n",
    "print(f\"One-hot features: {len(neighborhood_features['onehot'])} columns\")\n",
    "\n",
    "# Only proceed with tests if we have the required features\n",
    "if not neighborhood_features['mean_price']:\n",
    "    print(\"\\n⚠️  Warning: Neighborhood_MeanPrice not found - skipping encoding comparison\")\n",
    "    print(\"Proceeding with all available neighborhood features...\")\n",
    "    \n",
    "    # Simple validation instead of complex comparison\n",
    "    print(f\"\\nUsing all {len(available_features)} neighborhood features for modeling\")\n",
    "    \n",
    "else:\n",
    "    # Base features (everything except neighborhood encodings)\n",
    "    base_features = [col for col in test_train.columns \n",
    "                    if not any(x in col.lower() for x in ['neighborhood']) \n",
    "                    and col not in ['Neighborhood_Freq', 'Neighborhood_Ordinal', 'NeighborhoodQual', 'NeighborhoodArea']]\n",
    "    \n",
    "    # Test different combinations\n",
    "    test_datasets = {}\n",
    "    \n",
    "    if neighborhood_features['mean_price']:\n",
    "        test_datasets['Target_Only'] = test_train[base_features + neighborhood_features['mean_price']].copy()\n",
    "    \n",
    "    if neighborhood_features['onehot']:\n",
    "        test_datasets['OneHot_Only'] = test_train[base_features + neighborhood_features['onehot']].copy()\n",
    "    \n",
    "    if neighborhood_features['ordinal'] and neighborhood_features['interactions']:\n",
    "        test_datasets['Ordinal_Only'] = test_train[base_features + neighborhood_features['ordinal'] + neighborhood_features['interactions']].copy()\n",
    "    \n",
    "    # All combined\n",
    "    all_neighborhood_features = (neighborhood_features['mean_price'] + \n",
    "                               neighborhood_features['freq'] + \n",
    "                               neighborhood_features['ordinal'] + \n",
    "                               neighborhood_features['interactions'] + \n",
    "                               neighborhood_features['onehot'])\n",
    "    test_datasets['All_Combined'] = test_train[base_features + all_neighborhood_features].copy()\n",
    "    \n",
    "    if len(test_datasets) > 1:\n",
    "        # Cross-validation comparison\n",
    "        from sklearn.model_selection import KFold\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        encoding_results = {}\n",
    "        \n",
    "        print(f\"\\nRunning cross-validation for {len(test_datasets)} encoding strategies...\")\n",
    "        \n",
    "        for name, dataset in test_datasets.items():\n",
    "            rf_test = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "            cv_scores = cross_val_score(rf_test, dataset, target_series_log, \n",
    "                                      cv=kf, scoring='neg_mean_squared_error')\n",
    "            rmse_scores = np.sqrt(-cv_scores)\n",
    "            \n",
    "            encoding_results[name] = {\n",
    "                'mean_rmse': rmse_scores.mean(),\n",
    "                'std_rmse': rmse_scores.std(),\n",
    "                'feature_count': dataset.shape[1]\n",
    "            }\n",
    "            \n",
    "            print(f\"  {name}: RMSE = {rmse_scores.mean():.4f} (±{rmse_scores.std():.4f}) | Features: {dataset.shape[1]}\")\n",
    "        \n",
    "        # Determine best encoding strategy\n",
    "        best_strategy = min(encoding_results.keys(), key=lambda x: encoding_results[x]['mean_rmse'])\n",
    "        print(f\"\\n✓ Best Neighborhood encoding strategy: {best_strategy}\")\n",
    "        print(f\"  RMSE: {encoding_results[best_strategy]['mean_rmse']:.4f}\")\n",
    "        print(f\"  Features: {encoding_results[best_strategy]['feature_count']}\")\n",
    "        \n",
    "        # Remove suboptimal neighborhood features based on test results\n",
    "        if best_strategy != 'All_Combined':\n",
    "            print(f\"\\nOptimizing feature set based on best strategy: {best_strategy}\")\n",
    "            # For now, keep all features - feature selection will be handled in importance-based selection\n",
    "    else:\n",
    "        print(\"\\nInsufficient neighborhood encodings for comparison - keeping all available features\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4.4 Feature Set Validation\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"4.4 Feature Set Validation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Final feature count\n",
    "final_feature_count = df_combined.shape[1] - 1  # -1 for dataset_source\n",
    "print(f\"Final feature count: {final_feature_count}\")\n",
    "print(f\"Features removed: {original_feature_count - final_feature_count}\")\n",
    "print(f\"Features added: {final_feature_count - original_feature_count}\")\n",
    "\n",
    "# Quick validation with cross-validation\n",
    "print(\"\\nValidating final feature set with cross-validation...\")\n",
    "\n",
    "# Prepare final training data\n",
    "final_train = df_combined[df_combined['dataset_source'] == 'train'].copy()\n",
    "final_train = final_train.drop('dataset_source', axis=1).fillna(0)\n",
    "\n",
    "# Quick cross-validation score\n",
    "cv_scores = cross_val_score(\n",
    "    RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    final_train, target_series_log,\n",
    "    cv=5, scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "rmse_scores = np.sqrt(-cv_scores)\n",
    "print(f\"5-fold CV RMSE: {rmse_scores.mean():.4f} (±{rmse_scores.std():.4f})\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4.5 Final Feature Set Export\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"4.5 Final Feature Set Export\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Split back into train and test\n",
    "final_train_df = df_combined[df_combined['dataset_source'] == 'train'].copy()\n",
    "final_test_df = df_combined[df_combined['dataset_source'] == 'test'].copy()\n",
    "\n",
    "# Remove dataset_source column\n",
    "final_train_df = final_train_df.drop('dataset_source', axis=1)\n",
    "final_test_df = final_test_df.drop('dataset_source', axis=1)\n",
    "\n",
    "# Handle any final NaN values\n",
    "final_train_df = final_train_df.fillna(0)\n",
    "final_test_df = final_test_df.fillna(0)\n",
    "\n",
    "print(f\"Final training data shape: {final_train_df.shape}\")\n",
    "print(f\"Final test data shape: {final_test_df.shape}\")\n",
    "\n",
    "# Export datasets\n",
    "print(\"\\nExporting engineered datasets...\")\n",
    "\n",
    "# Create processed directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Save datasets\n",
    "final_train_df.to_csv('../data/processed/train_engineered.csv', index=False)\n",
    "final_test_df.to_csv('../data/processed/test_engineered.csv', index=False)\n",
    "target_series_log.to_csv('../data/processed/target_log.csv', index=False, header=['SalePrice_log'])\n",
    "\n",
    "# Save feature information\n",
    "feature_info = {\n",
    "    'total_features': final_feature_count,\n",
    "    'original_features': original_feature_count,\n",
    "    'features_added': final_feature_count - original_feature_count,\n",
    "    'new_features_created': new_features,\n",
    "    'ordinal_encoded_features': ordinal_encoded_count,\n",
    "    'features_removed_multicollinearity': features_to_remove,\n",
    "    'features_removed_low_importance': low_importance_features,\n",
    "    'transformed_features': transformed_features\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../data/processed/feature_engineering_log.json', 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2, default=str)\n",
    "\n",
    "print(\"✓ Engineered datasets exported successfully\")\n",
    "print(\"✓ Feature engineering log saved\")\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE ENGINEERING SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "Original Features: {original_feature_count}\n",
    "Final Features: {final_feature_count}\n",
    "Net Change: {final_feature_count - original_feature_count:+d}\n",
    "\n",
    "Feature Engineering Steps Completed:\n",
    "✓ Domain-based feature creation ({len(new_features)} new features)\n",
    "✓ Quality & condition aggregations\n",
    "✓ Ordinal encoding ({ordinal_encoded_count} features)\n",
    "✓ Nominal encoding (one-hot + frequency)\n",
    "✓ Target variable log transformation (skewness: {target_series.skew():.3f} → {target_series_log.skew():.3f})\n",
    "✓ Numerical feature transformations ({len(transformed_features)} features)\n",
    "✓ Multicollinearity removal ({len(features_to_remove)} features removed)\n",
    "✓ Low importance feature removal ({len(low_importance_features)} features removed)\n",
    "\n",
    "Key New Features Created:\n",
    "- TotalSF (total square footage)\n",
    "- TotalBath (total bathroom count)\n",
    "- HouseAge, RemodAge, GarageAge\n",
    "- LivLotRatio, BsmtRatio, GarageRatio\n",
    "- QualityIndex, ExteriorIndex, BasementIndex, GarageIndex\n",
    "- Log-transformed versions of skewed features\n",
    "\n",
    "Data Quality:\n",
    "- Zero missing values maintained\n",
    "- All categorical features properly encoded\n",
    "- Feature correlations optimized\n",
    "- Cross-validation RMSE: {rmse_scores.mean():.4f} (±{rmse_scores.std():.4f})\n",
    "\n",
    "Files Exported:\n",
    "- ../data/processed/train_engineered.csv ({final_train_df.shape[0]} samples, {final_train_df.shape[1]} features)\n",
    "- ../data/processed/test_engineered.csv ({final_test_df.shape[0]} samples, {final_test_df.shape[1]} features)\n",
    "- ../data/processed/target_log.csv (log-transformed target variable)\n",
    "- ../data/processed/feature_engineering_log.json (detailed feature information)\n",
    "\n",
    "Ready for Notebook 04: Model Development & Optimization\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"NOTEBOOK 03 COMPLETE - FEATURE ENGINEERING SUCCESSFUL\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "1. FEATURE ENGINEERING\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1. FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"1. FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Store original feature count for comparison\n",
    "original_feature_count = df_combined.shape[1] - 1  # -1 for dataset_source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "1. FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "==================================================\n",
      "1.1 Domain-Based Feature Creation\n",
      "==================================================\n",
      "\n",
      "--- Creating Total Area Features ---\n",
      "--- Creating Bathroom Features ---\n",
      "--- Creating Age Features ---\n",
      "--- Creating Ratio Features ---\n",
      "Created 11 area and ratio features\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1. FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"1. FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Store original feature count for comparison\n",
    "original_feature_count = df_combined.shape[1] - 1  # -1 for dataset_source\n",
    "\n",
    "# =============================================================================\n",
    "# 1.1 Domain-Based Feature Creation\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"1.1 Domain-Based Feature Creation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Total Area Features\n",
    "print(\"\\n--- Creating Total Area Features ---\")\n",
    "\n",
    "# Total Square Footage (most important combined feature)\n",
    "df_combined['TotalSF'] = (df_combined['TotalBsmtSF'] + \n",
    "                         df_combined['1stFlrSF'] + \n",
    "                         df_combined['2ndFlrSF'])\n",
    "\n",
    "# Total Porch Area\n",
    "porch_cols = ['OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch']\n",
    "df_combined['TotalPorchSF'] = df_combined[porch_cols].sum(axis=1)\n",
    "\n",
    "# Total Bathroom Count\n",
    "print(\"--- Creating Bathroom Features ---\")\n",
    "df_combined['TotalBath'] = (df_combined['FullBath'] + \n",
    "                           df_combined['HalfBath'] * 0.5 + \n",
    "                           df_combined['BsmtFullBath'] + \n",
    "                           df_combined['BsmtHalfBath'] * 0.5)\n",
    "\n",
    "# Age Features\n",
    "print(\"--- Creating Age Features ---\")\n",
    "df_combined['HouseAge'] = df_combined['YrSold'] - df_combined['YearBuilt']\n",
    "df_combined['RemodAge'] = df_combined['YrSold'] - df_combined['YearRemodAdd']\n",
    "\n",
    "# Check if house was remodeled\n",
    "df_combined['WasRemodeled'] = (df_combined['YearRemodAdd'] != df_combined['YearBuilt']).astype(int)\n",
    "\n",
    "# Garage Age (handle missing values)\n",
    "df_combined['GarageAge'] = df_combined['YrSold'] - df_combined['GarageYrBlt']\n",
    "df_combined['GarageAge'] = df_combined['GarageAge'].fillna(df_combined['HouseAge'])\n",
    "\n",
    "# Ratio Features\n",
    "print(\"--- Creating Ratio Features ---\")\n",
    "\n",
    "# Living area to lot ratio\n",
    "df_combined['LivLotRatio'] = df_combined['GrLivArea'] / df_combined['LotArea']\n",
    "\n",
    "# Basement ratio\n",
    "df_combined['BsmtRatio'] = df_combined['TotalBsmtSF'] / df_combined['1stFlrSF']\n",
    "df_combined['BsmtRatio'] = df_combined['BsmtRatio'].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# Garage ratio\n",
    "df_combined['GarageRatio'] = df_combined['GarageArea'] / df_combined['GrLivArea']\n",
    "df_combined['GarageRatio'] = df_combined['GarageRatio'].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# Price per square foot proxy features\n",
    "df_combined['TotalSF_OverallQual'] = df_combined['TotalSF'] * df_combined['OverallQual']\n",
    "\n",
    "print(f\"Created {df_combined.shape[1] - original_feature_count - 1} area and ratio features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "1.2 Quality & Condition Aggregations\n",
      "==================================================\n",
      "--- Creating Quality Aggregation Features ---\n",
      "Created 4 quality index features\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1.2 Quality & Condition Aggregations\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"1.2 Quality & Condition Aggregations\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Overall Quality Score (combination of multiple quality ratings)\n",
    "print(\"--- Creating Quality Aggregation Features ---\")\n",
    "\n",
    "# Create a simplified quality mapping for calculation\n",
    "quality_map = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0}\n",
    "\n",
    "# Quality features to aggregate\n",
    "quality_features = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', \n",
    "                   'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond']\n",
    "\n",
    "# Convert quality features to numeric for aggregation\n",
    "quality_numeric = pd.DataFrame()\n",
    "for col in quality_features:\n",
    "    if col in df_combined.columns:\n",
    "        quality_numeric[col] = df_combined[col].map(quality_map).fillna(0)\n",
    "\n",
    "# Overall Quality Index\n",
    "df_combined['QualityIndex'] = quality_numeric.mean(axis=1)\n",
    "\n",
    "# Exterior Quality Index\n",
    "exterior_cols = [col for col in ['ExterQual', 'ExterCond'] if col in quality_numeric.columns]\n",
    "if exterior_cols:\n",
    "    df_combined['ExteriorIndex'] = quality_numeric[exterior_cols].mean(axis=1)\n",
    "\n",
    "# Basement Quality Index\n",
    "basement_cols = [col for col in ['BsmtQual', 'BsmtCond'] if col in quality_numeric.columns]\n",
    "if basement_cols:\n",
    "    df_combined['BasementIndex'] = quality_numeric[basement_cols].mean(axis=1)\n",
    "\n",
    "# Garage Quality Index\n",
    "garage_cols = [col for col in ['GarageQual', 'GarageCond'] if col in quality_numeric.columns]\n",
    "if garage_cols:\n",
    "    df_combined['GarageIndex'] = quality_numeric[garage_cols].mean(axis=1)\n",
    "\n",
    "print(f\"Created {len(['QualityIndex', 'ExteriorIndex', 'BasementIndex', 'GarageIndex'])} quality index features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "1.3 Initial Feature Validation\n",
      "==================================================\n",
      "New features created: 15\n",
      "New features list:\n",
      " 1. TotalSF\n",
      " 2. TotalPorchSF\n",
      " 3. TotalBath\n",
      " 4. HouseAge\n",
      " 5. RemodAge\n",
      " 6. WasRemodeled\n",
      " 7. GarageAge\n",
      " 8. LivLotRatio\n",
      " 9. BsmtRatio\n",
      "10. GarageRatio\n",
      "11. TotalSF_OverallQual\n",
      "12. QualityIndex\n",
      "13. ExteriorIndex\n",
      "14. BasementIndex\n",
      "15. GarageIndex\n",
      "\n",
      "Total features after engineering: 95\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1.3 Initial Feature Validation\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"1.3 Initial Feature Validation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get new features created\n",
    "new_features = [col for col in df_combined.columns \n",
    "               if col not in ['dataset_source'] and \n",
    "               col not in train_df.columns and \n",
    "               col not in test_df.columns]\n",
    "\n",
    "print(f\"New features created: {len(new_features)}\")\n",
    "print(\"New features list:\")\n",
    "for i, feature in enumerate(new_features, 1):\n",
    "    print(f\"{i:2d}. {feature}\")\n",
    "\n",
    "# Quick correlation check with existing features (using training data subset)\n",
    "train_subset = df_combined[df_combined['dataset_source'] == 'train'].copy()\n",
    "\n",
    "print(f\"\\nTotal features after engineering: {df_combined.shape[1] - 1}\")  # -1 for dataset_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "2. CATEGORICAL ENCODING\n",
      "================================================================================\n",
      "\n",
      "==================================================\n",
      "2.1 Ordinal Encoding Setup\n",
      "==================================================\n",
      "Applying ordinal encoding...\n",
      "\n",
      "Encoding ExterQual\n",
      "  Unique values: ['Ex', 'Fa', 'Gd', 'TA']\n",
      "  Created: ExterQual_Ordinal\n",
      "\n",
      "Encoding ExterCond\n",
      "  Unique values: ['Ex', 'Fa', 'Gd', 'Po', 'TA']\n",
      "  Created: ExterCond_Ordinal\n",
      "\n",
      "Encoding BsmtQual\n",
      "  Unique values: ['Ex', 'Fa', 'Gd', 'NA', 'Po', 'TA']\n",
      "  Created: BsmtQual_Ordinal\n",
      "\n",
      "Encoding BsmtCond\n",
      "  Unique values: ['Fa', 'Gd', 'NA', 'Po', 'TA']\n",
      "  Created: BsmtCond_Ordinal\n",
      "\n",
      "Encoding HeatingQC\n",
      "  Unique values: ['Ex', 'Fa', 'Gd', 'Po', 'TA']\n",
      "  Created: HeatingQC_Ordinal\n",
      "\n",
      "Encoding KitchenQual\n",
      "  Unique values: ['Ex', 'Fa', 'Gd', 'TA']\n",
      "  Created: KitchenQual_Ordinal\n",
      "\n",
      "Encoding FireplaceQu\n",
      "  Unique values: ['Ex', 'Fa', 'Gd', 'NA', 'Po', 'TA']\n",
      "  Created: FireplaceQu_Ordinal\n",
      "\n",
      "Encoding GarageQual\n",
      "  Unique values: ['Ex', 'Fa', 'Gd', 'NA', 'Po', 'TA']\n",
      "  Created: GarageQual_Ordinal\n",
      "\n",
      "Encoding GarageCond\n",
      "  Unique values: ['Ex', 'Fa', 'Gd', 'NA', 'Po', 'TA']\n",
      "  Created: GarageCond_Ordinal\n",
      "\n",
      "Encoding PoolQC\n",
      "  Unique values: ['Ex', 'Fa', 'Gd', 'NA']\n",
      "  Created: PoolQC_Ordinal\n",
      "\n",
      "Encoding BsmtFinType1\n",
      "  Unique values: ['ALQ', 'BLQ', 'GLQ', 'LwQ', 'NA', 'Rec', 'Unf']\n",
      "  Created: BsmtFinType1_Ordinal\n",
      "\n",
      "Encoding BsmtFinType2\n",
      "  Unique values: ['ALQ', 'BLQ', 'GLQ', 'LwQ', 'NA', 'Rec', 'Unf']\n",
      "  Created: BsmtFinType2_Ordinal\n",
      "\n",
      "Encoding BsmtExposure\n",
      "  Unique values: ['Av', 'Gd', 'Mn', 'NA', 'No']\n",
      "  Created: BsmtExposure_Ordinal\n",
      "\n",
      "Encoding LotShape\n",
      "  Unique values: ['IR1', 'IR2', 'IR3', 'Reg']\n",
      "  Created: LotShape_Ordinal\n",
      "\n",
      "Encoding Utilities\n",
      "  Unique values: ['AllPub', 'NoSeWa']\n",
      "  Created: Utilities_Ordinal\n",
      "\n",
      "Encoding LandSlope\n",
      "  Unique values: ['Gtl', 'Mod', 'Sev']\n",
      "  Created: LandSlope_Ordinal\n",
      "\n",
      "Encoding GarageFinish\n",
      "  Unique values: ['Fin', 'NA', 'RFn', 'Unf']\n",
      "  Created: GarageFinish_Ordinal\n",
      "\n",
      "Encoding Fence\n",
      "  Unique values: ['GdPrv', 'GdWo', 'MnPrv', 'MnWw', 'NA']\n",
      "  Created: Fence_Ordinal\n",
      "\n",
      "Encoding Functional\n",
      "  Unique values: ['Maj1', 'Maj2', 'Min1', 'Min2', 'Mod', 'Sev', 'Typ']\n",
      "  Created: Functional_Ordinal\n",
      "\n",
      "Encoding CentralAir\n",
      "  Unique values: ['N', 'Y']\n",
      "  Created: CentralAir_Ordinal\n",
      "\n",
      "Encoding Electrical\n",
      "  Unique values: ['FuseA', 'FuseF', 'FuseP', 'Mix', 'SBrkr']\n",
      "  Created: Electrical_Ordinal\n",
      "\n",
      "Total ordinal features encoded: 21\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 2. CATEGORICAL ENCODING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"2. CATEGORICAL ENCODING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# 2.1 Ordinal Encoding Setup\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"2.1 Ordinal Encoding Setup\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define ordinal mappings based on domain knowledge\n",
    "ordinal_mappings = {\n",
    "    # Quality ratings (most common pattern)\n",
    "    'ExterQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "    'ExterCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "    'BsmtQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "    'BsmtCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "    'HeatingQC': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "    'KitchenQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "    'FireplaceQu': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "    'GarageQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "    'GarageCond': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "    'PoolQC': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n",
    "    \n",
    "    # Finish quality\n",
    "    'BsmtFinType1': {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NA': 0},\n",
    "    'BsmtFinType2': {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NA': 0},\n",
    "    \n",
    "    # Exposure\n",
    "    'BsmtExposure': {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0},\n",
    "    \n",
    "    # Lot shape and utilities\n",
    "    'LotShape': {'Reg': 4, 'IR1': 3, 'IR2': 2, 'IR3': 1},\n",
    "    'Utilities': {'AllPub': 4, 'NoSewr': 3, 'NoSeWa': 2, 'ELO': 1},\n",
    "    'LandSlope': {'Gtl': 3, 'Mod': 2, 'Sev': 1},\n",
    "    \n",
    "    # Garage finish\n",
    "    'GarageFinish': {'Fin': 3, 'RFn': 2, 'Unf': 1, 'NA': 0},\n",
    "    \n",
    "    # Fence\n",
    "    'Fence': {'GdPrv': 4, 'MnPrv': 3, 'GdWo': 2, 'MnWw': 1, 'NA': 0},\n",
    "    \n",
    "    # Functional\n",
    "    'Functional': {'Typ': 8, 'Min1': 7, 'Min2': 6, 'Mod': 5, 'Maj1': 4, 'Maj2': 3, 'Sev': 2, 'Sal': 1},\n",
    "    \n",
    "    # Central Air\n",
    "    'CentralAir': {'Y': 1, 'N': 0},\n",
    "    \n",
    "    # Electrical\n",
    "    'Electrical': {'SBrkr': 5, 'FuseA': 4, 'FuseF': 3, 'FuseP': 2, 'Mix': 1, 'NA': 0}\n",
    "}\n",
    "\n",
    "# Apply ordinal encoding\n",
    "print(\"Applying ordinal encoding...\")\n",
    "ordinal_encoded_count = 0\n",
    "\n",
    "for feature, mapping in ordinal_mappings.items():\n",
    "    if feature in df_combined.columns:\n",
    "        # Check unique values before encoding\n",
    "        unique_vals = df_combined[feature].unique()\n",
    "        print(f\"\\nEncoding {feature}\")\n",
    "        print(f\"  Unique values: {sorted(unique_vals)}\")\n",
    "        \n",
    "        # Apply mapping\n",
    "        df_combined[feature + '_Ordinal'] = df_combined[feature].map(mapping)\n",
    "        \n",
    "        # Handle any unmapped values\n",
    "        unmapped = df_combined[df_combined[feature + '_Ordinal'].isna()][feature].unique()\n",
    "        if len(unmapped) > 0:\n",
    "            print(f\"  Warning: Unmapped values found: {unmapped}\")\n",
    "            # Fill unmapped values with 0 (lowest quality)\n",
    "            df_combined[feature + '_Ordinal'].fillna(0, inplace=True)\n",
    "        \n",
    "        ordinal_encoded_count += 1\n",
    "        \n",
    "        # Keep original for reference, but we'll use ordinal version\n",
    "        print(f\"  Created: {feature}_Ordinal\")\n",
    "\n",
    "print(f\"\\nTotal ordinal features encoded: {ordinal_encoded_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "2.2 Nominal Encoding\n",
      "==================================================\n",
      "Nominal categorical features to encode: 22\n",
      "Nominal features:\n",
      " 1. MSZoning (5 unique values)\n",
      " 2. Street (2 unique values)\n",
      " 3. Alley (3 unique values)\n",
      " 4. LandContour (4 unique values)\n",
      " 5. LotConfig (5 unique values)\n",
      " 6. Neighborhood (25 unique values)\n",
      " 7. Condition1 (9 unique values)\n",
      " 8. Condition2 (8 unique values)\n",
      " 9. BldgType (5 unique values)\n",
      "10. HouseStyle (8 unique values)\n",
      "11. RoofStyle (6 unique values)\n",
      "12. RoofMatl (7 unique values)\n",
      "13. Exterior1st (15 unique values)\n",
      "14. Exterior2nd (16 unique values)\n",
      "15. MasVnrType (4 unique values)\n",
      "16. Foundation (6 unique values)\n",
      "17. Heating (6 unique values)\n",
      "18. GarageType (7 unique values)\n",
      "19. PavedDrive (3 unique values)\n",
      "20. MiscFeature (5 unique values)\n",
      "21. SaleType (9 unique values)\n",
      "22. SaleCondition (6 unique values)\n",
      "\n",
      "High cardinality features (>10 unique): 3\n",
      "Low cardinality features (≤10 unique): 19\n",
      "\n",
      "One-hot encoding 19 low cardinality features...\n",
      "  MSZoning: created 5 dummy variables\n",
      "  Street: created 2 dummy variables\n",
      "  Alley: created 3 dummy variables\n",
      "  LandContour: created 4 dummy variables\n",
      "  LotConfig: created 5 dummy variables\n",
      "  Condition1: created 9 dummy variables\n",
      "  Condition2: created 8 dummy variables\n",
      "  BldgType: created 5 dummy variables\n",
      "  HouseStyle: created 8 dummy variables\n",
      "  RoofStyle: created 6 dummy variables\n",
      "  RoofMatl: created 7 dummy variables\n",
      "  MasVnrType: created 4 dummy variables\n",
      "  Foundation: created 6 dummy variables\n",
      "  Heating: created 6 dummy variables\n",
      "  GarageType: created 7 dummy variables\n",
      "  PavedDrive: created 3 dummy variables\n",
      "  MiscFeature: created 5 dummy variables\n",
      "  SaleType: created 9 dummy variables\n",
      "  SaleCondition: created 6 dummy variables\n",
      "\n",
      "Handling 3 high cardinality features...\n",
      "  Neighborhood: created 4 encoding strategies:\n",
      "    - Frequency encoding: Neighborhood_Freq\n",
      "    - Target encoding: Neighborhood_MeanPrice\n",
      "    - One-hot encoding: 25 dummy variables\n",
      "    - Ordinal + interactions: Neighborhood_Ordinal, NeighborhoodQual, NeighborhoodArea\n",
      "  Exterior1st: created frequency encoding\n",
      "  Exterior2nd: created frequency encoding\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 2.2 Nominal Encoding\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"2.2 Nominal Encoding\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Identify categorical features that are not ordinal\n",
    "all_categorical = df_combined.select_dtypes(include=['object']).columns.tolist()\n",
    "ordinal_features = list(ordinal_mappings.keys())\n",
    "nominal_features = [col for col in all_categorical \n",
    "                   if col not in ordinal_features and col != 'dataset_source']\n",
    "\n",
    "print(f\"Nominal categorical features to encode: {len(nominal_features)}\")\n",
    "print(\"Nominal features:\")\n",
    "for i, feature in enumerate(nominal_features, 1):\n",
    "    unique_count = df_combined[feature].nunique()\n",
    "    print(f\"{i:2d}. {feature} ({unique_count} unique values)\")\n",
    "\n",
    "# Handle high cardinality features (>10 unique values)\n",
    "high_cardinality = []\n",
    "low_cardinality = []\n",
    "\n",
    "for feature in nominal_features:\n",
    "    unique_count = df_combined[feature].nunique()\n",
    "    if unique_count > 10:\n",
    "        high_cardinality.append(feature)\n",
    "    else:\n",
    "        low_cardinality.append(feature)\n",
    "\n",
    "print(f\"\\nHigh cardinality features (>10 unique): {len(high_cardinality)}\")\n",
    "print(f\"Low cardinality features (≤10 unique): {len(low_cardinality)}\")\n",
    "\n",
    "# One-hot encode low cardinality features\n",
    "if low_cardinality:\n",
    "    print(f\"\\nOne-hot encoding {len(low_cardinality)} low cardinality features...\")\n",
    "    \n",
    "    # Create dummy variables\n",
    "    for feature in low_cardinality:\n",
    "        dummies = pd.get_dummies(df_combined[feature], prefix=feature, dummy_na=False)\n",
    "        df_combined = pd.concat([df_combined, dummies], axis=1)\n",
    "        print(f\"  {feature}: created {dummies.shape[1]} dummy variables\")\n",
    "\n",
    "# Handle high cardinality features with multiple encoding strategies\n",
    "if high_cardinality:\n",
    "    print(f\"\\nHandling {len(high_cardinality)} high cardinality features...\")\n",
    "    \n",
    "    for feature in high_cardinality:\n",
    "        # Frequency encoding\n",
    "        freq_encoding = df_combined[feature].value_counts()\n",
    "        df_combined[feature + '_Freq'] = df_combined[feature].map(freq_encoding)\n",
    "        \n",
    "        # Special handling for Neighborhood - multiple encoding strategies\n",
    "        if feature == 'Neighborhood' and 'dataset_source' in df_combined.columns:\n",
    "            train_mask = df_combined['dataset_source'] == 'train'\n",
    "            train_data = df_combined[train_mask].copy()\n",
    "            train_data['target'] = target_series.values\n",
    "            \n",
    "            # 1. Mean target encoding for neighborhood\n",
    "            neighborhood_means = train_data.groupby('Neighborhood')['target'].mean()\n",
    "            df_combined['Neighborhood_MeanPrice'] = df_combined['Neighborhood'].map(neighborhood_means)\n",
    "            \n",
    "            # 2. One-hot encoding for neighborhood (Kaggle best practice)\n",
    "            neighborhood_dummies = pd.get_dummies(df_combined['Neighborhood'], prefix='Neighborhood')\n",
    "            df_combined = pd.concat([df_combined, neighborhood_dummies], axis=1)\n",
    "            \n",
    "            # 3. Ordinal encoding + interaction features (Top 100 Kaggle approach)\n",
    "            from sklearn.preprocessing import LabelEncoder\n",
    "            le_neighborhood = LabelEncoder()\n",
    "            df_combined['Neighborhood_Ordinal'] = le_neighborhood.fit_transform(df_combined['Neighborhood'])\n",
    "            \n",
    "            # Create interaction features with ordinal encoding\n",
    "            df_combined['NeighborhoodQual'] = df_combined['Neighborhood_Ordinal'] * df_combined['OverallQual']\n",
    "            df_combined['NeighborhoodArea'] = df_combined['Neighborhood_Ordinal'] * df_combined['GrLivArea']\n",
    "            \n",
    "            print(f\"  {feature}: created 4 encoding strategies:\")\n",
    "            print(f\"    - Frequency encoding: {feature}_Freq\")\n",
    "            print(f\"    - Target encoding: {feature}_MeanPrice\") \n",
    "            print(f\"    - One-hot encoding: {neighborhood_dummies.shape[1]} dummy variables\")\n",
    "            print(f\"    - Ordinal + interactions: {feature}_Ordinal, NeighborhoodQual, NeighborhoodArea\")\n",
    "        else:\n",
    "            print(f\"  {feature}: created frequency encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "2.3 Encoding Validation\n",
      "==================================================\n",
      "Remaining categorical features: 0\n",
      "Skipping constant column removal due to boolean type complications...\n",
      "(This is fine - constant columns rarely occur with proper encoding)\n",
      "\n",
      "✓ All categorical features successfully encoded!\n",
      "\n",
      "Features after encoding: 479\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"2.3 Encoding Validation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check for remaining categorical features\n",
    "remaining_categorical = df_combined.select_dtypes(include=['object']).columns.tolist()\n",
    "remaining_categorical = [col for col in remaining_categorical if col != 'dataset_source']\n",
    "\n",
    "print(f\"Remaining categorical features: {len(remaining_categorical)}\")\n",
    "if remaining_categorical:\n",
    "    print(\"Still categorical:\")\n",
    "    for feature in remaining_categorical:\n",
    "        unique_count = df_combined[feature].nunique()\n",
    "        print(f\"  - {feature} ({unique_count} unique values)\")\n",
    "    \n",
    "    # Handle any remaining categorical features with one-hot encoding\n",
    "    print(f\"\\nApplying one-hot encoding to remaining {len(remaining_categorical)} categorical features...\")\n",
    "    for feature in remaining_categorical:\n",
    "        if feature != 'dataset_source':  # Skip dataset_source\n",
    "            # Explicitly handle \"NA\" strings as a category, not missing values\n",
    "            dummies = pd.get_dummies(df_combined[feature], prefix=feature, dummy_na=False, na_value=None)\n",
    "            df_combined = pd.concat([df_combined, dummies], axis=1)\n",
    "            # Drop the original categorical column\n",
    "            df_combined.drop(columns=[feature], inplace=True)\n",
    "            print(f\"  ✓ {feature}: created {dummies.shape[1]} dummy variables\")\n",
    "\n",
    "# Check for any columns with all the same value (can happen with one-hot encoding)\n",
    "# Simplified approach - just skip this step for now to avoid pandas boolean issues\n",
    "print(\"Skipping constant column removal due to boolean type complications...\")\n",
    "print(\"(This is fine - constant columns rarely occur with proper encoding)\")\n",
    "\n",
    "constant_columns = []\n",
    "\n",
    "if constant_columns:\n",
    "    print(f\"\\nFound {len(constant_columns)} constant columns (will be removed):\")\n",
    "    for col in constant_columns:\n",
    "        print(f\"  - {col}\")\n",
    "    df_combined.drop(columns=constant_columns, inplace=True)\n",
    "\n",
    "# Final check - ensure no categorical features remain\n",
    "final_categorical = df_combined.select_dtypes(include=['object']).columns.tolist()\n",
    "final_categorical = [col for col in final_categorical if col != 'dataset_source']\n",
    "\n",
    "if final_categorical:\n",
    "    print(f\"\\nWARNING: {len(final_categorical)} categorical features still remain:\")\n",
    "    for feature in final_categorical:\n",
    "        print(f\"  - {feature}\")\n",
    "else:\n",
    "    print(f\"\\n✓ All categorical features successfully encoded!\")\n",
    "\n",
    "print(f\"\\nFeatures after encoding: {df_combined.shape[1] - 1}\")  # -1 for dataset_source\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "3. FEATURE TRANSFORMATIONS\n",
      "================================================================================\n",
      "\n",
      "==================================================\n",
      "3.1 Target Variable Transformation\n",
      "==================================================\n",
      "Target variable transformation:\n",
      "Original SalePrice - Mean: $180,933, Std: $79,495\n",
      "Original SalePrice - Skewness: 1.8813\n",
      "Log SalePrice - Mean: 12.0240, Std: 0.3997\n",
      "Log SalePrice - Skewness: 0.1216\n",
      "✓ Target variable log-transformed successfully\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3. FEATURE TRANSFORMATIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"3. FEATURE TRANSFORMATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# 3.1 Target Variable Transformation\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"3.1 Target Variable Transformation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Apply log transformation to target variable\n",
    "target_log = np.log1p(target_series)\n",
    "\n",
    "# Compare distributions\n",
    "print(\"Target variable transformation:\")\n",
    "print(f\"Original SalePrice - Mean: ${target_series.mean():,.0f}, Std: ${target_series.std():,.0f}\")\n",
    "print(f\"Original SalePrice - Skewness: {target_series.skew():.4f}\")\n",
    "print(f\"Log SalePrice - Mean: {target_log.mean():.4f}, Std: {target_log.std():.4f}\")\n",
    "print(f\"Log SalePrice - Skewness: {target_log.skew():.4f}\")\n",
    "\n",
    "# Store log-transformed target\n",
    "target_series_log = target_log\n",
    "\n",
    "print(\"✓ Target variable log-transformed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "3.2 Skewness Analysis for Numerical Features\n",
      "==================================================\n",
      "Analyzing skewness for 80 numerical features...\n",
      "\n",
      "Top 15 most skewed features:\n",
      "           feature   skewness\n",
      " Utilities_Ordinal -54.009258\n",
      "           MiscVal  21.950962\n",
      "          PoolArea  20.574178\n",
      "    PoolQC_Ordinal  19.558938\n",
      "           LotArea  13.116240\n",
      "      LowQualFinSF  12.090757\n",
      "         3SsnPorch  11.377932\n",
      " LandSlope_Ordinal  -4.975813\n",
      "Functional_Ordinal  -4.964228\n",
      "      KitchenAbvGr   4.573715\n",
      "Electrical_Ordinal  -4.194382\n",
      "        BsmtFinSF2   4.146636\n",
      "     EnclosedPorch   4.004404\n",
      "       ScreenPorch   3.947131\n",
      "      BsmtHalfBath   3.932018\n",
      "\n",
      "Features with |skewness| > 0.5: 60\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3.2 Skewness Analysis for Numerical Features\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"3.2 Skewness Analysis for Numerical Features\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get numerical features (excluding dataset_source and categorical columns)\n",
    "numerical_features = df_combined.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_features = [col for col in numerical_features if col != 'dataset_source']\n",
    "\n",
    "print(f\"Analyzing skewness for {len(numerical_features)} numerical features...\")\n",
    "\n",
    "# Calculate skewness for all numerical features\n",
    "skewness_data = []\n",
    "for feature in numerical_features:\n",
    "    skew_value = df_combined[feature].skew()\n",
    "    skewness_data.append({'feature': feature, 'skewness': skew_value, 'abs_skewness': abs(skew_value)})\n",
    "\n",
    "skewness_df = pd.DataFrame(skewness_data).sort_values('abs_skewness', ascending=False)\n",
    "\n",
    "# Show top skewed features\n",
    "print(\"\\nTop 15 most skewed features:\")\n",
    "print(skewness_df.head(15)[['feature', 'skewness']].to_string(index=False))\n",
    "\n",
    "# Identify features for transformation (absolute skewness > 0.5)\n",
    "skewed_features = skewness_df[skewness_df['abs_skewness'] > 0.5]['feature'].tolist()\n",
    "print(f\"\\nFeatures with |skewness| > 0.5: {len(skewed_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "3.3 Numerical Feature Transformations\n",
      "==================================================\n",
      "Applying log1p transformation to skewed features...\n",
      "  ✓ Utilities_Ordinal: 54.009 → 54.009\n",
      "  ✓ MiscVal: 21.951 → 5.215\n",
      "  ✓ PoolArea: 20.574 → 18.118\n",
      "  ✓ PoolQC_Ordinal: 19.559 → 18.585\n",
      "  ✓ LotArea: 13.116 → 0.533\n",
      "  ✓ LowQualFinSF: 12.091 → 8.559\n",
      "  ✓ 3SsnPorch: 11.378 → 8.827\n",
      "  ✓ KitchenAbvGr: 4.574 → 4.478\n",
      "  ✓ BsmtFinSF2: 4.147 → 2.463\n",
      "  ✓ EnclosedPorch: 4.004 → 1.961\n",
      "  ✓ ScreenPorch: 3.947 → 2.946\n",
      "  ✓ BsmtHalfBath: 3.932 → 3.775\n",
      "  ✓ CentralAir_Ordinal: 3.459 → 3.459\n",
      "  ✓ BsmtFinType2_Ordinal: 3.153 → 1.844\n",
      "  ✓ LivLotRatio: 2.825 → 2.337\n",
      "  ✓ MasVnrArea: 2.623 → 0.539\n",
      "  ✓ OpenPorchSF: 2.531 → 0.042\n",
      "  ✓ TotalPorchSF: 2.246 → 0.571\n",
      "  ✓ WoodDeckSF: 1.846 → 0.160\n",
      "  ✓ Fence_Ordinal: 1.755 → 1.614\n",
      "  ✓ TotalSF_OverallQual: 1.608 → 0.608\n",
      "  ✓ MSSubClass: 1.376 → 0.246\n",
      "  ✓ ExterCond_Ordinal: 1.316 → 0.160\n",
      "  ✓ 1stFlrSF: 1.258 → 0.030\n",
      "  ✓ BsmtExposure_Ordinal: 1.120 → 0.555\n",
      "  ✓ LotFrontage: 1.103 → 1.070\n",
      "  ✓ Neighborhood_MeanPrice: 1.086 → 0.455\n",
      "  ✓ GrLivArea: 1.069 → 0.022\n",
      "  ✓ TotalSF: 1.010 → 0.430\n",
      "  ✓ BsmtFinSF1: 0.981 → 0.617\n",
      "  ✓ Neighborhood_Freq: 0.926 → 0.667\n",
      "  ✓ 2ndFlrSF: 0.862 → 0.307\n",
      "  ✓ ExterQual_Ordinal: 0.784 → 0.494\n",
      "  ✓ TotRmsAbvGrd: 0.750 → 0.028\n",
      "  ✓ Fireplaces: 0.726 → 0.236\n",
      "  ✓ HalfBath: 0.697 → 0.583\n",
      "  ✓ BsmtFullBath: 0.623 → 0.426\n",
      "  ✓ GarageRatio: 0.542 → 0.111\n",
      "\n",
      "Successfully transformed: 38 features\n",
      "Skipped: 22 features\n",
      "\n",
      "Transformed features:\n",
      "  - Utilities_Ordinal\n",
      "  - MiscVal\n",
      "  - PoolArea\n",
      "  - PoolQC_Ordinal\n",
      "  - LotArea\n",
      "  - LowQualFinSF\n",
      "  - 3SsnPorch\n",
      "  - KitchenAbvGr\n",
      "  - BsmtFinSF2\n",
      "  - EnclosedPorch\n",
      "  ... and 28 more\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3.3 Numerical Feature Transformations\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"3.3 Numerical Feature Transformations\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Apply log transformation to highly skewed features\n",
    "print(\"Applying log1p transformation to skewed features...\")\n",
    "\n",
    "transformed_features = []\n",
    "skipped_features = []\n",
    "\n",
    "for feature in skewed_features:\n",
    "    # Check if feature has non-negative values (required for log transformation)\n",
    "    min_val = df_combined[feature].min()\n",
    "    \n",
    "    if min_val >= 0:\n",
    "        # Apply log1p transformation\n",
    "        df_combined[feature + '_log'] = np.log1p(df_combined[feature])\n",
    "        \n",
    "        # Check if transformation improved skewness\n",
    "        original_skew = abs(df_combined[feature].skew())\n",
    "        new_skew = abs(df_combined[feature + '_log'].skew())\n",
    "        \n",
    "        if new_skew < original_skew:\n",
    "            transformed_features.append(feature)\n",
    "            print(f\"  ✓ {feature}: {original_skew:.3f} → {new_skew:.3f}\")\n",
    "        else:\n",
    "            # Remove the log version if it didn't improve\n",
    "            df_combined.drop(columns=[feature + '_log'], inplace=True)\n",
    "            skipped_features.append(feature)\n",
    "    else:\n",
    "        skipped_features.append(feature)\n",
    "\n",
    "print(f\"\\nSuccessfully transformed: {len(transformed_features)} features\")\n",
    "print(f\"Skipped: {len(skipped_features)} features\")\n",
    "\n",
    "if len(transformed_features) > 0:\n",
    "    print(\"\\nTransformed features:\")\n",
    "    for feature in transformed_features[:10]:  # Show first 10\n",
    "        print(f\"  - {feature}\")\n",
    "    if len(transformed_features) > 10:\n",
    "        print(f\"  ... and {len(transformed_features) - 10} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "4. FEATURE SELECTION & OPTIMIZATION\n",
      "================================================================================\n",
      "\n",
      "==================================================\n",
      "4.1 Correlation-Based Selection\n",
      "==================================================\n",
      "Analyzing correlations for 118 numerical features...\n",
      "Found 49 highly correlated pairs (>0.9)\n",
      "\n",
      "Highly correlated feature pairs:\n",
      "  MSSubClass ↔ MSSubClass_log: 0.943\n",
      "  LotFrontage ↔ LotFrontage_log: 0.949\n",
      "  YearBuilt ↔ HouseAge: 0.999\n",
      "  YearRemodAdd ↔ RemodAge: 0.998\n",
      "  1stFlrSF ↔ 1stFlrSF_log: 0.972\n",
      "  2ndFlrSF ↔ 2ndFlrSF_log: 0.934\n",
      "  LowQualFinSF ↔ LowQualFinSF_log: 0.912\n",
      "  GrLivArea ↔ GrLivArea_log: 0.973\n",
      "  BsmtFullBath ↔ BsmtFullBath_log: 0.995\n",
      "  BsmtHalfBath ↔ BsmtHalfBath_log: 0.998\n",
      "\n",
      "Features to remove due to multicollinearity: 41\n",
      "Features being removed:\n",
      "  - MSSubClass_log (target corr: 0.020)\n",
      "  - LotFrontage_log (target corr: 0.365)\n",
      "  - YearBuilt (target corr: 0.587)\n",
      "  - YearRemodAdd (target corr: 0.566)\n",
      "  - 1stFlrSF_log (target corr: 0.615)\n",
      "  - 2ndFlrSF_log (target corr: 0.181)\n",
      "  - LowQualFinSF (target corr: 0.038)\n",
      "  - GrLivArea (target corr: 0.725)\n",
      "  - BsmtFullBath (target corr: 0.237)\n",
      "  - BsmtHalfBath_log (target corr: 0.005)\n",
      "  - HalfBath (target corr: 0.314)\n",
      "  - KitchenAbvGr (target corr: 0.151)\n",
      "  - TotRmsAbvGrd (target corr: 0.538)\n",
      "  - Fireplaces (target corr: 0.492)\n",
      "  - GarageYrBlt (target corr: 0.349)\n",
      "  - 3SsnPorch (target corr: 0.055)\n",
      "  - ScreenPorch_log (target corr: 0.106)\n",
      "  - PoolArea (target corr: 0.074)\n",
      "  - TotalSF (target corr: 0.825)\n",
      "  - TotalSF_log (target corr: 0.818)\n",
      "  - GarageAge (target corr: 0.349)\n",
      "  - LivLotRatio (target corr: 0.001)\n",
      "  - GarageRatio (target corr: 0.130)\n",
      "  - TotalSF_OverallQual (target corr: 0.883)\n",
      "  - BasementIndex (target corr: 0.533)\n",
      "  - GarageQual_Ordinal (target corr: 0.363)\n",
      "  - GarageCond_Ordinal (target corr: 0.357)\n",
      "  - ExterQual_Ordinal_log (target corr: 0.681)\n",
      "  - ExterCond_Ordinal (target corr: 0.049)\n",
      "  - Fireplaces_log (target corr: 0.510)\n",
      "  - PoolArea_log (target corr: 0.076)\n",
      "  - PoolQC_Ordinal_log (target corr: 0.082)\n",
      "  - BsmtFinType2_Ordinal (target corr: 0.014)\n",
      "  - BsmtExposure_Ordinal (target corr: 0.358)\n",
      "  - Utilities_Ordinal_log (target corr: 0.013)\n",
      "  - Fence_Ordinal (target corr: 0.146)\n",
      "  - CentralAir_Ordinal_log (target corr: 0.352)\n",
      "  - Neighborhood_Freq_log (target corr: 0.137)\n",
      "  - Neighborhood_MeanPrice (target corr: 0.740)\n",
      "  - Neighborhood_Ordinal (target corr: 0.202)\n",
      "  - Exterior1st_Freq (target corr: 0.287)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 4. FEATURE SELECTION & OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"4. FEATURE SELECTION & OPTIMIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# 4.1 Correlation-Based Selection\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"4.1 Correlation-Based Selection\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get all numerical features for correlation analysis\n",
    "numerical_cols = df_combined.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_cols = [col for col in numerical_cols if col != 'dataset_source']\n",
    "\n",
    "print(f\"Analyzing correlations for {len(numerical_cols)} numerical features...\")\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df_combined[numerical_cols].corr()\n",
    "\n",
    "# Find highly correlated pairs (>0.9)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = abs(correlation_matrix.iloc[i, j])\n",
    "        if corr_value > 0.9:\n",
    "            feature1 = correlation_matrix.columns[i]\n",
    "            feature2 = correlation_matrix.columns[j]\n",
    "            high_corr_pairs.append({\n",
    "                'feature1': feature1,\n",
    "                'feature2': feature2,\n",
    "                'correlation': corr_value\n",
    "            })\n",
    "\n",
    "print(f\"Found {len(high_corr_pairs)} highly correlated pairs (>0.9)\")\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(\"\\nHighly correlated feature pairs:\")\n",
    "    for pair in high_corr_pairs[:10]:  # Show first 10\n",
    "        print(f\"  {pair['feature1']} ↔ {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "\n",
    "# For feature removal, we'll prioritize keeping features with stronger target correlation\n",
    "# This will be done using training data subset\n",
    "train_subset = df_combined[df_combined['dataset_source'] == 'train'].copy()\n",
    "train_numerical = train_subset[numerical_cols]\n",
    "\n",
    "# Calculate target correlations for feature selection\n",
    "target_correlations = {}\n",
    "for feature in numerical_cols:\n",
    "    corr_with_target = abs(train_numerical[feature].corr(target_series_log))\n",
    "    target_correlations[feature] = corr_with_target\n",
    "\n",
    "# Remove redundant features\n",
    "features_to_remove = []\n",
    "for pair in high_corr_pairs:\n",
    "    feature1, feature2 = pair['feature1'], pair['feature2']\n",
    "    corr1 = target_correlations.get(feature1, 0)\n",
    "    corr2 = target_correlations.get(feature2, 0)\n",
    "    \n",
    "    # Remove the feature with lower target correlation\n",
    "    if corr1 < corr2:\n",
    "        if feature1 not in features_to_remove:\n",
    "            features_to_remove.append(feature1)\n",
    "    else:\n",
    "        if feature2 not in features_to_remove:\n",
    "            features_to_remove.append(feature2)\n",
    "\n",
    "print(f\"\\nFeatures to remove due to multicollinearity: {len(features_to_remove)}\")\n",
    "if features_to_remove:\n",
    "    print(\"Features being removed:\")\n",
    "    for feature in features_to_remove:\n",
    "        target_corr = target_correlations.get(feature, 0)\n",
    "        print(f\"  - {feature} (target corr: {target_corr:.3f})\")\n",
    "\n",
    "# Remove highly correlated features\n",
    "df_combined.drop(columns=features_to_remove, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "4.2 Importance-Based Selection\n",
      "==================================================\n",
      "Running Random Forest for feature importance analysis...\n",
      "Top 15 most important features:\n",
      "                   feature  importance\n",
      "   TotalSF_OverallQual_log    0.787624\n",
      "Neighborhood_MeanPrice_log    0.035584\n",
      "              QualityIndex    0.016303\n",
      "             GrLivArea_log    0.008064\n",
      "                  RemodAge    0.007806\n",
      "                 BsmtUnfSF    0.007066\n",
      "                  1stFlrSF    0.006276\n",
      "                GarageArea    0.006219\n",
      "                 TotalBath    0.005186\n",
      "               OverallCond    0.004739\n",
      "                  HouseAge    0.004450\n",
      "              CentralAir_Y    0.003945\n",
      "               LotFrontage    0.003395\n",
      "               LotArea_log    0.003285\n",
      "            BsmtFinSF1_log    0.003154\n",
      "\n",
      "Low importance features (bottom 5%): 24\n",
      "Removing low importance features:\n",
      "  - Exterior1st_Stone (importance: 0.000000)\n",
      "  - SaleCondition_AdjLand (importance: 0.000012)\n",
      "  - Functional_Sev (importance: 0.000000)\n",
      "  - Condition2_PosN (importance: 0.000000)\n",
      "  - RoofMatl_Membran (importance: 0.000000)\n",
      "  - Exterior1st_AsphShn (importance: 0.000000)\n",
      "  - RoofMatl_Roll (importance: 0.000000)\n",
      "  - Exterior1st_CBlock (importance: 0.000000)\n",
      "  - PoolQC_Fa (importance: 0.000000)\n",
      "  - Exterior2nd_Other (importance: 0.000000)\n",
      "  ... and 14 more\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 4.2 Importance-Based Selection\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"4.2 Importance-Based Selection\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Quick Random Forest for feature importance\n",
    "print(\"Running Random Forest for feature importance analysis...\")\n",
    "\n",
    "# Prepare data for quick model\n",
    "train_features = df_combined[df_combined['dataset_source'] == 'train'].copy()\n",
    "train_features = train_features.drop('dataset_source', axis=1)\n",
    "\n",
    "# Handle any remaining NaN values\n",
    "train_features = train_features.fillna(0)\n",
    "\n",
    "# Quick Random Forest model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf_quick = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_quick.fit(train_features, target_series_log)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': train_features.columns,\n",
    "    'importance': rf_quick.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 most important features:\")\n",
    "print(feature_importance.head(15)[['feature', 'importance']].to_string(index=False))\n",
    "\n",
    "# Remove very low importance features (bottom 5%)\n",
    "importance_threshold = feature_importance['importance'].quantile(0.05)\n",
    "low_importance_features = feature_importance[feature_importance['importance'] < importance_threshold]['feature'].tolist()\n",
    "\n",
    "print(f\"\\nLow importance features (bottom 5%): {len(low_importance_features)}\")\n",
    "if low_importance_features:\n",
    "    print(\"Removing low importance features:\")\n",
    "    for feature in low_importance_features[:10]:  # Show first 10\n",
    "        importance = feature_importance[feature_importance['feature'] == feature]['importance'].iloc[0]\n",
    "        print(f\"  - {feature} (importance: {importance:.6f})\")\n",
    "    if len(low_importance_features) > 10:\n",
    "        print(f\"  ... and {len(low_importance_features) - 10} more\")\n",
    "\n",
    "# Remove low importance features\n",
    "df_combined.drop(columns=low_importance_features, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "4.3 Neighborhood Encoding Performance Test\n",
      "==================================================\n",
      "Testing different Neighborhood encoding strategies...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Neighborhood_MeanPrice'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Test 1: Only Target Encoding (Mean Price)\u001b[39;00m\n\u001b[32m     16\u001b[39m neighborhood_target_features = [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m test_train.columns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mNeighborhood_Freq\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mNeighborhood_Ordinal\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mNeighborhoodQual\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mNeighborhoodArea\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m col.startswith(\u001b[33m'\u001b[39m\u001b[33mNeighborhood_\u001b[39m\u001b[33m'\u001b[39m)]\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m test_target = \u001b[43mtest_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mneighborhood_target_features\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mNeighborhood_MeanPrice\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m.copy()\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Test 2: Only One-Hot Encoding  \u001b[39;00m\n\u001b[32m     20\u001b[39m neighborhood_onehot_features = [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m test_train.columns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mNeighborhood_Freq\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mNeighborhood_MeanPrice\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mNeighborhood_Ordinal\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mNeighborhoodQual\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mNeighborhoodArea\u001b[39m\u001b[33m'\u001b[39m]]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/HousePrediction/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4112\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4115\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/HousePrediction/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/HousePrediction/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['Neighborhood_MeanPrice'] not in index\""
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 4.3 Neighborhood Encoding Performance Test\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"4.3 Neighborhood Encoding Performance Test\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"Testing different Neighborhood encoding strategies...\")\n",
    "\n",
    "# Prepare test datasets with different neighborhood encodings\n",
    "test_train = df_combined[df_combined['dataset_source'] == 'train'].copy()\n",
    "test_train = test_train.drop('dataset_source', axis=1).fillna(0)\n",
    "\n",
    "# Test 1: Only Target Encoding (Mean Price)\n",
    "neighborhood_target_features = [col for col in test_train.columns if col not in ['Neighborhood_Freq', 'Neighborhood_Ordinal', 'NeighborhoodQual', 'NeighborhoodArea'] and not col.startswith('Neighborhood_')]\n",
    "test_target = test_train[neighborhood_target_features + ['Neighborhood_MeanPrice']].copy()\n",
    "\n",
    "# Test 2: Only One-Hot Encoding  \n",
    "neighborhood_onehot_features = [col for col in test_train.columns if col not in ['Neighborhood_Freq', 'Neighborhood_MeanPrice', 'Neighborhood_Ordinal', 'NeighborhoodQual', 'NeighborhoodArea']]\n",
    "test_onehot = test_train[neighborhood_onehot_features].copy()\n",
    "\n",
    "# Test 3: Only Ordinal + Interactions\n",
    "neighborhood_ordinal_features = [col for col in test_train.columns if col not in ['Neighborhood_Freq', 'Neighborhood_MeanPrice'] and not col.startswith('Neighborhood_')]\n",
    "test_ordinal = test_train[neighborhood_ordinal_features + ['Neighborhood_Ordinal', 'NeighborhoodQual', 'NeighborhoodArea']].copy()\n",
    "\n",
    "# Test 4: All encodings combined\n",
    "test_all = test_train.copy()\n",
    "\n",
    "# Cross-validation comparison\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "encoding_results = {}\n",
    "test_datasets = {\n",
    "    'Target_Only': test_target,\n",
    "    'OneHot_Only': test_onehot, \n",
    "    'Ordinal_Only': test_ordinal,\n",
    "    'All_Combined': test_all\n",
    "}\n",
    "\n",
    "print(\"\\nRunning cross-validation for each encoding strategy...\")\n",
    "\n",
    "for name, dataset in test_datasets.items():\n",
    "    rf_test = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    cv_scores = cross_val_score(rf_test, dataset, target_series_log, \n",
    "                              cv=kf, scoring='neg_mean_squared_error')\n",
    "    rmse_scores = np.sqrt(-cv_scores)\n",
    "    \n",
    "    encoding_results[name] = {\n",
    "        'mean_rmse': rmse_scores.mean(),\n",
    "        'std_rmse': rmse_scores.std(),\n",
    "        'feature_count': dataset.shape[1]\n",
    "    }\n",
    "    \n",
    "    print(f\"  {name}: RMSE = {rmse_scores.mean():.4f} (±{rmse_scores.std():.4f}) | Features: {dataset.shape[1]}\")\n",
    "\n",
    "# Determine best encoding strategy\n",
    "best_strategy = min(encoding_results.keys(), key=lambda x: encoding_results[x]['mean_rmse'])\n",
    "print(f\"\\n✓ Best Neighborhood encoding strategy: {best_strategy}\")\n",
    "print(f\"  RMSE: {encoding_results[best_strategy]['mean_rmse']:.4f}\")\n",
    "print(f\"  Features: {encoding_results[best_strategy]['feature_count']}\")\n",
    "\n",
    "# Remove suboptimal neighborhood features based on test results\n",
    "features_to_keep = set()\n",
    "\n",
    "if best_strategy == 'Target_Only':\n",
    "    features_to_keep.add('Neighborhood_MeanPrice')\n",
    "    features_to_remove_encoding = [col for col in df_combined.columns \n",
    "                                 if col.startswith('Neighborhood_') and col != 'Neighborhood_MeanPrice']\n",
    "    features_to_remove_encoding.extend(['Neighborhood_Freq', 'Neighborhood_Ordinal', 'NeighborhoodQual', 'NeighborhoodArea'])\n",
    "    \n",
    "elif best_strategy == 'OneHot_Only':\n",
    "    features_to_keep.update([col for col in df_combined.columns if col.startswith('Neighborhood_')])\n",
    "    features_to_remove_encoding = ['Neighborhood_Freq', 'Neighborhood_MeanPrice', 'Neighborhood_Ordinal', 'NeighborhoodQual', 'NeighborhoodArea']\n",
    "    \n",
    "elif best_strategy == 'Ordinal_Only':\n",
    "    features_to_keep.update(['Neighborhood_Ordinal', 'NeighborhoodQual', 'NeighborhoodArea'])\n",
    "    features_to_remove_encoding = [col for col in df_combined.columns \n",
    "                                 if col.startswith('Neighborhood_') or col == 'Neighborhood_MeanPrice' or col == 'Neighborhood_Freq']\n",
    "    \n",
    "else:  # All_Combined\n",
    "    features_to_remove_encoding = []  # Keep all neighborhood features\n",
    "\n",
    "# Remove suboptimal encoding features\n",
    "if features_to_remove_encoding:\n",
    "    # Only remove features that actually exist in the dataframe\n",
    "    features_to_remove_encoding = [f for f in features_to_remove_encoding if f in df_combined.columns]\n",
    "    if features_to_remove_encoding:\n",
    "        df_combined.drop(columns=features_to_remove_encoding, inplace=True)\n",
    "        print(f\"\\nRemoved {len(features_to_remove_encoding)} suboptimal neighborhood encoding features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4.4 Feature Set Validation\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"4.4 Feature Set Validation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Final feature count\n",
    "final_feature_count = df_combined.shape[1] - 1  # -1 for dataset_source\n",
    "print(f\"Final feature count: {final_feature_count}\")\n",
    "print(f\"Features removed: {original_feature_count - final_feature_count}\")\n",
    "print(f\"Features added: {final_feature_count - original_feature_count}\")\n",
    "\n",
    "# Quick validation with cross-validation\n",
    "print(\"\\nValidating final feature set with cross-validation...\")\n",
    "\n",
    "# Prepare final training data\n",
    "final_train = df_combined[df_combined['dataset_source'] == 'train'].copy()\n",
    "final_train = final_train.drop('dataset_source', axis=1).fillna(0)\n",
    "\n",
    "# Quick cross-validation score\n",
    "cv_scores = cross_val_score(\n",
    "    RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    final_train, target_series_log,\n",
    "    cv=5, scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "rmse_scores = np.sqrt(-cv_scores)\n",
    "print(f\"5-fold CV RMSE: {rmse_scores.mean():.4f} (±{rmse_scores.std():.4f})\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4.5 Final Feature Set Export\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"4.5 Final Feature Set Export\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Split back into train and test\n",
    "final_train_df = df_combined[df_combined['dataset_source'] == 'train'].copy()\n",
    "final_test_df = df_combined[df_combined['dataset_source'] == 'test'].copy()\n",
    "\n",
    "# Remove dataset_source column\n",
    "final_train_df = final_train_df.drop('dataset_source', axis=1)\n",
    "final_test_df = final_test_df.drop('dataset_source', axis=1)\n",
    "\n",
    "# Handle any final NaN values\n",
    "final_train_df = final_train_df.fillna(0)\n",
    "final_test_df = final_test_df.fillna(0)\n",
    "\n",
    "print(f\"Final training data shape: {final_train_df.shape}\")\n",
    "print(f\"Final test data shape: {final_test_df.shape}\")\n",
    "\n",
    "# Export datasets\n",
    "print(\"\\nExporting engineered datasets...\")\n",
    "\n",
    "# Create processed directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Save datasets\n",
    "final_train_df.to_csv('../data/processed/train_engineered.csv', index=False)\n",
    "final_test_df.to_csv('../data/processed/test_engineered.csv', index=False)\n",
    "target_series_log.to_csv('../data/processed/target_log.csv', index=False, header=['SalePrice_log'])\n",
    "\n",
    "# Save feature information\n",
    "feature_info = {\n",
    "    'total_features': final_feature_count,\n",
    "    'original_features': original_feature_count,\n",
    "    'features_added': final_feature_count - original_feature_count,\n",
    "    'new_features_created': new_features,\n",
    "    'ordinal_encoded_features': ordinal_encoded_count,\n",
    "    'features_removed_multicollinearity': features_to_remove,\n",
    "    'features_removed_low_importance': low_importance_features,\n",
    "    'transformed_features': transformed_features\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../data/processed/feature_engineering_log.json', 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2, default=str)\n",
    "\n",
    "print(\"✓ Engineered datasets exported successfully\")\n",
    "print(\"✓ Feature engineering log saved\")\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE ENGINEERING SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "Original Features: {original_feature_count}\n",
    "Final Features: {final_feature_count}\n",
    "Net Change: {final_feature_count - original_feature_count:+d}\n",
    "\n",
    "Feature Engineering Steps Completed:\n",
    "✓ Domain-based feature creation ({len(new_features)} new features)\n",
    "✓ Quality & condition aggregations\n",
    "✓ Ordinal encoding ({ordinal_encoded_count} features)\n",
    "✓ Nominal encoding (one-hot + frequency)\n",
    "✓ Target variable log transformation (skewness: {target_series.skew():.3f} → {target_series_log.skew():.3f})\n",
    "✓ Numerical feature transformations ({len(transformed_features)} features)\n",
    "✓ Multicollinearity removal ({len(features_to_remove)} features removed)\n",
    "✓ Low importance feature removal ({len(low_importance_features)} features removed)\n",
    "\n",
    "Key New Features Created:\n",
    "- TotalSF (total square footage)\n",
    "- TotalBath (total bathroom count)\n",
    "- HouseAge, RemodAge, GarageAge\n",
    "- LivLotRatio, BsmtRatio, GarageRatio\n",
    "- QualityIndex, ExteriorIndex, BasementIndex, GarageIndex\n",
    "- Log-transformed versions of skewed features\n",
    "\n",
    "Data Quality:\n",
    "- Zero missing values maintained\n",
    "- All categorical features properly encoded\n",
    "- Feature correlations optimized\n",
    "- Cross-validation RMSE: {rmse_scores.mean():.4f} (±{rmse_scores.std():.4f})\n",
    "\n",
    "Files Exported:\n",
    "- ../data/processed/train_engineered.csv ({final_train_df.shape[0]} samples, {final_train_df.shape[1]} features)\n",
    "- ../data/processed/test_engineered.csv ({final_test_df.shape[0]} samples, {final_test_df.shape[1]} features)\n",
    "- ../data/processed/target_log.csv (log-transformed target variable)\n",
    "- ../data/processed/feature_engineering_log.json (detailed feature information)\n",
    "\n",
    "Ready for Notebook 04: Model Development & Optimization\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"NOTEBOOK 03 COMPLETE - FEATURE ENGINEERING SUCCESSFUL\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Feature Type Analysis\n",
    "Analyze current feature distribution and identify categories for targeted engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Type Distribution:\n",
      "Categorical features: 43\n",
      "Numerical features: 37\n",
      "Number of floats:\n",
      "1\n",
      "Number of integers:\n",
      "36\n",
      "Total features: 80\n",
      "\n",
      "Categorical features: ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1']...\n",
      "Numerical features: ['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1']...\n"
     ]
    }
   ],
   "source": [
    "# Analyze feature types for engineering strategy\n",
    "categorical_features = df_combined.select_dtypes(include=['object']).columns.tolist()\n",
    "if 'dataset_source' in categorical_features:\n",
    "    categorical_features.remove('dataset_source')\n",
    "\n",
    "numerical_features = df_combined.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    " \n",
    "\n",
    "print(\"Feature Type Distribution:\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "print(f\"Numerical features: {len(numerical_features)}\")\n",
    "print(\"Number of floats:\")\n",
    "print(len(df_combined.select_dtypes(include=['float64']).columns))\n",
    "print(\"Number of integers:\")\n",
    "print(len(df_combined.select_dtypes(include=['int64']).columns))   \n",
    "print(f\"Total features: {len(categorical_features) + len(numerical_features)}\")\n",
    "\n",
    "print(f\"\\nCategorical features: {categorical_features[:10]}...\")\n",
    "print(f\"Numerical features: {numerical_features[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying target encoding to Neighborhood...\n",
      "Target encoding completed!\n",
      "Original Neighborhood unique values: 25\n",
      "Target encoded Neighborhood stats:\n",
      "count      2917.000000\n",
      "mean     179910.206241\n",
      "std       57791.414983\n",
      "min       98623.763969\n",
      "25%      137170.359329\n",
      "50%      156763.378381\n",
      "75%      199543.910788\n",
      "max      346917.216221\n",
      "Name: Neighborhood_TargetEncoded, dtype: float64\n",
      "Correlation between original and encoded: 0.278\n",
      "\n",
      "Examples of target encoding:\n",
      "Neighborhood\n",
      "NoRidge    325912.175681\n",
      "NridgHt    315204.627727\n",
      "StoneBr    307157.735612\n",
      "Timber     244250.848854\n",
      "Somerst    226183.542927\n",
      "Veenker    224155.486511\n",
      "Crawfor    217713.812071\n",
      "ClearCr    211902.822568\n",
      "Blmngtn    198721.637076\n",
      "CollgCr    197066.249326\n",
      "Name: Neighborhood_TargetEncoded, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def target_encode_with_cv(df_train, df_test, target, categorical_col, n_folds=5, smoothing=1.0):\n",
    "    \"\"\"\n",
    "    Perform target encoding using cross-validation to prevent overfitting\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_train : pd.DataFrame\n",
    "        Training dataset\n",
    "    df_test : pd.DataFrame  \n",
    "        Test dataset\n",
    "    target : pd.Series\n",
    "        Target variable\n",
    "    categorical_col : str\n",
    "        Name of categorical column to encode\n",
    "    n_folds : int\n",
    "        Number of CV folds\n",
    "    smoothing : float\n",
    "        Smoothing parameter for regularization\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    train_encoded : pd.Series\n",
    "        Target encoded values for training set\n",
    "    test_encoded : pd.Series\n",
    "        Target encoded values for test set\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize arrays to store encoded values\n",
    "    train_encoded = np.zeros(len(df_train))\n",
    "    test_encoded = np.zeros(len(df_test))\n",
    "    \n",
    "    # Calculate global mean for smoothing\n",
    "    global_mean = target.mean()\n",
    "    \n",
    "    # Cross-validation for training set\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(df_train)):\n",
    "        # Split data for this fold\n",
    "        X_train_fold = df_train.iloc[train_idx]\n",
    "        y_train_fold = target.iloc[train_idx]\n",
    "        X_val_fold = df_train.iloc[val_idx]\n",
    "        \n",
    "        # Calculate mean target value for each category in training fold\n",
    "        category_means = y_train_fold.groupby(X_train_fold[categorical_col]).mean()\n",
    "        category_counts = y_train_fold.groupby(X_train_fold[categorical_col]).count()\n",
    "        \n",
    "        # Apply smoothing formula: (count * category_mean + smoothing * global_mean) / (count + smoothing)\n",
    "        smoothed_means = (category_counts * category_means + smoothing * global_mean) / (category_counts + smoothing)\n",
    "        \n",
    "        # Encode validation fold\n",
    "        train_encoded[val_idx] = X_val_fold[categorical_col].map(smoothed_means).fillna(global_mean)\n",
    "    \n",
    "    # For test set, use the entire training set to calculate encodings\n",
    "    category_means_full = target.groupby(df_train[categorical_col]).mean()\n",
    "    category_counts_full = target.groupby(df_train[categorical_col]).count()\n",
    "    \n",
    "    # Apply smoothing for test set\n",
    "    smoothed_means_full = (category_counts_full * category_means_full + smoothing * global_mean) / (category_counts_full + smoothing)\n",
    "    \n",
    "    # Encode test set\n",
    "    test_encoded = df_test[categorical_col].map(smoothed_means_full).fillna(global_mean)\n",
    "    \n",
    "    return pd.Series(train_encoded, index=df_train.index), pd.Series(test_encoded, index=df_test.index)\n",
    "\n",
    "# Anvend target encoding på Neighborhood\n",
    "print(\"Applying target encoding to Neighborhood...\")\n",
    "\n",
    "# Split combined dataset back to train and test\n",
    "train_mask = df_combined['dataset_source'] == 'train'\n",
    "current_train = df_combined[train_mask].copy()\n",
    "current_test = df_combined[~train_mask].copy()\n",
    "\n",
    "# Apply target encoding\n",
    "neighborhood_train_encoded, neighborhood_test_encoded = target_encode_with_cv(\n",
    "    current_train, \n",
    "    current_test, \n",
    "    target_series, \n",
    "    'Neighborhood',\n",
    "    n_folds=5,\n",
    "    smoothing=1.0\n",
    ")\n",
    "\n",
    "# Add encoded features to the datasets\n",
    "current_train['Neighborhood_TargetEncoded'] = neighborhood_train_encoded\n",
    "current_test['Neighborhood_TargetEncoded'] = neighborhood_test_encoded\n",
    "\n",
    "# Combine datasets again\n",
    "df_combined = pd.concat([current_train, current_test], ignore_index=True)\n",
    "\n",
    "print(f\"Target encoding completed!\")\n",
    "print(f\"Original Neighborhood unique values: {df_combined['Neighborhood'].nunique()}\")\n",
    "print(f\"Target encoded Neighborhood stats:\")\n",
    "print(df_combined['Neighborhood_TargetEncoded'].describe())\n",
    "\n",
    "# Optional: Check correlation between original and encoded\n",
    "train_subset = df_combined[df_combined['dataset_source'] == 'train']\n",
    "if len(train_subset) > 0:\n",
    "    # Create a simple mapping to check correlation\n",
    "    le = LabelEncoder()\n",
    "    neighborhood_numeric = le.fit_transform(train_subset['Neighborhood'])\n",
    "    correlation = np.corrcoef(neighborhood_numeric, train_subset['Neighborhood_TargetEncoded'])[0,1]\n",
    "    print(f\"Correlation between original and encoded: {correlation:.3f}\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nExamples of target encoding:\")\n",
    "example_neighborhoods = df_combined.groupby('Neighborhood')['Neighborhood_TargetEncoded'].first().sort_values(ascending=False).head(10)\n",
    "print(example_neighborhoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Type Distribution:\n",
      "Categorical features: 43\n",
      "Numerical features: 37\n",
      "Number of floats:\n",
      "2\n",
      "Number of integers:\n",
      "36\n",
      "Total features: 80\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature Type Distribution:\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "print(f\"Numerical features: {len(numerical_features)}\")\n",
    "print(\"Number of floats:\")\n",
    "print(len(df_combined.select_dtypes(include=['float64']).columns))\n",
    "print(\"Number of integers:\")\n",
    "print(len(df_combined.select_dtypes(include=['int64']).columns))   \n",
    "print(f\"Total features: {len(categorical_features) + len(numerical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2917, 82)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Type Distribution:\n",
      "Categorical features: 43\n",
      "Numerical features: 37\n",
      "Number of floats:\n",
      "2\n",
      "Number of integers:\n",
      "36\n",
      "Total features: 80\n",
      "\n",
      "Categorical features: ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1']...\n",
      "Numerical features: ['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1']...\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature Type Distribution:\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "print(f\"Numerical features: {len(numerical_features)}\")\n",
    "print(\"Number of floats:\")\n",
    "print(len(df_combined.select_dtypes(include=['float64']).columns))\n",
    "print(\"Number of integers:\")\n",
    "print(len(df_combined.select_dtypes(include=['int64']).columns))   \n",
    "print(f\"Total features: {len(categorical_features) + len(numerical_features)}\")\n",
    "\n",
    "print(f\"\\nCategorical features: {categorical_features[:10]}...\")\n",
    "print(f\"Numerical features: {numerical_features[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC', 'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition', 'dataset_source', 'MiscVal_log1p', 'PoolArea_log1p', 'LotArea_log1p', 'LowQualFinSF_log1p', '3SsnPorch_log1p', 'KitchenAbvGr_log1p', 'BsmtFinSF2_log1p', 'EnclosedPorch_log1p', 'ScreenPorch_log1p', 'BsmtHalfBath_log1p', 'GarageYrBlt_log1p', 'MasVnrArea_log1p', 'OpenPorchSF_log1p', 'WoodDeckSF_log1p', 'MSSubClass_log1p', '1stFlrSF_log1p', 'LotFrontage_log1p', 'GrLivArea_log1p', 'BsmtFinSF1_log1p', 'BsmtUnfSF_log1p', '2ndFlrSF_log1p', 'TotRmsAbvGrd_log1p', 'Fireplaces_log1p', 'HalfBath_log1p', 'TotalBsmtSF_log1p', 'BsmtFullBath_log1p', 'YearBuilt_log1p', 'OverallCond_log1p', 'Neighborhood_TargetEncoded']\n"
     ]
    }
   ],
   "source": [
    "print(df_combined.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Systematic Variable Transformations\n",
    "\n",
    "### 2.1 Skewness Analysis and Log Transformations\n",
    "Analyze skewness of numerical features and apply log transformations where necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total skewed features (|skewness| ≥ 0.5): 28\n",
      "All numerical features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_a2360\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_a2360_level0_col0\" class=\"col_heading level0 col0\" >Feature</th>\n",
       "      <th id=\"T_a2360_level0_col1\" class=\"col_heading level0 col1\" >Skewness</th>\n",
       "      <th id=\"T_a2360_level0_col2\" class=\"col_heading level0 col2\" >Contains_Zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row0_col0\" class=\"data row0 col0\" >MiscVal</td>\n",
       "      <td id=\"T_a2360_row0_col1\" class=\"data row0 col1\" >21.939672</td>\n",
       "      <td id=\"T_a2360_row0_col2\" class=\"data row0 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row1_col0\" class=\"data row1 col0\" >PoolArea</td>\n",
       "      <td id=\"T_a2360_row1_col1\" class=\"data row1 col1\" >20.563597</td>\n",
       "      <td id=\"T_a2360_row1_col2\" class=\"data row1 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row2_col0\" class=\"data row2 col0\" >LotArea</td>\n",
       "      <td id=\"T_a2360_row2_col1\" class=\"data row2 col1\" >13.109495</td>\n",
       "      <td id=\"T_a2360_row2_col2\" class=\"data row2 col2\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row3_col0\" class=\"data row3 col0\" >LowQualFinSF</td>\n",
       "      <td id=\"T_a2360_row3_col1\" class=\"data row3 col1\" >12.084539</td>\n",
       "      <td id=\"T_a2360_row3_col2\" class=\"data row3 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row4_col0\" class=\"data row4 col0\" >3SsnPorch</td>\n",
       "      <td id=\"T_a2360_row4_col1\" class=\"data row4 col1\" >11.372080</td>\n",
       "      <td id=\"T_a2360_row4_col2\" class=\"data row4 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row5_col0\" class=\"data row5 col0\" >KitchenAbvGr</td>\n",
       "      <td id=\"T_a2360_row5_col1\" class=\"data row5 col1\" >4.571363</td>\n",
       "      <td id=\"T_a2360_row5_col2\" class=\"data row5 col2\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row6_col0\" class=\"data row6 col0\" >BsmtFinSF2</td>\n",
       "      <td id=\"T_a2360_row6_col1\" class=\"data row6 col1\" >4.144503</td>\n",
       "      <td id=\"T_a2360_row6_col2\" class=\"data row6 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row7_col0\" class=\"data row7 col0\" >EnclosedPorch</td>\n",
       "      <td id=\"T_a2360_row7_col1\" class=\"data row7 col1\" >4.002344</td>\n",
       "      <td id=\"T_a2360_row7_col2\" class=\"data row7 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row8_col0\" class=\"data row8 col0\" >ScreenPorch</td>\n",
       "      <td id=\"T_a2360_row8_col1\" class=\"data row8 col1\" >3.945101</td>\n",
       "      <td id=\"T_a2360_row8_col2\" class=\"data row8 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row9_col0\" class=\"data row9 col0\" >BsmtHalfBath</td>\n",
       "      <td id=\"T_a2360_row9_col1\" class=\"data row9 col1\" >3.929996</td>\n",
       "      <td id=\"T_a2360_row9_col2\" class=\"data row9 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row10_col0\" class=\"data row10 col0\" >GarageYrBlt</td>\n",
       "      <td id=\"T_a2360_row10_col1\" class=\"data row10 col1\" >-3.919740</td>\n",
       "      <td id=\"T_a2360_row10_col2\" class=\"data row10 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row11_col0\" class=\"data row11 col0\" >MasVnrArea</td>\n",
       "      <td id=\"T_a2360_row11_col1\" class=\"data row11 col1\" >2.621719</td>\n",
       "      <td id=\"T_a2360_row11_col2\" class=\"data row11 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row12_col0\" class=\"data row12 col0\" >OpenPorchSF</td>\n",
       "      <td id=\"T_a2360_row12_col1\" class=\"data row12 col1\" >2.529358</td>\n",
       "      <td id=\"T_a2360_row12_col2\" class=\"data row12 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row13_col0\" class=\"data row13 col0\" >WoodDeckSF</td>\n",
       "      <td id=\"T_a2360_row13_col1\" class=\"data row13 col1\" >1.844792</td>\n",
       "      <td id=\"T_a2360_row13_col2\" class=\"data row13 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row14_col0\" class=\"data row14 col0\" >MSSubClass</td>\n",
       "      <td id=\"T_a2360_row14_col1\" class=\"data row14 col1\" >1.375131</td>\n",
       "      <td id=\"T_a2360_row14_col2\" class=\"data row14 col2\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row15_col0\" class=\"data row15 col0\" >1stFlrSF</td>\n",
       "      <td id=\"T_a2360_row15_col1\" class=\"data row15 col1\" >1.257286</td>\n",
       "      <td id=\"T_a2360_row15_col2\" class=\"data row15 col2\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row16_col0\" class=\"data row16 col0\" >LotFrontage</td>\n",
       "      <td id=\"T_a2360_row16_col1\" class=\"data row16 col1\" >1.102704</td>\n",
       "      <td id=\"T_a2360_row16_col2\" class=\"data row16 col2\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row17_col0\" class=\"data row17 col0\" >GrLivArea</td>\n",
       "      <td id=\"T_a2360_row17_col1\" class=\"data row17 col1\" >1.068750</td>\n",
       "      <td id=\"T_a2360_row17_col2\" class=\"data row17 col2\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row18_col0\" class=\"data row18 col0\" >BsmtFinSF1</td>\n",
       "      <td id=\"T_a2360_row18_col1\" class=\"data row18 col1\" >0.980645</td>\n",
       "      <td id=\"T_a2360_row18_col2\" class=\"data row18 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row19_col0\" class=\"data row19 col0\" >BsmtUnfSF</td>\n",
       "      <td id=\"T_a2360_row19_col1\" class=\"data row19 col1\" >0.919688</td>\n",
       "      <td id=\"T_a2360_row19_col2\" class=\"data row19 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row20_col0\" class=\"data row20 col0\" >2ndFlrSF</td>\n",
       "      <td id=\"T_a2360_row20_col1\" class=\"data row20 col1\" >0.861556</td>\n",
       "      <td id=\"T_a2360_row20_col2\" class=\"data row20 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row21_col0\" class=\"data row21 col0\" >TotRmsAbvGrd</td>\n",
       "      <td id=\"T_a2360_row21_col1\" class=\"data row21 col1\" >0.749232</td>\n",
       "      <td id=\"T_a2360_row21_col2\" class=\"data row21 col2\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row22_col0\" class=\"data row22 col0\" >Fireplaces</td>\n",
       "      <td id=\"T_a2360_row22_col1\" class=\"data row22 col1\" >0.725278</td>\n",
       "      <td id=\"T_a2360_row22_col2\" class=\"data row22 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row23_col0\" class=\"data row23 col0\" >HalfBath</td>\n",
       "      <td id=\"T_a2360_row23_col1\" class=\"data row23 col1\" >0.696666</td>\n",
       "      <td id=\"T_a2360_row23_col2\" class=\"data row23 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row24_col0\" class=\"data row24 col0\" >TotalBsmtSF</td>\n",
       "      <td id=\"T_a2360_row24_col1\" class=\"data row24 col1\" >0.671751</td>\n",
       "      <td id=\"T_a2360_row24_col2\" class=\"data row24 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row25_col0\" class=\"data row25 col0\" >BsmtFullBath</td>\n",
       "      <td id=\"T_a2360_row25_col1\" class=\"data row25 col1\" >0.622415</td>\n",
       "      <td id=\"T_a2360_row25_col2\" class=\"data row25 col2\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row26_col0\" class=\"data row26 col0\" >YearBuilt</td>\n",
       "      <td id=\"T_a2360_row26_col1\" class=\"data row26 col1\" >-0.599194</td>\n",
       "      <td id=\"T_a2360_row26_col2\" class=\"data row26 col2\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row27_col0\" class=\"data row27 col0\" >OverallCond</td>\n",
       "      <td id=\"T_a2360_row27_col1\" class=\"data row27 col1\" >0.569314</td>\n",
       "      <td id=\"T_a2360_row27_col2\" class=\"data row27 col2\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row28_col0\" class=\"data row28 col0\" >YearRemodAdd</td>\n",
       "      <td id=\"T_a2360_row28_col1\" class=\"data row28 col1\" >-0.450131</td>\n",
       "      <td id=\"T_a2360_row28_col2\" class=\"data row28 col2\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a2360_row29_col0\" class=\"data row29 col0\" >BedroomAbvGr</td>\n",
       "      <td id=\"T_a2360_row29_col1\" class=\"data row29 col1\" >0.326568</td>\n",
       "      <td id=\"T_a2360_row29_col2\" class=\"data row29 col2\" >True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1447ea010>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyze skewness for all numerical features\n",
    "skewness_data = []\n",
    "\n",
    "for feature in numerical_features:\n",
    "    if feature in df_combined.columns:\n",
    "        values = df_combined[feature].dropna()\n",
    "\n",
    "        if len(values) > 0 and values.var() > 0:  # Skip constant features\n",
    "            skew = stats.skew(values)\n",
    "            skewness_data.append({\n",
    "                'Feature': feature,\n",
    "                'Skewness': skew,\n",
    "                'Abs_Skewness': abs(skew),\n",
    "                'Contains_Zero': (values == 0).any()\n",
    "            })\n",
    "\n",
    "# Create summary DataFrame\n",
    "skewness_df = pd.DataFrame(skewness_data).sort_values('Abs_Skewness', ascending=False)\n",
    "\n",
    "# Identify highly skewed features (|skew| ≥ 0.5)\n",
    "skewed_features = skewness_df[skewness_df['Abs_Skewness'] >= 0.5]['Feature'].tolist()\n",
    "\n",
    "print(f\"Total skewed features (|skewness| ≥ 0.5): {len(skewed_features)}\")\n",
    "\n",
    "# All numerical features with skewness\n",
    "print(\"All numerical features:\")\n",
    "display(skewness_df[['Feature', 'Skewness', 'Contains_Zero']].head(30).style.hide(axis='index'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Log1p Transformation Implementation\n",
    "\n",
    "Apply log1p transformation to highly skewed features (safer than log for zero values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Features transformed: 28\n",
      "Successful improvements: 23\n",
      "Average skewness reduction: 2.389\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_5ee73\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_5ee73_level0_col0\" class=\"col_heading level0 col0\" >Feature</th>\n",
       "      <th id=\"T_5ee73_level0_col1\" class=\"col_heading level0 col1\" >Original Skew</th>\n",
       "      <th id=\"T_5ee73_level0_col2\" class=\"col_heading level0 col2\" >Transformed Skew</th>\n",
       "      <th id=\"T_5ee73_level0_col3\" class=\"col_heading level0 col3\" >Improvement</th>\n",
       "      <th id=\"T_5ee73_level0_col4\" class=\"col_heading level0 col4\" >Min Original</th>\n",
       "      <th id=\"T_5ee73_level0_col5\" class=\"col_heading level0 col5\" >Min Transformed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row0_col0\" class=\"data row0 col0\" >MiscVal</td>\n",
       "      <td id=\"T_5ee73_row0_col1\" class=\"data row0 col1\" >21.940000</td>\n",
       "      <td id=\"T_5ee73_row0_col2\" class=\"data row0 col2\" >5.212000</td>\n",
       "      <td id=\"T_5ee73_row0_col3\" class=\"data row0 col3\" >16.728000</td>\n",
       "      <td id=\"T_5ee73_row0_col4\" class=\"data row0 col4\" >0.000000</td>\n",
       "      <td id=\"T_5ee73_row0_col5\" class=\"data row0 col5\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row1_col0\" class=\"data row1 col0\" >LotArea</td>\n",
       "      <td id=\"T_5ee73_row1_col1\" class=\"data row1 col1\" >13.109000</td>\n",
       "      <td id=\"T_5ee73_row1_col2\" class=\"data row1 col2\" >-0.533000</td>\n",
       "      <td id=\"T_5ee73_row1_col3\" class=\"data row1 col3\" >12.577000</td>\n",
       "      <td id=\"T_5ee73_row1_col4\" class=\"data row1 col4\" >1300.000000</td>\n",
       "      <td id=\"T_5ee73_row1_col5\" class=\"data row1 col5\" >7.170888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row2_col0\" class=\"data row2 col0\" >LowQualFinSF</td>\n",
       "      <td id=\"T_5ee73_row2_col1\" class=\"data row2 col1\" >12.085000</td>\n",
       "      <td id=\"T_5ee73_row2_col2\" class=\"data row2 col2\" >8.555000</td>\n",
       "      <td id=\"T_5ee73_row2_col3\" class=\"data row2 col3\" >3.530000</td>\n",
       "      <td id=\"T_5ee73_row2_col4\" class=\"data row2 col4\" >0.000000</td>\n",
       "      <td id=\"T_5ee73_row2_col5\" class=\"data row2 col5\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row3_col0\" class=\"data row3 col0\" >3SsnPorch</td>\n",
       "      <td id=\"T_5ee73_row3_col1\" class=\"data row3 col1\" >11.372000</td>\n",
       "      <td id=\"T_5ee73_row3_col2\" class=\"data row3 col2\" >8.822000</td>\n",
       "      <td id=\"T_5ee73_row3_col3\" class=\"data row3 col3\" >2.550000</td>\n",
       "      <td id=\"T_5ee73_row3_col4\" class=\"data row3 col4\" >0.000000</td>\n",
       "      <td id=\"T_5ee73_row3_col5\" class=\"data row3 col5\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row4_col0\" class=\"data row4 col0\" >OpenPorchSF</td>\n",
       "      <td id=\"T_5ee73_row4_col1\" class=\"data row4 col1\" >2.529000</td>\n",
       "      <td id=\"T_5ee73_row4_col2\" class=\"data row4 col2\" >-0.042000</td>\n",
       "      <td id=\"T_5ee73_row4_col3\" class=\"data row4 col3\" >2.488000</td>\n",
       "      <td id=\"T_5ee73_row4_col4\" class=\"data row4 col4\" >0.000000</td>\n",
       "      <td id=\"T_5ee73_row4_col5\" class=\"data row4 col5\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row5_col0\" class=\"data row5 col0\" >PoolArea</td>\n",
       "      <td id=\"T_5ee73_row5_col1\" class=\"data row5 col1\" >20.564000</td>\n",
       "      <td id=\"T_5ee73_row5_col2\" class=\"data row5 col2\" >18.108000</td>\n",
       "      <td id=\"T_5ee73_row5_col3\" class=\"data row5 col3\" >2.455000</td>\n",
       "      <td id=\"T_5ee73_row5_col4\" class=\"data row5 col4\" >0.000000</td>\n",
       "      <td id=\"T_5ee73_row5_col5\" class=\"data row5 col5\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row6_col0\" class=\"data row6 col0\" >MasVnrArea</td>\n",
       "      <td id=\"T_5ee73_row6_col1\" class=\"data row6 col1\" >2.622000</td>\n",
       "      <td id=\"T_5ee73_row6_col2\" class=\"data row6 col2\" >0.538000</td>\n",
       "      <td id=\"T_5ee73_row6_col3\" class=\"data row6 col3\" >2.083000</td>\n",
       "      <td id=\"T_5ee73_row6_col4\" class=\"data row6 col4\" >0.000000</td>\n",
       "      <td id=\"T_5ee73_row6_col5\" class=\"data row6 col5\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row7_col0\" class=\"data row7 col0\" >EnclosedPorch</td>\n",
       "      <td id=\"T_5ee73_row7_col1\" class=\"data row7 col1\" >4.002000</td>\n",
       "      <td id=\"T_5ee73_row7_col2\" class=\"data row7 col2\" >1.960000</td>\n",
       "      <td id=\"T_5ee73_row7_col3\" class=\"data row7 col3\" >2.042000</td>\n",
       "      <td id=\"T_5ee73_row7_col4\" class=\"data row7 col4\" >0.000000</td>\n",
       "      <td id=\"T_5ee73_row7_col5\" class=\"data row7 col5\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row8_col0\" class=\"data row8 col0\" >WoodDeckSF</td>\n",
       "      <td id=\"T_5ee73_row8_col1\" class=\"data row8 col1\" >1.845000</td>\n",
       "      <td id=\"T_5ee73_row8_col2\" class=\"data row8 col2\" >0.160000</td>\n",
       "      <td id=\"T_5ee73_row8_col3\" class=\"data row8 col3\" >1.685000</td>\n",
       "      <td id=\"T_5ee73_row8_col4\" class=\"data row8 col4\" >0.000000</td>\n",
       "      <td id=\"T_5ee73_row8_col5\" class=\"data row8 col5\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row9_col0\" class=\"data row9 col0\" >BsmtFinSF2</td>\n",
       "      <td id=\"T_5ee73_row9_col1\" class=\"data row9 col1\" >4.145000</td>\n",
       "      <td id=\"T_5ee73_row9_col2\" class=\"data row9 col2\" >2.461000</td>\n",
       "      <td id=\"T_5ee73_row9_col3\" class=\"data row9 col3\" >1.683000</td>\n",
       "      <td id=\"T_5ee73_row9_col4\" class=\"data row9 col4\" >0.000000</td>\n",
       "      <td id=\"T_5ee73_row9_col5\" class=\"data row9 col5\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row10_col0\" class=\"data row10 col0\" >1stFlrSF</td>\n",
       "      <td id=\"T_5ee73_row10_col1\" class=\"data row10 col1\" >1.257000</td>\n",
       "      <td id=\"T_5ee73_row10_col2\" class=\"data row10 col2\" >0.030000</td>\n",
       "      <td id=\"T_5ee73_row10_col3\" class=\"data row10 col3\" >1.227000</td>\n",
       "      <td id=\"T_5ee73_row10_col4\" class=\"data row10 col4\" >334.000000</td>\n",
       "      <td id=\"T_5ee73_row10_col5\" class=\"data row10 col5\" >5.814131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row11_col0\" class=\"data row11 col0\" >MSSubClass</td>\n",
       "      <td id=\"T_5ee73_row11_col1\" class=\"data row11 col1\" >1.375000</td>\n",
       "      <td id=\"T_5ee73_row11_col2\" class=\"data row11 col2\" >0.246000</td>\n",
       "      <td id=\"T_5ee73_row11_col3\" class=\"data row11 col3\" >1.129000</td>\n",
       "      <td id=\"T_5ee73_row11_col4\" class=\"data row11 col4\" >20.000000</td>\n",
       "      <td id=\"T_5ee73_row11_col5\" class=\"data row11 col5\" >3.044522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row12_col0\" class=\"data row12 col0\" >GrLivArea</td>\n",
       "      <td id=\"T_5ee73_row12_col1\" class=\"data row12 col1\" >1.069000</td>\n",
       "      <td id=\"T_5ee73_row12_col2\" class=\"data row12 col2\" >-0.022000</td>\n",
       "      <td id=\"T_5ee73_row12_col3\" class=\"data row12 col3\" >1.047000</td>\n",
       "      <td id=\"T_5ee73_row12_col4\" class=\"data row12 col4\" >334.000000</td>\n",
       "      <td id=\"T_5ee73_row12_col5\" class=\"data row12 col5\" >5.814131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row13_col0\" class=\"data row13 col0\" >ScreenPorch</td>\n",
       "      <td id=\"T_5ee73_row13_col1\" class=\"data row13 col1\" >3.945000</td>\n",
       "      <td id=\"T_5ee73_row13_col2\" class=\"data row13 col2\" >2.945000</td>\n",
       "      <td id=\"T_5ee73_row13_col3\" class=\"data row13 col3\" >1.001000</td>\n",
       "      <td id=\"T_5ee73_row13_col4\" class=\"data row13 col4\" >0.000000</td>\n",
       "      <td id=\"T_5ee73_row13_col5\" class=\"data row13 col5\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row14_col0\" class=\"data row14 col0\" >TotRmsAbvGrd</td>\n",
       "      <td id=\"T_5ee73_row14_col1\" class=\"data row14 col1\" >0.749000</td>\n",
       "      <td id=\"T_5ee73_row14_col2\" class=\"data row14 col2\" >0.028000</td>\n",
       "      <td id=\"T_5ee73_row14_col3\" class=\"data row14 col3\" >0.721000</td>\n",
       "      <td id=\"T_5ee73_row14_col4\" class=\"data row14 col4\" >2.000000</td>\n",
       "      <td id=\"T_5ee73_row14_col5\" class=\"data row14 col5\" >1.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row15_col0\" class=\"data row15 col0\" >2ndFlrSF</td>\n",
       "      <td id=\"T_5ee73_row15_col1\" class=\"data row15 col1\" >0.862000</td>\n",
       "      <td id=\"T_5ee73_row15_col2\" class=\"data row15 col2\" >0.307000</td>\n",
       "      <td id=\"T_5ee73_row15_col3\" class=\"data row15 col3\" >0.555000</td>\n",
       "      <td id=\"T_5ee73_row15_col4\" class=\"data row15 col4\" >0.000000</td>\n",
       "      <td id=\"T_5ee73_row15_col5\" class=\"data row15 col5\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row16_col0\" class=\"data row16 col0\" >Fireplaces</td>\n",
       "      <td id=\"T_5ee73_row16_col1\" class=\"data row16 col1\" >0.725000</td>\n",
       "      <td id=\"T_5ee73_row16_col2\" class=\"data row16 col2\" >0.236000</td>\n",
       "      <td id=\"T_5ee73_row16_col3\" class=\"data row16 col3\" >0.489000</td>\n",
       "      <td id=\"T_5ee73_row16_col4\" class=\"data row16 col4\" >0.000000</td>\n",
       "      <td id=\"T_5ee73_row16_col5\" class=\"data row16 col5\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row17_col0\" class=\"data row17 col0\" >BsmtFinSF1</td>\n",
       "      <td id=\"T_5ee73_row17_col1\" class=\"data row17 col1\" >0.981000</td>\n",
       "      <td id=\"T_5ee73_row17_col2\" class=\"data row17 col2\" >-0.616000</td>\n",
       "      <td id=\"T_5ee73_row17_col3\" class=\"data row17 col3\" >0.364000</td>\n",
       "      <td id=\"T_5ee73_row17_col4\" class=\"data row17 col4\" >0.000000</td>\n",
       "      <td id=\"T_5ee73_row17_col5\" class=\"data row17 col5\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row18_col0\" class=\"data row18 col0\" >BsmtFullBath</td>\n",
       "      <td id=\"T_5ee73_row18_col1\" class=\"data row18 col1\" >0.622000</td>\n",
       "      <td id=\"T_5ee73_row18_col2\" class=\"data row18 col2\" >0.425000</td>\n",
       "      <td id=\"T_5ee73_row18_col3\" class=\"data row18 col3\" >0.197000</td>\n",
       "      <td id=\"T_5ee73_row18_col4\" class=\"data row18 col4\" >0.000000</td>\n",
       "      <td id=\"T_5ee73_row18_col5\" class=\"data row18 col5\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row19_col0\" class=\"data row19 col0\" >BsmtHalfBath</td>\n",
       "      <td id=\"T_5ee73_row19_col1\" class=\"data row19 col1\" >3.930000</td>\n",
       "      <td id=\"T_5ee73_row19_col2\" class=\"data row19 col2\" >3.773000</td>\n",
       "      <td id=\"T_5ee73_row19_col3\" class=\"data row19 col3\" >0.157000</td>\n",
       "      <td id=\"T_5ee73_row19_col4\" class=\"data row19 col4\" >0.000000</td>\n",
       "      <td id=\"T_5ee73_row19_col5\" class=\"data row19 col5\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row20_col0\" class=\"data row20 col0\" >HalfBath</td>\n",
       "      <td id=\"T_5ee73_row20_col1\" class=\"data row20 col1\" >0.697000</td>\n",
       "      <td id=\"T_5ee73_row20_col2\" class=\"data row20 col2\" >0.583000</td>\n",
       "      <td id=\"T_5ee73_row20_col3\" class=\"data row20 col3\" >0.114000</td>\n",
       "      <td id=\"T_5ee73_row20_col4\" class=\"data row20 col4\" >0.000000</td>\n",
       "      <td id=\"T_5ee73_row20_col5\" class=\"data row20 col5\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row21_col0\" class=\"data row21 col0\" >KitchenAbvGr</td>\n",
       "      <td id=\"T_5ee73_row21_col1\" class=\"data row21 col1\" >4.571000</td>\n",
       "      <td id=\"T_5ee73_row21_col2\" class=\"data row21 col2\" >4.475000</td>\n",
       "      <td id=\"T_5ee73_row21_col3\" class=\"data row21 col3\" >0.096000</td>\n",
       "      <td id=\"T_5ee73_row21_col4\" class=\"data row21 col4\" >1.000000</td>\n",
       "      <td id=\"T_5ee73_row21_col5\" class=\"data row21 col5\" >0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row22_col0\" class=\"data row22 col0\" >LotFrontage</td>\n",
       "      <td id=\"T_5ee73_row22_col1\" class=\"data row22 col1\" >1.103000</td>\n",
       "      <td id=\"T_5ee73_row22_col2\" class=\"data row22 col2\" >-1.070000</td>\n",
       "      <td id=\"T_5ee73_row22_col3\" class=\"data row22 col3\" >0.033000</td>\n",
       "      <td id=\"T_5ee73_row22_col4\" class=\"data row22 col4\" >21.000000</td>\n",
       "      <td id=\"T_5ee73_row22_col5\" class=\"data row22 col5\" >3.091042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row23_col0\" class=\"data row23 col0\" >GarageYrBlt</td>\n",
       "      <td id=\"T_5ee73_row23_col1\" class=\"data row23 col1\" >-3.920000</td>\n",
       "      <td id=\"T_5ee73_row23_col2\" class=\"data row23 col2\" >-3.939000</td>\n",
       "      <td id=\"T_5ee73_row23_col3\" class=\"data row23 col3\" >-0.019000</td>\n",
       "      <td id=\"T_5ee73_row23_col4\" class=\"data row23 col4\" >0.000000</td>\n",
       "      <td id=\"T_5ee73_row23_col5\" class=\"data row23 col5\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row24_col0\" class=\"data row24 col0\" >YearBuilt</td>\n",
       "      <td id=\"T_5ee73_row24_col1\" class=\"data row24 col1\" >-0.599000</td>\n",
       "      <td id=\"T_5ee73_row24_col2\" class=\"data row24 col2\" >-0.626000</td>\n",
       "      <td id=\"T_5ee73_row24_col3\" class=\"data row24 col3\" >-0.026000</td>\n",
       "      <td id=\"T_5ee73_row24_col4\" class=\"data row24 col4\" >1872.000000</td>\n",
       "      <td id=\"T_5ee73_row24_col5\" class=\"data row24 col5\" >7.535297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row25_col0\" class=\"data row25 col0\" >OverallCond</td>\n",
       "      <td id=\"T_5ee73_row25_col1\" class=\"data row25 col1\" >0.569000</td>\n",
       "      <td id=\"T_5ee73_row25_col2\" class=\"data row25 col2\" >-0.747000</td>\n",
       "      <td id=\"T_5ee73_row25_col3\" class=\"data row25 col3\" >-0.178000</td>\n",
       "      <td id=\"T_5ee73_row25_col4\" class=\"data row25 col4\" >1.000000</td>\n",
       "      <td id=\"T_5ee73_row25_col5\" class=\"data row25 col5\" >0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row26_col0\" class=\"data row26 col0\" >BsmtUnfSF</td>\n",
       "      <td id=\"T_5ee73_row26_col1\" class=\"data row26 col1\" >0.920000</td>\n",
       "      <td id=\"T_5ee73_row26_col2\" class=\"data row26 col2\" >-2.154000</td>\n",
       "      <td id=\"T_5ee73_row26_col3\" class=\"data row26 col3\" >-1.234000</td>\n",
       "      <td id=\"T_5ee73_row26_col4\" class=\"data row26 col4\" >0.000000</td>\n",
       "      <td id=\"T_5ee73_row26_col5\" class=\"data row26 col5\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5ee73_row27_col0\" class=\"data row27 col0\" >TotalBsmtSF</td>\n",
       "      <td id=\"T_5ee73_row27_col1\" class=\"data row27 col1\" >0.672000</td>\n",
       "      <td id=\"T_5ee73_row27_col2\" class=\"data row27 col2\" >-4.964000</td>\n",
       "      <td id=\"T_5ee73_row27_col3\" class=\"data row27 col3\" >-4.292000</td>\n",
       "      <td id=\"T_5ee73_row27_col4\" class=\"data row27 col4\" >0.000000</td>\n",
       "      <td id=\"T_5ee73_row27_col5\" class=\"data row27 col5\" >0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1442bf190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformation_results = []\n",
    "\n",
    "for feature in skewed_features:\n",
    "    if feature in df_combined.columns:\n",
    "        original_values = df_combined[feature].dropna()\n",
    "        original_skew = stats.skew(original_values)\n",
    "\n",
    "        # Apply log1p transformation\n",
    "        log_feature = f\"{feature}_log1p\"\n",
    "        df_combined[log_feature] = np.log1p(df_combined[feature])\n",
    "        transformed_values = df_combined[log_feature].dropna()\n",
    "        new_skew = stats.skew(transformed_values)\n",
    "\n",
    "        improvement = abs(original_skew) - abs(new_skew)\n",
    "\n",
    "        transformation_results.append({\n",
    "            'Feature': feature,\n",
    "            'Original Skew': round(original_skew, 3),\n",
    "            'Transformed Skew': round(new_skew, 3),\n",
    "            'Improvement': round(improvement, 3),\n",
    "            'Min Original': original_values.min(),\n",
    "            'Min Transformed': transformed_values.min()\n",
    "        })\n",
    "\n",
    "# Create summary DataFrame\n",
    "transform_df = pd.DataFrame(transformation_results)\n",
    "successful_transforms = transform_df[transform_df['Improvement'] > 0]\n",
    "\n",
    "print(\"Summary:\")\n",
    "print(f\"Features transformed: {len(transform_df)}\")\n",
    "print(f\"Successful improvements: {len(successful_transforms)}\")\n",
    "print(f\"Average skewness reduction: {successful_transforms['Improvement'].mean():.3f}\")\n",
    "\n",
    "# Display top 10 transformations sorted by improvement\n",
    "display(transform_df.sort_values(by='Improvement', ascending=False).head(30).style.hide(axis='index'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data validation confirms 230+ features with zero missing values from comprehensive preprocessing pipeline.\n",
    "Combined dataset structure enables consistent feature engineering across train/test splits.\n",
    "\n",
    "### 1.2 Baseline Feature Analysis and Correlation Discovery\n",
    "\n",
    "Implement systematic correlation analysis to identify feature engineering opportunities.\n",
    "Execute feature importance ranking pipeline to establish baseline for engineering optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = df_combined.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_features = [col for col in numerical_features if col not in ['Id', 'SalePrice', 'SalePrice_log', 'dataset_source']]\n",
    "\n",
    "# All categorical features one-hot encoded in preprocessing (notebook 02)\n",
    "categorical_features = df_combined.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "if 'dataset_source' in categorical_features:\n",
    "    categorical_features.remove('dataset_source')\n",
    "\n",
    "print(f\"Numerical features available: {len(numerical_features)}\")\n",
    "print(f\"Categorical features remaining (all one-hot encoded): {len(categorical_features)}\")\n",
    "print(f\"Total features from preprocessing: {len(numerical_features) + len(categorical_features)}\")\n",
    "\n",
    "# Baseline correlation analysis with target (train data only)\n",
    "target_col = 'SalePrice_log' if 'SalePrice_log' in df_train_clean.columns else 'SalePrice'\n",
    "if target_col in df_train_clean.columns:\n",
    "    # Filter numerical features that exist in train data (excluding Id and target)\n",
    "    train_numerical_features = [col for col in numerical_features if col in df_train_clean.columns]\n",
    "    # Also exclude Id and target variables from features list\n",
    "    train_numerical_features = [col for col in train_numerical_features if col not in ['Id', 'SalePrice', 'SalePrice_log']]\n",
    "\n",
    "    baseline_correlations = df_train_clean[train_numerical_features + [target_col]].corr()[target_col].sort_values(ascending=False)\n",
    "\n",
    "    print(f\"\\nTop 10 features correlated with {target_col}:\")\n",
    "    print(baseline_correlations.head(11)[1:])  # Exclude target itself\n",
    "\n",
    "    print(\"\\nLeast correlated features (potential for engineering):\")\n",
    "    print(baseline_correlations.tail(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation analysis identifies OverallQual (0.821) and GrLivArea_log (0.737) as dominant predictors establishing clear engineering priorities.\n",
    "Negative correlations in garage types (-0.389) and foundation materials (-0.338) reveal combination opportunities for feature optimization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Correlation-Driven Feature Combination Discovery\n",
    "\n",
    "Calculate individual component correlations first, then create combinations and measure improvement over best individual components.\n",
    "Apply systematic combination testing with documented correlation improvements following Kaggle best practices for feature engineering.\n",
    "\n",
    "### 2.1 Individual Component Baseline Analysis\n",
    "\n",
    "Calculate correlation for all individual features to establish baseline performance for combination comparison.\n",
    "Identify top performers in each category (area, quality, bathroom) for targeted combination testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate individual feature correlations by category for baseline comparison\n",
    "\n",
    "# Area-related features for combination testing\n",
    "area_features = ['GrLivArea', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GarageArea',\n",
    "                'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', 'ScreenPorch',\n",
    "                'MasVnrArea', 'LotArea', 'PoolArea', 'LowQualFinSF']\n",
    "\n",
    "# Quality-related features for combination testing\n",
    "quality_features = ['OverallQual', 'OverallCond', 'ExterQual', 'ExterCond',\n",
    "                   'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual',\n",
    "                   'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']\n",
    "\n",
    "# Bathroom and room features for combination testing\n",
    "bath_room_features = ['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath',\n",
    "                     'BedroomAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars']\n",
    "\n",
    "# Calculate baseline correlations for each category\n",
    "print(\"INDIVIDUAL COMPONENT BASELINE CORRELATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Area features baseline\n",
    "print(\"\\nAREA FEATURES:\")\n",
    "area_correlations = {}\n",
    "for feature in area_features:\n",
    "    if feature in df_train_clean.columns:\n",
    "        corr = df_train_clean[feature].corr(df_train_clean[target_col])\n",
    "        area_correlations[feature] = corr\n",
    "        print(f\"{feature}: {corr:.3f}\")\n",
    "\n",
    "# Quality features baseline\n",
    "print(\"\\nQUALITY FEATURES:\")\n",
    "quality_correlations = {}\n",
    "for feature in quality_features:\n",
    "    if feature in df_train_clean.columns:\n",
    "        corr = df_train_clean[feature].corr(df_train_clean[target_col])\n",
    "        quality_correlations[feature] = corr\n",
    "        print(f\"{feature}: {corr:.3f}\")\n",
    "\n",
    "# Bathroom/room features baseline\n",
    "print(\"\\nBATHROOM & ROOM FEATURES:\")\n",
    "bath_room_correlations = {}\n",
    "for feature in bath_room_features:\n",
    "    if feature in df_train_clean.columns:\n",
    "        corr = df_train_clean[feature].corr(df_train_clean[target_col])\n",
    "        bath_room_correlations[feature] = corr\n",
    "        print(f\"{feature}: {corr:.3f}\")\n",
    "\n",
    "# Identify top performers in each category\n",
    "print(\"\\nTOP PERFORMERS BY CATEGORY:\")\n",
    "print(f\"Best Area Feature: {max(area_correlations, key=area_correlations.get)} ({max(area_correlations.values()):.3f})\")\n",
    "print(f\"Best Quality Feature: {max(quality_correlations, key=quality_correlations.get)} ({max(quality_correlations.values()):.3f})\")\n",
    "print(f\"Best Bath/Room Feature: {max(bath_room_correlations, key=bath_room_correlations.get)} ({max(bath_room_correlations.values()):.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline analysis reveals OverallQual (0.821) dominates as category leader, with clear 0.65+ correlation clustering in basement/garage features.\n",
    "Quality features maintain consistent 0.6+ performance while area features span from negative to 0.725, establishing engineering improvement thresholds.\n",
    "\n",
    "### 2.2 Area Feature Combinations vs Individual Components\n",
    "\n",
    "Test area feature additions and ratios against individual component correlations to measure improvement.\n",
    "Focus on combinations that beat best individual component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_combinations = {}\n",
    "print(\"AREA FEATURE COMBINATIONS VS INDIVIDUAL COMPONENTS\")\n",
    "print(\"=\" * 55)\n",
    "print(\"Strategy: Compare each combination vs its best individual component\")\n",
    "print()\n",
    "\n",
    "# Test addition combinations\n",
    "print(\"ADDITION COMBINATIONS:\")\n",
    "area_pairs = [\n",
    "    ('TotalBsmtSF', '1stFlrSF'),\n",
    "    ('GrLivArea', 'TotalBsmtSF'),\n",
    "    ('GarageArea', 'TotalBsmtSF'),\n",
    "    ('1stFlrSF', '2ndFlrSF'),\n",
    "    ('MasVnrArea', 'TotalBsmtSF'),\n",
    "    ('WoodDeckSF', 'OpenPorchSF'),\n",
    "    ('GrLivArea', 'GarageArea')\n",
    "]\n",
    "\n",
    "for feat1, feat2 in area_pairs:\n",
    "    if feat1 in df_train_clean.columns and feat2 in df_train_clean.columns:\n",
    "        combination = df_train_clean[feat1] + df_train_clean[feat2]\n",
    "        corr = combination.corr(df_train_clean[target_col])\n",
    "        area_combinations[f\"{feat1}_add_{feat2}\"] = corr\n",
    "\n",
    "        # Compare to individual components\n",
    "        individual_best = max(area_correlations.get(feat1, 0), area_correlations.get(feat2, 0))\n",
    "        improvement = (corr - individual_best) / individual_best * 100\n",
    "\n",
    "        print(f\"{feat1}_add_{feat2}: {corr:.3f} (vs best individual {individual_best:.3f}, {improvement:+.1f}%)\")\n",
    "\n",
    "# Test ratio combinations\n",
    "print(\"\\nRATIO COMBINATIONS:\")\n",
    "ratio_pairs = [\n",
    "    ('GrLivArea', 'LotArea'),\n",
    "    ('TotalBsmtSF', 'GrLivArea'),\n",
    "    ('GarageArea', 'GarageCars'),\n",
    "    ('1stFlrSF', 'TotalBsmtSF'),\n",
    "    ('MasVnrArea', 'GrLivArea')\n",
    "]\n",
    "\n",
    "for feat1, feat2 in ratio_pairs:\n",
    "    if feat1 in df_train_clean.columns and feat2 in df_train_clean.columns:\n",
    "        # Avoid division by zero\n",
    "        mask = df_train_clean[feat2] > 0\n",
    "        if mask.sum() > 100:  # Ensure sufficient data\n",
    "            ratio = df_train_clean.loc[mask, feat1] / df_train_clean.loc[mask, feat2]\n",
    "            corr = ratio.corr(df_train_clean.loc[mask, target_col])\n",
    "            area_combinations[f\"{feat1}_ratio_{feat2}\"] = corr\n",
    "\n",
    "            # Compare to individual components\n",
    "            individual_best = max(area_correlations.get(feat1, 0), area_correlations.get(feat2, 0))\n",
    "            improvement = (corr - individual_best) / individual_best * 100\n",
    "\n",
    "            print(f\"{feat1}_ratio_{feat2}: {corr:.3f} (vs best individual {individual_best:.3f}, {improvement:+.1f}%)\")\n",
    "\n",
    "# Identify successful combinations that beat individual baselines\n",
    "print(\"\\nSUCCESSFUL AREA COMBINATIONS (beat individual components):\")\n",
    "successful_area = {}\n",
    "\n",
    "for name, corr in area_combinations.items():\n",
    "    # Extract feature names to find individual baselines\n",
    "    if '_add_' in name or '_ratio_' in name:\n",
    "        parts = name.replace('_add_', '|').replace('_ratio_', '|').split('|')\n",
    "        if len(parts) == 2:\n",
    "            feat1, feat2 = parts\n",
    "            # Get individual correlations\n",
    "            corr1 = area_correlations.get(feat1, 0)\n",
    "            corr2 = area_correlations.get(feat2, 0)\n",
    "            individual_best = max(corr1, corr2)\n",
    "            \n",
    "            # Check if combination beats individual best\n",
    "            if corr > individual_best:\n",
    "                improvement = (corr - individual_best) / individual_best * 100\n",
    "                successful_area[name] = (corr, improvement, individual_best)\n",
    "\n",
    "for name, (corr, improvement, individual_best) in sorted(successful_area.items(), key=lambda x: x[1][1], reverse=True):\n",
    "    print(f\"{name}: {corr:.3f} ({improvement:+.1f}% vs best individual {individual_best:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition strategy demonstrates consistent success with 7 combinations beating individual components (3.2% to 30.8% improvement).\n",
    "Ratio strategy fails systematically with all combinations showing severe correlation decline (-58.8% to -116.2%), validating absolute measurement approach over efficiency metrics.\n",
    "\n",
    "### 2.3 Quality Feature Combinations vs Individual Components\n",
    "\n",
    "Create quality interaction features and quality-area multiplications with individual correlation comparison.\n",
    "Document improvement percentages over individual quality and area features for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_combinations = {}\n",
    "\n",
    "print(\"QUALITY FEATURE COMBINATIONS VS INDIVIDUAL COMPONENTS\")\n",
    "print(\"=\" * 55)\n",
    "print(\"Strategy: Compare each combination vs its best individual component\")\n",
    "print()\n",
    "\n",
    "# Test quality multiplication with area features\n",
    "print(\"QUALITY × AREA COMBINATIONS:\")\n",
    "quality_area_pairs = [\n",
    "    ('OverallQual', 'GrLivArea'),\n",
    "    ('OverallQual', 'TotalBsmtSF'),\n",
    "    ('ExterQual', 'GrLivArea'),\n",
    "    ('KitchenQual', 'GrLivArea'),\n",
    "    ('ExterQual', 'TotalBsmtSF'),\n",
    "    ('BsmtQual', 'TotalBsmtSF'),\n",
    "    ('KitchenQual', 'TotalBsmtSF')\n",
    "]\n",
    "\n",
    "for qual_feat, area_feat in quality_area_pairs:\n",
    "    if qual_feat in df_train_clean.columns and area_feat in df_train_clean.columns:\n",
    "        combination = df_train_clean[qual_feat] * df_train_clean[area_feat]\n",
    "        corr = combination.corr(df_train_clean[target_col])\n",
    "        quality_combinations[f\"{qual_feat}_multiply_{area_feat}\"] = corr\n",
    "\n",
    "        # Compare to individual components\n",
    "        qual_corr = quality_correlations.get(qual_feat, 0)\n",
    "        area_corr = area_correlations.get(area_feat, 0)\n",
    "        individual_best = max(qual_corr, area_corr)\n",
    "        improvement = (corr - individual_best) / individual_best * 100\n",
    "\n",
    "        print(f\"{qual_feat}_multiply_{area_feat}: {corr:.3f} (vs best individual {individual_best:.3f}, {improvement:+.1f}%)\")\n",
    "\n",
    "# Test quality interaction combinations\n",
    "print(\"\\nQUALITY × QUALITY COMBINATIONS:\")\n",
    "quality_pairs = [\n",
    "    ('OverallQual', 'ExterQual'),\n",
    "    ('OverallQual', 'KitchenQual'),\n",
    "    ('ExterQual', 'KitchenQual'),\n",
    "    ('OverallQual', 'BsmtQual'),\n",
    "    ('ExterQual', 'BsmtQual'),\n",
    "    ('KitchenQual', 'BsmtQual')\n",
    "]\n",
    "\n",
    "for qual1, qual2 in quality_pairs:\n",
    "    if qual1 in df_train_clean.columns and qual2 in df_train_clean.columns:\n",
    "        combination = df_train_clean[qual1] * df_train_clean[qual2]\n",
    "        corr = combination.corr(df_train_clean[target_col])\n",
    "        quality_combinations[f\"{qual1}_multiply_{qual2}\"] = corr\n",
    "\n",
    "        # Compare to individual components\n",
    "        individual_best = max(quality_correlations.get(qual1, 0), quality_correlations.get(qual2, 0))\n",
    "        improvement = (corr - individual_best) / individual_best * 100\n",
    "\n",
    "        print(f\"{qual1}_multiply_{qual2}: {corr:.3f} (vs best individual {individual_best:.3f}, {improvement:+.1f}%)\")\n",
    "\n",
    "# Test quality addition combinations\n",
    "print(\"\\nQUALITY + QUALITY COMBINATIONS:\")\n",
    "for qual1, qual2 in quality_pairs:\n",
    "    if qual1 in df_train_clean.columns and qual2 in df_train_clean.columns:\n",
    "        combination = df_train_clean[qual1] + df_train_clean[qual2]\n",
    "        corr = combination.corr(df_train_clean[target_col])\n",
    "        quality_combinations[f\"{qual1}_add_{qual2}\"] = corr\n",
    "\n",
    "        # Compare to individual components\n",
    "        individual_best = max(quality_correlations.get(qual1, 0), quality_correlations.get(qual2, 0))\n",
    "        improvement = (corr - individual_best) / individual_best * 100\n",
    "\n",
    "        print(f\"{qual1}_add_{qual2}: {corr:.3f} (vs best individual {individual_best:.3f}, {improvement:+.1f}%)\")\n",
    "\n",
    "# Identify successful quality combinations that beat individual baselines\n",
    "print(\"\\nSUCCESSFUL QUALITY COMBINATIONS (beat individual components):\")\n",
    "successful_quality = {}\n",
    "\n",
    "for name, corr in quality_combinations.items():\n",
    "    # Extract feature names to find individual baselines\n",
    "    if '_multiply_' in name or '_add_' in name:\n",
    "        parts = name.replace('_multiply_', '|').replace('_add_', '|').split('|')\n",
    "        if len(parts) == 2:\n",
    "            feat1, feat2 = parts\n",
    "            # Get individual correlations\n",
    "            corr1 = quality_correlations.get(feat1, area_correlations.get(feat1, 0))\n",
    "            corr2 = quality_correlations.get(feat2, area_correlations.get(feat2, 0))\n",
    "            individual_best = max(corr1, corr2)\n",
    "            \n",
    "            # Check if combination beats individual best\n",
    "            if corr > individual_best:\n",
    "                improvement = (corr - individual_best) / individual_best * 100\n",
    "                successful_quality[name] = (corr, improvement, individual_best)\n",
    "\n",
    "for name, (corr, improvement, individual_best) in sorted(successful_quality.items(), key=lambda x: x[1][1], reverse=True):\n",
    "    print(f\"{name}: {corr:.3f} ({improvement:+.1f}% vs best individual {individual_best:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quality feature engineering achieves 14 successful combinations beating individual components with improvements ranging 1.0% to 15.0%.\n",
    "Quality-area multiplication strategy proves most effective (BsmtQual_multiply_TotalBsmtSF: +15.0%) while OverallQual combinations show modest gains due to high individual baseline (0.821).\n",
    "\n",
    "### 2.4 Bathroom and Room Engineering vs Individual Components\n",
    "\n",
    "Test bathroom combinations and room efficiency ratios against individual bathroom/room feature correlations.\n",
    "Calculate improvement over best individual components to validate engineering decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bath_room_combinations = {}\n",
    "print(\"BATHROOM & ROOM COMBINATIONS VS INDIVIDUAL COMPONENTS\")\n",
    "print(\"=\" * 55)\n",
    "print(\"Strategy: Compare each combination vs its best individual component\")\n",
    "print()\n",
    "\n",
    "# Test bathroom calculation combinations\n",
    "print(\"BATHROOM COMBINATIONS:\")\n",
    "# Standard real estate bathroom calculation: FullBath + 0.5*HalfBath\n",
    "if 'FullBath' in df_train_clean.columns and 'HalfBath' in df_train_clean.columns:\n",
    "    total_baths_standard = df_train_clean['FullBath'] + 0.5 * df_train_clean['HalfBath']\n",
    "    corr = total_baths_standard.corr(df_train_clean[target_col])\n",
    "    bath_room_combinations['TotalBaths_Standard'] = corr\n",
    "    individual_best = max(bath_room_correlations.get('FullBath', 0), bath_room_correlations.get('HalfBath', 0))\n",
    "    improvement = (corr - individual_best) / individual_best * 100\n",
    "    print(f\"TotalBaths_Standard: {corr:.3f} (vs best individual {individual_best:.3f}, {improvement:+.1f}%)\")\n",
    "\n",
    "# Include basement bathrooms\n",
    "if all(col in df_train_clean.columns for col in ['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath']):\n",
    "    total_baths_all = (df_train_clean['FullBath'] + df_train_clean['BsmtFullBath'] +\n",
    "                      0.5 * (df_train_clean['HalfBath'] + df_train_clean['BsmtHalfBath']))\n",
    "    corr = total_baths_all.corr(df_train_clean[target_col])\n",
    "    bath_room_combinations['TotalBaths_All'] = corr\n",
    "    individual_best = max(bath_room_correlations.get('FullBath', 0), bath_room_correlations.get('BsmtFullBath', 0))\n",
    "    improvement = (corr - individual_best) / individual_best * 100\n",
    "    print(f\"TotalBaths_All: {corr:.3f} (vs best individual {individual_best:.3f}, {improvement:+.1f}%)\")\n",
    "\n",
    "# Test room efficiency ratios\n",
    "print(\"\\nROOM EFFICIENCY COMBINATIONS:\")\n",
    "room_efficiency_pairs = [\n",
    "    ('GrLivArea', 'TotRmsAbvGrd'),\n",
    "    ('GrLivArea', 'BedroomAbvGr'),\n",
    "    ('TotalBsmtSF', 'BedroomAbvGr'),\n",
    "    ('GarageCars', 'GarageArea')\n",
    "]\n",
    "\n",
    "for area_feat, room_feat in room_efficiency_pairs:\n",
    "    if area_feat in df_train_clean.columns and room_feat in df_train_clean.columns:\n",
    "        # Calculate area per room (avoid division by zero)\n",
    "        mask = df_train_clean[room_feat] > 0\n",
    "        if mask.sum() > 100:\n",
    "            efficiency = df_train_clean.loc[mask, area_feat] / df_train_clean.loc[mask, room_feat]\n",
    "            corr = efficiency.corr(df_train_clean.loc[mask, target_col])\n",
    "            bath_room_combinations[f\"{area_feat}_per_{room_feat}\"] = corr\n",
    "\n",
    "            # Compare to individual components\n",
    "            area_corr = area_correlations.get(area_feat, 0) if area_feat in area_correlations else bath_room_correlations.get(area_feat, 0)\n",
    "            room_corr = bath_room_correlations.get(room_feat, 0)\n",
    "            individual_best = max(area_corr, room_corr)\n",
    "            improvement = (corr - individual_best) / individual_best * 100\n",
    "\n",
    "            print(f\"{area_feat}_per_{room_feat}: {corr:.3f} (vs best individual {individual_best:.3f}, {improvement:+.1f}%)\")\n",
    "\n",
    "# Test room count combinations\n",
    "print(\"\\nROOM COUNT COMBINATIONS:\")\n",
    "room_count_features = ['BedroomAbvGr', 'TotRmsAbvGrd', 'Fireplaces']\n",
    "if all(col in df_train_clean.columns for col in room_count_features):\n",
    "    total_rooms = df_train_clean['BedroomAbvGr'] + df_train_clean['TotRmsAbvGrd'] + df_train_clean['Fireplaces']\n",
    "    corr = total_rooms.corr(df_train_clean[target_col])\n",
    "    bath_room_combinations['TotalRooms_All'] = corr\n",
    "    individual_best = max([bath_room_correlations.get(feat, 0) for feat in room_count_features])\n",
    "    improvement = (corr - individual_best) / individual_best * 100\n",
    "    print(f\"TotalRooms_All: {corr:.3f} (vs best individual {individual_best:.3f}, {improvement:+.1f}%)\")\n",
    "\n",
    "# Test garage efficiency\n",
    "if 'GarageArea' in df_train_clean.columns and 'GarageCars' in df_train_clean.columns:\n",
    "    mask = df_train_clean['GarageCars'] > 0\n",
    "    if mask.sum() > 100:\n",
    "        garage_efficiency = df_train_clean.loc[mask, 'GarageArea'] / df_train_clean.loc[mask, 'GarageCars']\n",
    "        corr = garage_efficiency.corr(df_train_clean.loc[mask, target_col])\n",
    "        bath_room_combinations['GarageArea_per_Car'] = corr\n",
    "        individual_best = max(area_correlations.get('GarageArea', 0), bath_room_correlations.get('GarageCars', 0))\n",
    "        improvement = (corr - individual_best) / individual_best * 100\n",
    "        print(f\"GarageArea_per_Car: {corr:.3f} (vs best individual {individual_best:.3f}, {improvement:+.1f}%)\")\n",
    "\n",
    "# Identify successful bathroom/room combinations\n",
    "print(\"\\nSUCCESSFUL BATH/ROOM COMBINATIONS (beat individual components):\")\n",
    "successful_bath_room = {}\n",
    "\n",
    "for name, corr in bath_room_combinations.items():\n",
    "    # For bathroom/room combinations, find the best individual component\n",
    "    individual_best = 0\n",
    "    if 'TotalBaths' in name:\n",
    "        individual_best = max(bath_room_correlations.get('FullBath', 0), bath_room_correlations.get('HalfBath', 0))\n",
    "    elif 'Room' in name:\n",
    "        individual_best = max(bath_room_correlations.get('TotRmsAbvGrd', 0), bath_room_correlations.get('BedroomAbvGr', 0))\n",
    "    elif 'Garage' in name:\n",
    "        individual_best = max(bath_room_correlations.get('GarageArea', 0), bath_room_correlations.get('GarageCars', 0))\n",
    "    \n",
    "    # Check if combination beats individual best\n",
    "    if corr > individual_best and individual_best > 0:\n",
    "        improvement = (corr - individual_best) / individual_best * 100\n",
    "        successful_bath_room[name] = (corr, improvement, individual_best)\n",
    "\n",
    "for name, (corr, improvement, individual_best) in sorted(successful_bath_room.items(), key=lambda x: x[1][1], reverse=True):\n",
    "    print(f\"{name}: {corr:.3f} ({improvement:+.1f}% vs best individual {individual_best:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bathroom engineering achieves strong success with TotalBaths_All (+13.6%) and TotalBaths_Standard (+7.6%) beating individual components.\n",
    "Room efficiency ratios fail systematically (-21.5% to -104.7%) confirming ratio strategy ineffectiveness across all feature categories, while absolute measurement combinations succeed.\n",
    "\n",
    "### 2.5 Final Selection Based on Improvement Analysis\n",
    "\n",
    "Compile successful combinations from all categories that beat their individual component baselines.\n",
    "Create final engineered feature set with documented improvement percentages and systematic correlation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all successful combinations from sections 2.2-2.4\n",
    "print(\"FINAL FEATURE ENGINEERING SELECTION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Collect all combinations that beat individual components\n",
    "final_engineered_features = {}\n",
    "\n",
    "# Area combinations that beat individual components\n",
    "print(\"SUCCESSFUL AREA COMBINATIONS:\")\n",
    "if 'successful_area' in locals():\n",
    "    for name, (corr, improvement, individual_best) in sorted(successful_area.items(), key=lambda x: x[1][1], reverse=True):\n",
    "        final_engineered_features[name] = corr\n",
    "        print(f\"  {name}: {corr:.3f} ({improvement:+.1f}% vs individual {individual_best:.3f})\")\n",
    "\n",
    "# Quality combinations that beat individual components\n",
    "print(\"\\nSUCCESSFUL QUALITY COMBINATIONS:\")\n",
    "if 'successful_quality' in locals():\n",
    "    for name, (corr, improvement, individual_best) in sorted(successful_quality.items(), key=lambda x: x[1][1], reverse=True):\n",
    "        final_engineered_features[name] = corr\n",
    "        print(f\"  {name}: {corr:.3f} ({improvement:+.1f}% vs individual {individual_best:.3f})\")\n",
    "\n",
    "# Bathroom/room combinations that beat individual components\n",
    "print(\"\\nSUCCESSFUL BATHROOM/ROOM COMBINATIONS:\")\n",
    "if 'successful_bath_room' in locals():\n",
    "    for name, (corr, improvement, individual_best) in sorted(successful_bath_room.items(), key=lambda x: x[1][1], reverse=True):\n",
    "        final_engineered_features[name] = corr\n",
    "        print(f\"  {name}: {corr:.3f} ({improvement:+.1f}% vs individual {individual_best:.3f})\")\n",
    "\n",
    "# Engineering strategy analysis\n",
    "print(f\"\\nENGINEERING STRATEGY ANALYSIS:\")\n",
    "print(f\"Total engineered features selected: {len(final_engineered_features)}\")\n",
    "\n",
    "# Categorize by strategy type\n",
    "addition_features = [name for name in final_engineered_features.keys() if '_add_' in name]\n",
    "multiply_features = [name for name in final_engineered_features.keys() if '_multiply_' in name]\n",
    "standard_features = [name for name in final_engineered_features.keys() if 'Standard' in name or 'All' in name]\n",
    "\n",
    "print(f\"Addition strategy successes: {len(addition_features)}\")\n",
    "print(f\"Multiplication strategy successes: {len(multiply_features)}\")\n",
    "print(f\"Standard formula successes: {len(standard_features)}\")\n",
    "\n",
    "# Create final feature recommendations\n",
    "print(f\"\\nFINAL FEATURE RECOMMENDATIONS:\")\n",
    "print(f\"Top engineered features by correlation strength:\")\n",
    "sorted_features = sorted(final_engineered_features.items(), key=lambda x: x[1], reverse=True)\n",
    "for i, (name, corr) in enumerate(sorted_features, 1):\n",
    "    print(f\"  {i}. {name}: {corr:.3f}\")\n",
    "\n",
    "# Engineering methodology summary\n",
    "print(f\"\\nMETHODOLOGY VALIDATION:\")\n",
    "print(f\"✓ Addition strategy: Effective for area features\")\n",
    "print(f\"✓ Multiplication strategy: Effective for quality-area interactions\")\n",
    "print(f\"✓ Standard formulas: Effective for bathroom calculations\")\n",
    "print(f\"✗ Ratio strategy: Consistently ineffective across all categories\")\n",
    "\n",
    "# CRITICAL: Add successful engineered features to df_combined dataset\n",
    "print(f\"\\nADDING ENGINEERED FEATURES TO DATASET:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "features_added = 0\n",
    "features_skipped = 0\n",
    "\n",
    "# Recreate successful area combinations\n",
    "for name in addition_features:\n",
    "    if '_add_' in name:\n",
    "        parts = name.split('_add_')\n",
    "        if len(parts) == 2:\n",
    "            feat1, feat2 = parts\n",
    "            if feat1 in df_combined.columns and feat2 in df_combined.columns:\n",
    "                df_combined[name] = df_combined[feat1] + df_combined[feat2]\n",
    "                features_added += 1\n",
    "                print(f\"  Added: {name}\")\n",
    "            else:\n",
    "                features_skipped += 1\n",
    "                print(f\"  Skipped: {name} (missing components)\")\n",
    "\n",
    "# Recreate successful quality interactions\n",
    "for name in multiply_features:\n",
    "    if '_multiply_' in name:\n",
    "        parts = name.split('_multiply_')\n",
    "        if len(parts) == 2:\n",
    "            feat1, feat2 = parts\n",
    "            if feat1 in df_combined.columns and feat2 in df_combined.columns:\n",
    "                df_combined[name] = df_combined[feat1] * df_combined[feat2]\n",
    "                features_added += 1\n",
    "                print(f\"  Added: {name}\")\n",
    "            else:\n",
    "                features_skipped += 1\n",
    "                print(f\"  Skipped: {name} (missing components)\")\n",
    "\n",
    "# Recreate successful bathroom and room standard formulas\n",
    "for name in standard_features:\n",
    "    if 'TotalBaths_Standard' in name:\n",
    "        if all(col in df_combined.columns for col in ['FullBath', 'HalfBath']):\n",
    "            df_combined[name] = df_combined['FullBath'] + 0.5 * df_combined['HalfBath']\n",
    "            features_added += 1\n",
    "            print(f\"  Added: {name}\")\n",
    "        else:\n",
    "            features_skipped += 1\n",
    "            print(f\"  Skipped: {name} (missing bathroom components)\")\n",
    "    \n",
    "    elif 'TotalBaths_All' in name:\n",
    "        bathroom_cols = ['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath']\n",
    "        if all(col in df_combined.columns for col in bathroom_cols):\n",
    "            df_combined[name] = (df_combined['FullBath'] + df_combined['BsmtFullBath'] + \n",
    "                               0.5 * (df_combined['HalfBath'] + df_combined['BsmtHalfBath']))\n",
    "            features_added += 1\n",
    "            print(f\"  Added: {name}\")\n",
    "        else:\n",
    "            features_skipped += 1\n",
    "            print(f\"  Skipped: {name} (missing bathroom components)\")\n",
    "    \n",
    "    elif 'TotalRooms_All' in name:\n",
    "        room_cols = ['BedroomAbvGr', 'TotRmsAbvGrd', 'Fireplaces']\n",
    "        if all(col in df_combined.columns for col in room_cols):\n",
    "            df_combined[name] = df_combined['BedroomAbvGr'] + df_combined['TotRmsAbvGrd'] + df_combined['Fireplaces']\n",
    "            features_added += 1\n",
    "            print(f\"  Added: {name}\")\n",
    "        else:\n",
    "            features_skipped += 1\n",
    "            print(f\"  Skipped: {name} (missing room components)\")\n",
    "\n",
    "print(f\"\\nFEATURE ADDITION SUMMARY:\")\n",
    "print(f\"Features successfully added to dataset: {features_added}\")\n",
    "print(f\"Features skipped (missing components): {features_skipped}\")\n",
    "print(f\"Total dataset features: {len(df_combined.columns)}\")\n",
    "\n",
    "# Validate feature addition\n",
    "new_feature_count = len(df_combined.columns)\n",
    "print(f\"\\nValidation: Dataset now contains {new_feature_count} features including {features_added} new engineered features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Systematic feature engineering produces 24 successful combinations beating individual component baselines with improvements ranging 0.7% to 30.8%.\n",
    "Successful engineered features added to df_combined dataset ensuring availability for subsequent analysis sections, with strategy validation confirming addition (12 features) and multiplication (9 features) effectiveness while ratio approaches fail consistently.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Age-Based and Temporal Feature Engineering\n",
    "\n",
    "Calculate property age using sale year reference and create lifecycle-based features.\n",
    "Handle temporal anomalies and implement remodel recency calculations for property improvement analysis.\n",
    "\n",
    "### 3.1 Age Calculation Feature Creation\n",
    "\n",
    "Calculate property age using sale year reference and investigate temporal data quality issues.\n",
    "Implement systematic age feature creation with data-driven anomaly detection and correction pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic age feature creation using sale year reference\n",
    "age_features = {}\n",
    "\n",
    "# Create initial age features without assumptions\n",
    "if all(col in df_combined.columns for col in ['YrSold', 'YearBuilt']):\n",
    "    df_combined['PropertyAge'] = df_combined['YrSold'] - df_combined['YearBuilt']\n",
    "    age_features['PropertyAge'] = df_combined['PropertyAge']\n",
    "\n",
    "if all(col in df_combined.columns for col in ['YrSold', 'GarageYrBlt']):\n",
    "    # Fill missing garage years with 0 for calculation\n",
    "    df_combined['GarageYrBlt'].fillna(0, inplace=True)\n",
    "    df_combined['GarageAge'] = df_combined['YrSold'] - df_combined['GarageYrBlt']\n",
    "    age_features['GarageAge'] = df_combined['GarageAge']\n",
    "\n",
    "if all(col in df_combined.columns for col in ['YrSold', 'YearRemodAdd']):\n",
    "    df_combined['RemodAge'] = df_combined['YrSold'] - df_combined['YearRemodAdd']\n",
    "    age_features['RemodAge'] = df_combined['RemodAge']\n",
    "\n",
    "# Display initial ranges to show data quality issues\n",
    "print(\"Initial age feature ranges:\")\n",
    "for feature_name, feature_data in age_features.items():\n",
    "    print(f\"{feature_name}: {feature_data.min():.0f} to {feature_data.max():.0f} years\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age calculation produces negative values (-1 to -2 years) and extreme garage ages (2010 years) requiring investigation.\n",
    "Initial ranges: PropertyAge (-1 to 136), GarageAge (-200 to 2010), RemodAge (-2 to 60) indicating data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality investigation for temporal anomalies\n",
    "print(\"TEMPORAL DATA QUALITY INVESTIGATION:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. NEGATIVE PROPERTY AGE INVESTIGATION\n",
    "negative_property = df_combined['PropertyAge'] < 0\n",
    "print(f\"1. Properties with negative PropertyAge: {negative_property.sum()}\")\n",
    "if negative_property.any():\n",
    "    negative_cases = df_combined[negative_property][['Id', 'YrSold', 'YearBuilt', 'PropertyAge']]\n",
    "    print(\"   Future construction cases:\")\n",
    "    for _, row in negative_cases.iterrows():\n",
    "        print(f\"   ID {row['Id']}: Sale {row['YrSold']}, Built {row['YearBuilt']}, Age {row['PropertyAge']}\")\n",
    "\n",
    "# 2. NEGATIVE GARAGE AGE INVESTIGATION\n",
    "negative_garage = df_combined['GarageAge'] < 0\n",
    "print(f\"\\n2. Properties with negative GarageAge: {negative_garage.sum()}\")\n",
    "if negative_garage.any():\n",
    "    garage_cases = df_combined[negative_garage][['Id', 'YrSold', 'GarageYrBlt', 'GarageAge']]\n",
    "    print(\"   Future garage construction cases:\")\n",
    "    for _, row in garage_cases.iterrows():\n",
    "        print(f\"   ID {row['Id']}: Sale {row['YrSold']}, Garage {row['GarageYrBlt']}, Age {row['GarageAge']}\")\n",
    "\n",
    "# 3. NEGATIVE REMODEL AGE INVESTIGATION\n",
    "negative_remodel = df_combined['RemodAge'] < 0\n",
    "print(f\"\\n3. Properties with negative RemodAge: {negative_remodel.sum()}\")\n",
    "if negative_remodel.any():\n",
    "    remodel_cases = df_combined[negative_remodel][['Id', 'YrSold', 'YearRemodAdd', 'RemodAge']]\n",
    "    print(\"   Future remodel cases:\")\n",
    "    for _, row in remodel_cases.iterrows():\n",
    "        print(f\"   ID {row['Id']}: Sale {row['YrSold']}, Remodel {row['YearRemodAdd']}, Age {row['RemodAge']}\")\n",
    "\n",
    "# 4. TEMPORAL ANOMALY INVESTIGATION (Garages built before houses)\n",
    "if all(col in df_combined.columns for col in ['GarageYrBlt', 'YearBuilt']):\n",
    "    has_garage_year = df_combined['GarageYrBlt'] > 0\n",
    "    garage_before_house = (df_combined['GarageYrBlt'] < df_combined['YearBuilt']) & has_garage_year\n",
    "\n",
    "    print(f\"\\n4. Properties with garages built BEFORE houses: {garage_before_house.sum()}\")\n",
    "    if garage_before_house.any():\n",
    "        temporal_cases = df_combined[garage_before_house][['Id', 'YearBuilt', 'GarageYrBlt']].head(10)\n",
    "        print(\"   Temporal anomaly cases:\")\n",
    "        for _, row in temporal_cases.iterrows():\n",
    "            years_diff = row['YearBuilt'] - row['GarageYrBlt']\n",
    "            print(f\"   ID {row['Id']}: House {row['YearBuilt']}, Garage {row['GarageYrBlt']} ({years_diff} years difference)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigation reveals 1 future construction (ID 2550), 2 future remodels (IDs 2296, 2550), 1 data entry error (ID 2593: 2207→2007), and 18 garages built before houses.\n",
    "Most anomalies involve ID 2550 with multiple future dates, while garages-before-houses represent legitimate historical construction patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply data-driven corrections based on investigation findings\n",
    "print(\"\\nAPPLYING TARGETED CORRECTIONS:\")\n",
    "\n",
    "# First fix obvious data entry errors\n",
    "if 'GarageYrBlt' in df_combined.columns:\n",
    "    # Fix ID 2593: 2207 → 2007 (identified in investigation)\n",
    "    error_2207 = df_combined['GarageYrBlt'] == 2207\n",
    "    if error_2207.any():\n",
    "        df_combined.loc[error_2207, 'GarageYrBlt'] = 2007\n",
    "        print(f\"Fixed data entry error: 2207 → 2007 for {error_2207.sum()} properties\")\n",
    "\n",
    "# Test two approaches for \"no garage\" (GarageYrBlt=0) properties\n",
    "print(\"\\nTesting garage age calculation approaches:\")\n",
    "\n",
    "# Approach 1: GarageAge = PropertyAge for no garage\n",
    "df_test1 = df_combined.copy()\n",
    "missing_garage_1 = df_test1['GarageYrBlt'] == 0\n",
    "df_test1.loc[missing_garage_1, 'GarageYrBlt'] = df_test1.loc[missing_garage_1, 'YearBuilt']\n",
    "df_test1['GarageAge'] = df_test1['YrSold'] - df_test1['GarageYrBlt']\n",
    "\n",
    "# Approach 2: GarageAge = 0 for no garage\n",
    "df_test2 = df_combined.copy()\n",
    "df_test2['GarageAge'] = df_test2['YrSold'] - df_test2['GarageYrBlt']\n",
    "missing_garage_2 = df_test2['GarageYrBlt'] == 0\n",
    "df_test2.loc[missing_garage_2, 'GarageAge'] = 0\n",
    "\n",
    "# Compare correlations (train data only)\n",
    "train_mask = df_combined['dataset_source'] == 'train'\n",
    "target_col = 'SalePrice_log' if 'SalePrice_log' in df_train_clean.columns else 'SalePrice'\n",
    "\n",
    "if target_col in df_train_clean.columns and train_mask.sum() == len(df_train_clean):\n",
    "    target_data = df_train_clean[target_col]\n",
    "    corr1 = df_test1.loc[train_mask, 'GarageAge'].corr(target_data)\n",
    "    corr2 = df_test2.loc[train_mask, 'GarageAge'].corr(target_data)\n",
    "\n",
    "    print(f\"Approach 1 (GarageAge = PropertyAge for no garage): {corr1:.3f}\")\n",
    "    print(f\"Approach 2 (GarageAge = 0 for no garage): {corr2:.3f}\")\n",
    "\n",
    "    # Apply the better approach\n",
    "    if abs(corr1) > abs(corr2):\n",
    "        print(\"Using Approach 1: GarageAge = PropertyAge for no garage\")\n",
    "        df_combined = df_test1.copy()\n",
    "        chosen_approach = \"PropertyAge\"\n",
    "    else:\n",
    "        print(\"Using Approach 2: GarageAge = 0 for no garage\")\n",
    "        df_combined = df_test2.copy()\n",
    "        chosen_approach = \"Zero\"\n",
    "else:\n",
    "    print(\"Using default Approach 2: GarageAge = 0 for no garage\")\n",
    "    df_combined['GarageAge'] = df_combined['YrSold'] - df_combined['GarageYrBlt']\n",
    "    missing_garage = df_combined['GarageYrBlt'] == 0\n",
    "    df_combined.loc[missing_garage, 'GarageAge'] = 0\n",
    "    chosen_approach = \"Zero\"\n",
    "\n",
    "# Age clipping: handle negative ages with logical approach\n",
    "# Future construction/remodel treated as brand new (0 years old)\n",
    "clipped_property = (df_combined['PropertyAge'] < 0).sum()\n",
    "clipped_garage = (df_combined['GarageAge'] < 0).sum()\n",
    "clipped_remodel = (df_combined['RemodAge'] < 0).sum()\n",
    "\n",
    "df_combined['PropertyAge'] = df_combined['PropertyAge'].clip(lower=0)\n",
    "df_combined['GarageAge'] = df_combined['GarageAge'].clip(lower=0)\n",
    "df_combined['RemodAge'] = df_combined['RemodAge'].clip(lower=0)\n",
    "\n",
    "print(f\"Age clipping applied: {clipped_property} property, {clipped_garage} garage, {clipped_remodel} remodel\")\n",
    "\n",
    "# Fix missing garage year logic: no garage year = garage age equals property age\n",
    "missing_garage_mask = df_combined['GarageYrBlt'] == 0\n",
    "df_combined.loc[missing_garage_mask, 'GarageAge'] = df_combined.loc[missing_garage_mask, 'PropertyAge']\n",
    "\n",
    "# Create remodel indicator before dropping year features\n",
    "df_combined['IsRemodeled'] = (df_combined['YearRemodAdd'] != df_combined['YearBuilt']).astype(int)\n",
    "age_features['IsRemodeled'] = df_combined['IsRemodeled']\n",
    "\n",
    "# Drop original year features - age features provide better interpretability\n",
    "print(\"\\nReplacing year features with age features for better interpretability:\")\n",
    "print(\"Dropping: YearBuilt, GarageYrBlt, YearRemodAdd\")\n",
    "\n",
    "year_features_to_drop = ['YearBuilt', 'GarageYrBlt', 'YearRemodAdd']\n",
    "features_before = len(df_combined.columns)\n",
    "df_combined = df_combined.drop(columns=year_features_to_drop, errors='ignore')\n",
    "features_after = len(df_combined.columns)\n",
    "print(f\"Features: {features_before} → {features_after} (removed {features_before - features_after} year features)\")\n",
    "\n",
    "# Final age feature correlation analysis\n",
    "print(\"\\nFINAL AGE FEATURE PERFORMANCE:\")\n",
    "train_mask = df_combined['dataset_source'] == 'train'\n",
    "target_data = df_train_clean[target_col] if 'df_train_clean' in locals() else None\n",
    "\n",
    "if target_data is not None:\n",
    "    age_correlations = {}\n",
    "    for feature_name in ['PropertyAge', 'GarageAge', 'RemodAge', 'IsRemodeled']:\n",
    "        if feature_name in df_combined.columns:\n",
    "            train_feature = df_combined.loc[train_mask, feature_name]\n",
    "            if len(train_feature) == len(target_data):\n",
    "                corr = train_feature.corr(target_data)\n",
    "                age_correlations[feature_name] = corr\n",
    "                print(f\"{feature_name}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design-time redundancy prevention applied: original year features dropped after age feature creation for better interpretability.\n",
    "Age-based temporal features demonstrate strong depreciation patterns (PropertyAge: -0.588, GarageAge: -0.570, RemodAge: -0.569) while eliminating correlation redundancy through immediate feature replacement strategy.\n",
    "\n",
    "### 3.2 Remodel and Improvement Recency\n",
    "\n",
    "Calculate years since last remodel and create improvement recency features.\n",
    "Test interaction between property age and remodel recency for renovation impact analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remodel recency calculation (years since last remodel)\n",
    "remodel_features = {}\n",
    "\n",
    "# Years since last remodel (using existing RemodAge as base)\n",
    "if 'RemodAge' in df_combined.columns:\n",
    "    # RemodAge is years since remodel (YrSold - YearRemodAdd)\n",
    "    # Convert to positive \"years since improvement\" for better interpretation\n",
    "    df_combined['YearsSinceRemod'] = df_combined['RemodAge']\n",
    "    remodel_features['YearsSinceRemod'] = df_combined['YearsSinceRemod']\n",
    "\n",
    "# Remodel recency categories (recent vs old improvements)\n",
    "if 'YearsSinceRemod' in df_combined.columns:\n",
    "    # Recent remodel: within 10 years of sale\n",
    "    df_combined['RecentRemod'] = (df_combined['YearsSinceRemod'] <= 10).astype(int)\n",
    "    remodel_features['RecentRemod'] = df_combined['RecentRemod']\n",
    "\n",
    "    # Very recent remodel: within 5 years of sale\n",
    "    df_combined['VeryRecentRemod'] = (df_combined['YearsSinceRemod'] <= 5).astype(int)\n",
    "    remodel_features['VeryRecentRemod'] = df_combined['VeryRecentRemod']\n",
    "\n",
    "# Property improvement intensity (remodel frequency)\n",
    "# Calculate improvement gap using existing age features (PropertyAge - RemodAge)\n",
    "if all(col in df_combined.columns for col in ['PropertyAge', 'RemodAge']):\n",
    "    # Gap = time between construction and first remodel\n",
    "    df_combined['ImprovementGap'] = df_combined['PropertyAge'] - df_combined['RemodAge']\n",
    "    # Properties with gap=0 were never remodeled\n",
    "    remodel_features['ImprovementGap'] = df_combined['ImprovementGap']\n",
    "\n",
    "# Display remodel feature distributions\n",
    "print(\"REMODEL RECENCY FEATURE ANALYSIS:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for feature_name, feature_data in remodel_features.items():\n",
    "    print(f\"\\n{feature_name}:\")\n",
    "    if feature_data.dtype in ['int64', 'float64']:\n",
    "        print(f\"  Range: {feature_data.min():.0f} to {feature_data.max():.0f}\")\n",
    "        print(f\"  Mean: {feature_data.mean():.1f}\")\n",
    "    else:\n",
    "        print(f\"  Value counts: {feature_data.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remodel recency features reveal 40% of properties have recent improvements (≤10 years) with 25-year average since last renovation.\n",
    "ImprovementGap negative value indicates data quality issue requiring investigation before correlation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First investigate the negative ImprovementGap value\n",
    "print(\"IMPROVEMENT GAP INVESTIGATION:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "negative_gap = df_combined['ImprovementGap'] < 0\n",
    "print(f\"Properties with negative ImprovementGap: {negative_gap.sum()}\")\n",
    "if negative_gap.any():\n",
    "    gap_cases = df_combined[negative_gap][['Id', 'PropertyAge', 'RemodAge', 'ImprovementGap']].head(5)\n",
    "    print(\"Sample negative gap cases:\")\n",
    "    for _, row in gap_cases.iterrows():\n",
    "        print(f\"  ID {row['Id']}: PropertyAge {row['PropertyAge']}, RemodAge {row['RemodAge']}, Gap {row['ImprovementGap']}\")\n",
    "\n",
    "# Fix negative gaps (remodel before build year - data quality issue)\n",
    "df_combined['ImprovementGap'] = df_combined['ImprovementGap'].clip(lower=0)\n",
    "print(f\"Negative gaps clipped to 0, new range: {df_combined['ImprovementGap'].min()} to {df_combined['ImprovementGap'].max()}\")\n",
    "\n",
    "# Calculate remodel feature correlations with target (train data only)\n",
    "print(\"\\nREMODEL FEATURE CORRELATIONS:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "train_mask = df_combined['dataset_source'] == 'train'\n",
    "target_col = 'SalePrice_log' if 'SalePrice_log' in df_train_clean.columns else 'SalePrice'\n",
    "\n",
    "if target_col in df_train_clean.columns:\n",
    "    target_data = df_train_clean[target_col]\n",
    "    remodel_correlations = {}\n",
    "\n",
    "    for feature_name in ['YearsSinceRemod', 'RecentRemod', 'VeryRecentRemod', 'ImprovementGap']:\n",
    "        if feature_name in df_combined.columns:\n",
    "            train_feature = df_combined.loc[train_mask, feature_name]\n",
    "            if len(train_feature) == len(target_data):\n",
    "                corr = train_feature.corr(target_data)\n",
    "                remodel_correlations[feature_name] = corr\n",
    "                print(f\"{feature_name}: {corr:.3f}\")\n",
    "\n",
    "# Age-remodel interaction features\n",
    "print(\"\\nAGE-REMODEL INTERACTION FEATURES:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Property lifecycle stage: age vs remodel gap ratio\n",
    "if all(col in df_combined.columns for col in ['PropertyAge', 'ImprovementGap']):\n",
    "    # Avoid division by zero for properties never remodeled\n",
    "    mask = df_combined['ImprovementGap'] > 0\n",
    "    df_combined['LifecycleRatio'] = 0  # Default for never remodeled\n",
    "    df_combined.loc[mask, 'LifecycleRatio'] = (\n",
    "        df_combined.loc[mask, 'PropertyAge'] / df_combined.loc[mask, 'ImprovementGap']\n",
    "    )\n",
    "\n",
    "    # Calculate correlation for lifecycle ratio\n",
    "    if target_col in df_train_clean.columns:\n",
    "        train_lifecycle = df_combined.loc[train_mask, 'LifecycleRatio']\n",
    "        lifecycle_corr = train_lifecycle.corr(target_data)\n",
    "        print(f\"LifecycleRatio: {lifecycle_corr:.3f}\")\n",
    "\n",
    "# Renovation freshness: property age adjusted by recent improvements\n",
    "if all(col in df_combined.columns for col in ['PropertyAge', 'RecentRemod']):\n",
    "    # Effective age: reduced for recent renovations\n",
    "    df_combined['EffectiveAge'] = df_combined['PropertyAge'] * (1 - 0.3 * df_combined['RecentRemod'])\n",
    "\n",
    "    # Calculate correlation for effective age\n",
    "    if target_col in df_train_clean.columns:\n",
    "        train_effective = df_combined.loc[train_mask, 'EffectiveAge']\n",
    "        effective_corr = train_effective.corr(target_data)\n",
    "        print(f\"EffectiveAge: {effective_corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remodel correlation analysis confirms renovation timing significantly impacts property valuation with RecentRemod (0.426) and EffectiveAge (-0.600) showing strongest predictive relationships.\n",
    "Single data quality issue corrected while lifecycle interaction features demonstrate limited correlation potential for engineering applications.\n",
    "\n",
    "Age-based temporal engineering delivers strong correlation features with EffectiveAge (-0.600) outperforming individual PropertyAge (-0.588) through renovation adjustment logic.\n",
    "Systematic temporal anomaly correction preserves 100% data integrity while RecentRemod (0.426) validates renovation timing importance for property valuation accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Ratio and Efficiency Feature Creation\n",
    "\n",
    "Create garage efficiency features by dividing GarageArea by GarageCars to measure space per vehicle.\n",
    "Test area efficiency ratios (GrLivArea/TotRmsAbvGrd, TotalBsmtSF/BedroomAbvGr) to capture room size patterns.\n",
    "\n",
    "### 4.1 Efficiency Ratio Discovery and Creation\n",
    "\n",
    "Create garage efficiency features by dividing GarageArea by GarageCars to measure space per vehicle.\n",
    "Test area efficiency ratios (GrLivArea/TotRmsAbvGrd, TotalBsmtSF/BedroomAbvGr) to capture room size patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create efficiency ratio features focusing on space utilization metrics\n",
    "# Test garage, room, and area efficiency ratios with baseline comparison\n",
    "\n",
    "efficiency_features = {}\n",
    "\n",
    "print(\"RATIO AND EFFICIENCY FEATURE CREATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Garage efficiency: space per car\n",
    "print(\"\\n1. GARAGE EFFICIENCY RATIOS:\")\n",
    "if all(col in df_combined.columns for col in ['GarageArea', 'GarageCars']):\n",
    "    # Avoid division by zero - only calculate for properties with cars\n",
    "    car_mask = df_combined['GarageCars'] > 0\n",
    "    print(f\"Properties with garage cars: {car_mask.sum()} of {len(df_combined)}\")\n",
    "\n",
    "    if car_mask.sum() > 100:  # Ensure sufficient data\n",
    "        df_combined['GarageArea_per_Car'] = 0  # Default for no cars\n",
    "        df_combined.loc[car_mask, 'GarageArea_per_Car'] = (\n",
    "            df_combined.loc[car_mask, 'GarageArea'] / df_combined.loc[car_mask, 'GarageCars']\n",
    "        )\n",
    "        efficiency_features['GarageArea_per_Car'] = df_combined['GarageArea_per_Car']\n",
    "\n",
    "        # Display efficiency statistics\n",
    "        efficiency_stats = df_combined.loc[car_mask, 'GarageArea_per_Car']\n",
    "        print(f\"Garage efficiency range: {efficiency_stats.min():.0f} to {efficiency_stats.max():.0f} sq ft per car\")\n",
    "        print(f\"Average garage space per car: {efficiency_stats.mean():.0f} sq ft\")\n",
    "\n",
    "# Room efficiency: living area per room\n",
    "print(\"\\n2. ROOM EFFICIENCY RATIOS:\")\n",
    "room_ratios = [\n",
    "    ('GrLivArea', 'TotRmsAbvGrd', 'Living area per room'),\n",
    "    ('GrLivArea', 'BedroomAbvGr', 'Living area per bedroom'),\n",
    "    ('TotalBsmtSF', 'BedroomAbvGr', 'Basement per bedroom')\n",
    "]\n",
    "\n",
    "for area_col, room_col, description in room_ratios:\n",
    "    if all(col in df_combined.columns for col in [area_col, room_col]):\n",
    "        room_mask = df_combined[room_col] > 0\n",
    "        print(f\"\\n{description}:\")\n",
    "        print(f\"Properties with {room_col} > 0: {room_mask.sum()}\")\n",
    "\n",
    "        if room_mask.sum() > 100:\n",
    "            ratio_name = f\"{area_col}_per_{room_col}\"\n",
    "            df_combined[ratio_name] = 0  # Default for zero rooms\n",
    "            df_combined.loc[room_mask, ratio_name] = (\n",
    "                df_combined.loc[room_mask, area_col] / df_combined.loc[room_mask, room_col]\n",
    "            )\n",
    "            efficiency_features[ratio_name] = df_combined[ratio_name]\n",
    "\n",
    "            # Display ratio statistics\n",
    "            ratio_stats = df_combined.loc[room_mask, ratio_name]\n",
    "            print(f\"Range: {ratio_stats.min():.0f} to {ratio_stats.max():.0f}\")\n",
    "            print(f\"Average: {ratio_stats.mean():.0f}\")\n",
    "\n",
    "# Lot utilization: building footprint efficiency\n",
    "print(\"\\n3. LOT UTILIZATION RATIOS:\")\n",
    "lot_ratios = [\n",
    "    ('GrLivArea', 'LotArea', 'Living area utilization'),\n",
    "    ('1stFlrSF', 'LotArea', 'Footprint utilization'),\n",
    "    ('TotalBsmtSF', 'LotArea', 'Basement utilization')\n",
    "]\n",
    "\n",
    "for building_col, lot_col, description in lot_ratios:\n",
    "    if all(col in df_combined.columns for col in [building_col, lot_col]):\n",
    "        lot_mask = df_combined[lot_col] > 0\n",
    "        print(f\"\\n{description}:\")\n",
    "\n",
    "        if lot_mask.sum() > 100:\n",
    "            ratio_name = f\"{building_col}_per_{lot_col}\"\n",
    "            df_combined[ratio_name] = (\n",
    "                df_combined[building_col] / df_combined[lot_col]\n",
    "            )\n",
    "            efficiency_features[ratio_name] = df_combined[ratio_name]\n",
    "\n",
    "            # Display utilization statistics (as percentages)\n",
    "            ratio_stats = df_combined.loc[lot_mask, ratio_name] * 100\n",
    "            print(f\"Range: {ratio_stats.min():.2f}% to {ratio_stats.max():.2f}%\")\n",
    "            print(f\"Average: {ratio_stats.mean():.2f}%\")\n",
    "\n",
    "print(f\"\\nTotal efficiency features created: {len(efficiency_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficiency ratio creation reveals realistic property utilization patterns with 272 sq ft average garage space per car and 18% lot utilization.\n",
    "Room efficiency ratios span wide ranges indicating diverse property layouts requiring correlation analysis for predictive value assessment.\n",
    "\n",
    "### 4.2 Size Normalization Features\n",
    "\n",
    "Create per-square-foot value features and lot utilization ratios for property efficiency analysis.\n",
    "Test building footprint efficiency and land use optimization features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test efficiency ratio correlations with target variable\n",
    "# Compare ratio performance against individual component correlations\n",
    "\n",
    "print(\"EFFICIENCY RATIO CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "train_mask = df_combined['dataset_source'] == 'train'\n",
    "target_col = 'SalePrice_log' if 'SalePrice_log' in df_train_clean.columns else 'SalePrice'\n",
    "\n",
    "if target_col in df_train_clean.columns:\n",
    "    target_data = df_train_clean[target_col]\n",
    "    efficiency_correlations = {}\n",
    "\n",
    "    print(\"\\nEFFICIENCY RATIO CORRELATIONS:\")\n",
    "    for feature_name in efficiency_features.keys():\n",
    "        if feature_name in df_combined.columns:\n",
    "            train_feature = df_combined.loc[train_mask, feature_name]\n",
    "            if len(train_feature) == len(target_data):\n",
    "                corr = train_feature.corr(target_data)\n",
    "                efficiency_correlations[feature_name] = corr\n",
    "                print(f\"{feature_name}: {corr:.3f}\")\n",
    "\n",
    "    # Compare against baseline individual features\n",
    "    print(\"\\nCOMPARISON WITH INDIVIDUAL COMPONENTS:\")\n",
    "    print(\"(Previous correlations from section 2.1)\")\n",
    "    baseline_comparisons = {\n",
    "        'GarageArea_per_Car': ('GarageArea', 'GarageCars'),\n",
    "        'GrLivArea_per_TotRmsAbvGrd': ('GrLivArea', 'TotRmsAbvGrd'),\n",
    "        'GrLivArea_per_BedroomAbvGr': ('GrLivArea', 'BedroomAbvGr'),\n",
    "        'TotalBsmtSF_per_BedroomAbvGr': ('TotalBsmtSF', 'BedroomAbvGr')\n",
    "    }\n",
    "\n",
    "    for ratio_name, (comp1, comp2) in baseline_comparisons.items():\n",
    "        if ratio_name in efficiency_correlations:\n",
    "            ratio_corr = efficiency_correlations[ratio_name]\n",
    "            # Get individual correlations from previous analysis\n",
    "            comp1_corr = area_correlations.get(comp1, bath_room_correlations.get(comp1, 0))\n",
    "            comp2_corr = bath_room_correlations.get(comp2, area_correlations.get(comp2, 0))\n",
    "            best_individual = max(abs(comp1_corr), abs(comp2_corr))\n",
    "\n",
    "            print(f\"\\n{ratio_name}: {ratio_corr:.3f}\")\n",
    "            print(f\"  vs {comp1}: {comp1_corr:.3f}\")\n",
    "            print(f\"  vs {comp2}: {comp2_corr:.3f}\")\n",
    "            print(f\"  Best individual: {best_individual:.3f}\")\n",
    "\n",
    "            if abs(ratio_corr) > best_individual:\n",
    "                improvement = ((abs(ratio_corr) - best_individual) / best_individual) * 100\n",
    "                print(f\"  ✓ Ratio improves by {improvement:.1f}%\")\n",
    "            else:\n",
    "                decline = ((best_individual - abs(ratio_corr)) / best_individual) * 100\n",
    "                print(f\"  ✗ Ratio declines by {decline:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficiency ratio analysis confirms systematic underperformance with all ratios declining 21-69% below individual component correlations.\n",
    "GrLivArea_per_TotRmsAbvGrd (0.569) shows least decline at 21.5% while garage efficiency drops 69.2%, validating ratio strategy ineffectiveness for property valuation modeling.\n",
    "\n",
    "Ratio-based feature engineering demonstrates comprehensive failure across garage, room, and lot utilization categories with 21-69% correlation decline from individual components.\n",
    "Systematic ratio ineffectiveness validates addition and multiplication strategy superiority while confirming space efficiency metrics inadequacy for property valuation prediction accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Distribution Analysis and Transformation Strategy\n",
    "\n",
    "Analyze distribution characteristics of all engineered features and identify transformation requirements.\n",
    "Apply log transformation to highly skewed engineered features for improved model performance.\n",
    "\n",
    "### 5.1 Skewness Detection and Analysis\n",
    "\n",
    "Calculate skewness for all engineered features and identify transformation candidates.\n",
    "Implement systematic transformation testing for features with |skewness| > 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ENGINEERED FEATURE DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Collect all engineered features created in previous sections\n",
    "engineered_feature_list = []\n",
    "\n",
    "# Section 2 features: successful combinations from final_engineered_features\n",
    "if 'final_engineered_features' in locals():\n",
    "    engineered_feature_list.extend(final_engineered_features.keys())\n",
    "    print(f\"Section 2 engineered features: {len(final_engineered_features)}\")\n",
    "else:\n",
    "    print(\"Section 2 engineered features: 0 (run Section 2.5 first to populate final_engineered_features)\")\n",
    "\n",
    "# Section 3 features: collect all age and remodel features created in sections 3.1-3.2\n",
    "print(\"Collecting age and remodel features from Section 3...\")\n",
    "section3_features = []\n",
    "\n",
    "# Collect from age_features (Section 3.1)\n",
    "if 'age_features' in locals():\n",
    "    section3_features.extend(age_features.keys())\n",
    "    \n",
    "# Collect from remodel_features (Section 3.2)\n",
    "if 'remodel_features' in locals():\n",
    "    section3_features.extend(remodel_features.keys())\n",
    "\n",
    "# Add interaction features from Section 3.2\n",
    "interaction_features = ['LifecycleRatio', 'EffectiveAge']\n",
    "section3_features.extend([f for f in interaction_features if f in df_combined.columns])\n",
    "\n",
    "# Filter to only existing features in dataset\n",
    "section3_existing = [f for f in section3_features if f in df_combined.columns]\n",
    "engineered_feature_list.extend(section3_existing)\n",
    "print(f\"Section 3 age/remodel features: {len(section3_existing)}\")\n",
    "\n",
    "# Section 4 features: efficiency ratios\n",
    "if 'efficiency_features' in locals():\n",
    "    engineered_feature_list.extend(efficiency_features.keys())\n",
    "    print(f\"Section 4 efficiency features: {len(efficiency_features)}\")\n",
    "\n",
    "print(f\"\\nTotal engineered features for analysis: {len(engineered_feature_list)}\")\n",
    "\n",
    "# Calculate skewness for all engineered features\n",
    "feature_skewness = {}\n",
    "transformation_candidates = {}\n",
    "\n",
    "print(f\"\\nSKEWNESS ANALYSIS:\")\n",
    "print(f\"Feature Name{' '*25} | Skewness | Status\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for feature_name in engineered_feature_list:\n",
    "    if feature_name in df_combined.columns:\n",
    "        feature_data = df_combined[feature_name]\n",
    "\n",
    "        # Calculate skewness (exclude zero values for ratio features)\n",
    "        if feature_data.nunique() > 1:  # Avoid constant features\n",
    "            skew_value = stats.skew(feature_data.dropna())\n",
    "            feature_skewness[feature_name] = skew_value\n",
    "\n",
    "            # Determine transformation requirement\n",
    "            if abs(skew_value) > 0.5:\n",
    "                status = \"TRANSFORM\" if abs(skew_value) > 1.0 else \"CONSIDER\"\n",
    "                transformation_candidates[feature_name] = skew_value\n",
    "            else:\n",
    "                status = \"NORMAL\"\n",
    "\n",
    "            print(f\"{feature_name:<35} | {skew_value:>8.3f} | {status}\")\n",
    "\n",
    "print(f\"\\nTRANSFORMATION SUMMARY:\")\n",
    "print(f\"Features requiring transformation (|skew| > 1.0): {len([f for f, s in transformation_candidates.items() if abs(s) > 1.0])}\")\n",
    "print(f\"Features to consider (0.5 < |skew| ≤ 1.0): {len([f for f, s in transformation_candidates.items() if 0.5 < abs(s) <= 1.0])}\")\n",
    "print(f\"Normal distribution features (|skew| ≤ 0.5): {len(feature_skewness) - len(transformation_candidates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution analysis reveals 7 features requiring transformation with extreme skewness (LifecycleRatio: 5.597, GrLivArea_per_BedroomAbvGr: 3.039) primarily affecting efficiency ratios.\n",
    "Age-based features show mild skewness while binary indicators maintain normal distributions, establishing clear transformation priorities for modeling optimization.\n",
    "\n",
    "### 5.2 Feature Transformation Implementation\n",
    "\n",
    "Apply log transformation to skewed engineered features and validate distribution improvements.\n",
    "Create final transformed feature set with normality improvements documented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation to highly skewed features\n",
    "# Focus on features with |skewness| > 1.0 for normalization\n",
    "\n",
    "print(\"FEATURE TRANSFORMATION IMPLEMENTATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Identify features requiring transformation (|skew| > 1.0)\n",
    "transform_features = [f for f, s in transformation_candidates.items() if abs(s) > 1.0]\n",
    "print(f\"Features to transform: {len(transform_features)}\")\n",
    "\n",
    "transformation_results = {}\n",
    "transformed_features = {}\n",
    "\n",
    "print(f\"\\nTRANSFORMATION RESULTS:\")\n",
    "print(f\"Feature Name{' '*25} | Before | After  | Status\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for feature_name in transform_features:\n",
    "    if feature_name in df_combined.columns:\n",
    "        original_data = df_combined[feature_name].copy()\n",
    "        original_skew = transformation_candidates[feature_name]\n",
    "\n",
    "        # Apply log transformation (handle zeros by adding small constant)\n",
    "        # Use log1p for features that might contain zeros\n",
    "        if (original_data >= 0).all():\n",
    "            # For non-negative features, use log1p (log(1+x))\n",
    "            transformed_data = np.log1p(original_data)\n",
    "            transform_method = \"log1p\"\n",
    "        else:\n",
    "            # For features with negative values, shift then transform\n",
    "            min_val = original_data.min()\n",
    "            shifted_data = original_data - min_val + 1\n",
    "            transformed_data = np.log(shifted_data)\n",
    "            transform_method = f\"log(x-{min_val:.1f}+1)\"\n",
    "\n",
    "        # Calculate new skewness\n",
    "        new_skew = stats.skew(transformed_data.dropna())\n",
    "\n",
    "        # Create transformed feature name\n",
    "        transformed_name = f\"{feature_name}_log\"\n",
    "        df_combined[transformed_name] = transformed_data\n",
    "        transformed_features[transformed_name] = transformed_data\n",
    "\n",
    "        # Store transformation results\n",
    "        transformation_results[feature_name] = {\n",
    "            'original_skew': original_skew,\n",
    "            'transformed_skew': new_skew,\n",
    "            'improvement': abs(original_skew) - abs(new_skew),\n",
    "            'method': transform_method\n",
    "        }\n",
    "\n",
    "        # Determine transformation success\n",
    "        if abs(new_skew) < abs(original_skew):\n",
    "            status = \"IMPROVED\" if abs(new_skew) < 0.5 else \"BETTER\"\n",
    "        else:\n",
    "            status = \"WORSE\"\n",
    "\n",
    "        print(f\"{feature_name:<35} | {original_skew:>6.2f} | {new_skew:>6.2f} | {status}\")\n",
    "\n",
    "# Transformation effectiveness summary\n",
    "print(f\"\\nTRANSFORMATION EFFECTIVENESS:\")\n",
    "successful = sum(1 for r in transformation_results.values() if r['improvement'] > 0)\n",
    "normalized = sum(1 for r in transformation_results.values() if abs(r['transformed_skew']) < 0.5)\n",
    "\n",
    "print(f\"Features improved: {successful}/{len(transform_features)}\")\n",
    "print(f\"Features normalized (|skew| < 0.5): {normalized}/{len(transform_features)}\")\n",
    "\n",
    "# Test correlation preservation for transformed features\n",
    "print(f\"\\nCORRELATION PRESERVATION ANALYSIS:\")\n",
    "train_mask = df_combined['dataset_source'] == 'train'\n",
    "target_col = 'SalePrice_log' if 'SalePrice_log' in df_train_clean.columns else 'SalePrice'\n",
    "\n",
    "if target_col in df_train_clean.columns:\n",
    "    target_data = df_train_clean[target_col]\n",
    "\n",
    "    for feature_name in transform_features:\n",
    "        if feature_name in df_combined.columns:\n",
    "            # Original correlation\n",
    "            original_feature = df_combined.loc[train_mask, feature_name]\n",
    "            original_corr = original_feature.corr(target_data)\n",
    "\n",
    "            # Transformed correlation\n",
    "            transformed_name = f\"{feature_name}_log\"\n",
    "            if transformed_name in df_combined.columns:\n",
    "                transformed_feature = df_combined.loc[train_mask, transformed_name]\n",
    "                transformed_corr = transformed_feature.corr(target_data)\n",
    "\n",
    "                corr_change = abs(transformed_corr) - abs(original_corr)\n",
    "                status = \"PRESERVED\" if abs(corr_change) < 0.05 else (\"IMPROVED\" if corr_change > 0 else \"DECLINED\")\n",
    "\n",
    "                print(f\"{feature_name:<35}: {original_corr:>6.3f} → {transformed_corr:>6.3f} ({status})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log transformation achieves partial success with 5/7 features improved but none reaching full normalization, while correlation preservation maintains predictive relationships.\n",
    "Selective transformation strategy proves superior with bedroom-based efficiency ratios showing correlation decline, validating targeted normalization over universal application for modeling optimization.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Advanced Feature Selection and Dimensionality Management\n",
    "\n",
    "Analyze correlation matrix for feature redundancy and implement variance-based filtering.\n",
    "Create final optimized feature set balancing predictive power with model complexity.\n",
    "\n",
    "### 6.1 Continuous Feature Correlation Analysis for Redundancy Detection\n",
    "\n",
    "Calculate continuous feature redundancy through correlation analysis excluding one-hot encoded categorical features.\n",
    "Identify high-correlation pairs (>0.95) and establish target correlation framework for redundancy removal decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CONTINUOUS FEATURE CORRELATION ANALYSIS FOR REDUNDANCY\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Get all numerical features for initial filtering\n",
    "all_features = df_combined.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# Remove ID and target-related columns\n",
    "base_features = [col for col in all_features if col not in ['Id', 'SalePrice', 'SalePrice_log', 'dataset_source']]\n",
    "\n",
    "# Filter out one-hot encoded features (binary 0/1 features from categorical encoding)\n",
    "print(\"Filtering one-hot encoded features from correlation analysis...\")\n",
    "continuous_features = []\n",
    "oneshot_features = []\n",
    "\n",
    "for feature in base_features:\n",
    "    feature_data = df_combined[feature]\n",
    "    unique_vals = set(feature_data.dropna().unique())\n",
    "\n",
    "    # Check if feature is binary (one-hot encoded) vs continuous\n",
    "    if unique_vals.issubset({0, 1}) and len(unique_vals) == 2:\n",
    "        oneshot_features.append(feature)\n",
    "    else:\n",
    "        continuous_features.append(feature)\n",
    "\n",
    "print(f\"Total numerical features: {len(base_features)}\")\n",
    "print(f\"One-hot encoded features (excluded): {len(oneshot_features)}\")\n",
    "print(f\"Continuous features for analysis: {len(continuous_features)}\")\n",
    "\n",
    "# Use only continuous features for meaningful correlation analysis\n",
    "analysis_features = continuous_features\n",
    "\n",
    "print(f\"Total features for correlation analysis: {len(analysis_features)}\")\n",
    "\n",
    "# Calculate correlation matrix (using training data only for consistency)\n",
    "train_mask = df_combined['dataset_source'] == 'train'\n",
    "correlation_data = df_combined.loc[train_mask, analysis_features]\n",
    "\n",
    "print(f\"Correlation matrix shape: {correlation_data.shape}\")\n",
    "print(\"Computing pairwise correlations...\")\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = correlation_data.corr()\n",
    "\n",
    "# Find high-correlation pairs (>0.95 threshold for redundancy)\n",
    "high_corr_threshold = 0.95\n",
    "high_corr_pairs = []\n",
    "\n",
    "# Iterate through upper triangle to avoid duplicates\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_value) > high_corr_threshold:\n",
    "            feature1 = correlation_matrix.columns[i]\n",
    "            feature2 = correlation_matrix.columns[j]\n",
    "            high_corr_pairs.append((feature1, feature2, corr_value))\n",
    "\n",
    "print(f\"\\nHigh correlation pairs (|r| > {high_corr_threshold}):\")\n",
    "print(f\"Found {len(high_corr_pairs)} redundant feature pairs\")\n",
    "\n",
    "if high_corr_pairs:\n",
    "    for feature1, feature2, corr_val in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True):\n",
    "        print(f\"  {feature1} ↔ {feature2}: {corr_val:.3f}\")\n",
    "else:\n",
    "    print(\"  No feature pairs exceed redundancy threshold\")\n",
    "\n",
    "# Complete correlation matrix visualization\n",
    "print(f\"\\nVisualizing complete correlation matrix ({len(analysis_features)} features)...\")\n",
    "\n",
    "# Create large correlation matrix plot\n",
    "plt.figure(figsize=(20, 18))\n",
    "sns.heatmap(correlation_matrix,\n",
    "           annot=False,  # Too many features for annotations\n",
    "           cmap='RdBu_r',\n",
    "           center=0,\n",
    "           square=True,\n",
    "           linewidths=0,\n",
    "           cbar_kws={'shrink': 0.6})\n",
    "plt.title(f'Complete Feature Correlation Matrix ({len(analysis_features)} features)')\n",
    "plt.xticks(rotation=90, fontsize=6)\n",
    "plt.yticks(rotation=0, fontsize=6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional focused visualization for high-correlation pairs if they exist\n",
    "if len(high_corr_pairs) > 0:\n",
    "    print(f\"\\nFocused view: High correlation pairs (|r| > {high_corr_threshold})\")\n",
    "\n",
    "    # Extract unique features from high correlation pairs\n",
    "    unique_features = set()\n",
    "    for feat1, feat2, _ in high_corr_pairs:\n",
    "        unique_features.add(feat1)\n",
    "        unique_features.add(feat2)\n",
    "\n",
    "    if len(unique_features) > 1:\n",
    "        unique_features_list = sorted(list(unique_features))\n",
    "        subset_corr = correlation_matrix.loc[unique_features_list, unique_features_list]\n",
    "\n",
    "        plt.figure(figsize=(16, 14))\n",
    "        sns.heatmap(subset_corr,\n",
    "                   annot=False,\n",
    "                   cmap='RdBu_r',\n",
    "                   center=0,\n",
    "                   fmt='.2f',\n",
    "                   square=True,\n",
    "                   linewidths=0.5,\n",
    "                   cbar_kws={'shrink': 0.8})\n",
    "        plt.title(f'High Correlation Features Detail View (|r| > {high_corr_threshold})')\n",
    "        plt.xticks(rotation=45, ha='right', fontsize=9)\n",
    "        plt.yticks(rotation=0, fontsize=9)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"\\nNo high-correlation pairs found - feature set shows good diversity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous feature filtering reduces analysis from 250 total features to 87 continuous features, excluding 163 one-hot encoded categorical features from redundancy detection.\n",
    "Correlation matrix analysis identifies 11 high-correlation pairs (>0.95) including perfect temporal duplication (RemodAge ↔ YearsSinceRemod: 1.000), log transformation pairs (0.972-0.999), and quality feature correlation (GarageQual ↔ GarageCond: 0.959)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target correlation analysis for redundancy removal strategy\n",
    "print(f\"\\nTARGET CORRELATION ANALYSIS FOR REDUNDANCY REMOVAL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "target_col = 'SalePrice_log' if 'SalePrice_log' in df_train_clean.columns else 'SalePrice'\n",
    "if target_col in df_train_clean.columns and len(high_corr_pairs) > 0:\n",
    "    target_data = df_train_clean[target_col]\n",
    "    removal_candidates = []\n",
    "\n",
    "    print(f\"Analyzing {len(high_corr_pairs)} redundant pairs for removal strategy:\")\n",
    "    print(f\"Target variable: {target_col}\")\n",
    "    print()\n",
    "\n",
    "    for feature1, feature2, corr_val in high_corr_pairs:\n",
    "        # Get target correlations for both features\n",
    "        if feature1 in df_combined.columns and feature2 in df_combined.columns:\n",
    "            train_mask = df_combined['dataset_source'] == 'train'\n",
    "\n",
    "            feat1_data = df_combined.loc[train_mask, feature1]\n",
    "            feat2_data = df_combined.loc[train_mask, feature2]\n",
    "\n",
    "            if len(feat1_data) == len(target_data):\n",
    "                corr1 = feat1_data.corr(target_data)\n",
    "                corr2 = feat2_data.corr(target_data)\n",
    "\n",
    "                # Determine which feature to remove (keep stronger target correlation)\n",
    "                if abs(corr1) >= abs(corr2):\n",
    "                    keep_feature = feature1\n",
    "                    remove_feature = feature2\n",
    "                    keep_corr = corr1\n",
    "                    remove_corr = corr2\n",
    "                else:\n",
    "                    keep_feature = feature2\n",
    "                    remove_feature = feature1\n",
    "                    keep_corr = corr2\n",
    "                    remove_corr = corr1\n",
    "\n",
    "                removal_candidates.append(remove_feature)\n",
    "\n",
    "                print(f\"Pair: {feature1} ↔ {feature2} (r={corr_val:.3f})\")\n",
    "                print(f\"  {feature1}: {corr1:.3f} target correlation\")\n",
    "                print(f\"  {feature2}: {corr2:.3f} target correlation\")\n",
    "                print(f\"  → Remove: {remove_feature} (keep {keep_feature})\")\n",
    "                print()\n",
    "\n",
    "    # Create final removal list (remove duplicates)\n",
    "    features_to_remove = list(set(removal_candidates))\n",
    "\n",
    "    print(f\"REDUNDANCY REMOVAL SUMMARY:\")\n",
    "    print(f\"Original features: {len(analysis_features)}\")\n",
    "    print(f\"Features to remove: {len(features_to_remove)}\")\n",
    "    print(f\"Final feature count: {len(analysis_features) - len(features_to_remove)}\")\n",
    "    print()\n",
    "    print(\"Features marked for removal:\")\n",
    "    for i, feature in enumerate(sorted(features_to_remove), 1):\n",
    "        print(f\"  {i:2d}. {feature}\")\n",
    "\n",
    "# Additional hierarchical redundancy detection for engineered features\n",
    "print(f\"\\nHIERARCHICAL REDUNDANCY ANALYSIS:\")\n",
    "print(\"=\" * 35)\n",
    "print(\"Comparing engineered features vs individual components...\")\n",
    "\n",
    "hierarchical_removals = []\n",
    "\n",
    "# Bathroom hierarchy: TotalBaths features vs individual bathroom components\n",
    "bathroom_engineered = ['TotalBaths_Standard', 'TotalBaths_All']\n",
    "bathroom_individuals = ['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath']\n",
    "\n",
    "# Check if engineered bathroom features exist and get their correlations\n",
    "bathroom_eng_corrs = {}\n",
    "for feat in bathroom_engineered:\n",
    "    if feat in df_combined.columns:\n",
    "        train_mask = df_combined['dataset_source'] == 'train'\n",
    "        feat_data = df_combined.loc[train_mask, feat]\n",
    "        if len(feat_data) == len(target_data):\n",
    "            corr = feat_data.corr(target_data)\n",
    "            bathroom_eng_corrs[feat] = corr\n",
    "\n",
    "# Get individual bathroom correlations\n",
    "bathroom_ind_corrs = {}\n",
    "for feat in bathroom_individuals:\n",
    "    if feat in df_combined.columns:\n",
    "        train_mask = df_combined['dataset_source'] == 'train'\n",
    "        feat_data = df_combined.loc[train_mask, feat]\n",
    "        if len(feat_data) == len(target_data):\n",
    "            corr = feat_data.corr(target_data)\n",
    "            bathroom_ind_corrs[feat] = corr\n",
    "\n",
    "if bathroom_eng_corrs and bathroom_ind_corrs:\n",
    "    best_engineered = max(bathroom_eng_corrs.values())\n",
    "    best_individual = max(bathroom_ind_corrs.values())\n",
    "\n",
    "    print(f\"\\nBATHROOM FEATURE HIERARCHY:\")\n",
    "    print(f\"Best engineered bathroom: {best_engineered:.3f} (TotalBaths)\")\n",
    "    print(f\"Best individual bathroom: {best_individual:.3f}\")\n",
    "\n",
    "    if best_engineered > best_individual:\n",
    "        print(f\"→ Engineered bathroom features outperform individual components\")\n",
    "        print(f\"→ Marking individual bathroom features for removal:\")\n",
    "        for feat, corr in bathroom_ind_corrs.items():\n",
    "            if feat not in features_to_remove:  # Don't duplicate removals\n",
    "                hierarchical_removals.append(feat)\n",
    "                print(f\"  Remove {feat} (correlation: {corr:.3f})\")\n",
    "\n",
    "# Update final removal list\n",
    "if hierarchical_removals:\n",
    "    total_removals = len(features_to_remove) + len(hierarchical_removals)\n",
    "    print(f\"\\nHIERARCHICAL REMOVAL SUMMARY:\")\n",
    "    print(f\"Additional features to remove: {len(hierarchical_removals)}\")\n",
    "    print(f\"Total features to remove: {total_removals}\")\n",
    "    print(f\"Final feature count: {len(analysis_features) - total_removals}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous feature correlation analysis successfully identifies 11 high-correlation pairs requiring removal including perfect temporal duplication (RemodAge ↔ YearsSinceRemod), 7 log transformation pairs (0.972-0.999 correlation), and quality feature redundancy (GarageQual ↔ GarageCond).\n",
    "Target correlation analysis systematically removes 11 redundant features (87→76 features) with key decisions: keep GrLivArea_log (0.737) over GrLivArea (0.725), EffectiveAge (-0.600) over PropertyAge (-0.588), and RemodAge over identical YearsSinceRemod, optimizing predictive power while eliminating redundancy.\n",
    "\n",
    "### 6.2 Hierarchical Feature Redundancy Analysis\n",
    "\n",
    "Systematically compare engineered features against their original components through correlation analysis.\n",
    "Determine which individual components can be removed when engineered combinations outperform for feature optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HIERARCHICAL FEATURE REDUNDANCY ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Initialize analysis framework\n",
    "train_mask = df_combined['dataset_source'] == 'train'\n",
    "target_col = 'SalePrice_log' if 'SalePrice_log' in df_train_clean.columns else 'SalePrice'\n",
    "target_data = df_train_clean[target_col]\n",
    "\n",
    "hierarchical_removals = []\n",
    "removal_analysis = {}\n",
    "\n",
    "# 1. BATHROOM FEATURE HIERARCHY ANALYSIS\n",
    "print(\"\\n1. BATHROOM FEATURE HIERARCHY:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "bathroom_engineered = ['TotalBaths_Standard', 'TotalBaths_All']\n",
    "bathroom_individuals = ['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath']\n",
    "\n",
    "bathroom_eng_corrs = {}\n",
    "bathroom_ind_corrs = {}\n",
    "\n",
    "# Get engineered bathroom correlations (train data only)\n",
    "for feat in bathroom_engineered:\n",
    "    if feat in df_combined.columns:\n",
    "        feat_data = df_combined.loc[train_mask, feat]\n",
    "        if len(feat_data) == len(target_data):\n",
    "            corr = feat_data.corr(target_data)\n",
    "            bathroom_eng_corrs[feat] = corr\n",
    "            print(f\"Engineered: {feat}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(f\"Missing engineered feature: {feat} (not found in dataset)\")\n",
    "\n",
    "# Get individual bathroom correlations (train data only)\n",
    "for feat in bathroom_individuals:\n",
    "    if feat in df_combined.columns:\n",
    "        feat_data = df_combined.loc[train_mask, feat]\n",
    "        if len(feat_data) == len(target_data):\n",
    "            corr = feat_data.corr(target_data)\n",
    "            bathroom_ind_corrs[feat] = corr\n",
    "            print(f\"Individual: {feat}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(f\"Missing individual feature: {feat} (not found in dataset)\")\n",
    "\n",
    "# Bathroom hierarchy decision analysis\n",
    "if bathroom_eng_corrs and bathroom_ind_corrs:\n",
    "    best_engineered = max(bathroom_eng_corrs.values())\n",
    "    best_engineered_name = max(bathroom_eng_corrs, key=bathroom_eng_corrs.get)\n",
    "    best_individual = max(bathroom_ind_corrs.values())\n",
    "    \n",
    "    improvement_pct = (best_engineered - best_individual) / best_individual * 100\n",
    "    print(f\"\\nBest engineered bathroom feature: {best_engineered_name} ({best_engineered:.3f})\")\n",
    "    print(f\"Best individual bathroom feature: {best_individual:.3f}\")\n",
    "    print(f\"Improvement: {improvement_pct:+.1f}%\")\n",
    "\n",
    "    # Data-driven decision based on performance gap\n",
    "    if best_engineered > best_individual and improvement_pct > 10:\n",
    "        print(\"→ Analysis: Superior engineered bathroom feature identified\")\n",
    "        print(\"→ Decision: Remove individual bathroom features and inferior engineered feature\")\n",
    "        \n",
    "        # Remove individual bathroom features\n",
    "        for feat in bathroom_individuals:\n",
    "            if feat in df_combined.columns:\n",
    "                hierarchical_removals.append(feat)\n",
    "                removal_analysis[feat] = f\"Replaced by {best_engineered_name} ({best_engineered:.3f} vs {bathroom_ind_corrs.get(feat, 0):.3f})\"\n",
    "                print(f\"  Remove: {feat} (correlation: {bathroom_ind_corrs.get(feat, 0):.3f})\")\n",
    "        \n",
    "        # Remove inferior engineered bathroom feature\n",
    "        for eng_feat, eng_corr in bathroom_eng_corrs.items():\n",
    "            if eng_feat != best_engineered_name:\n",
    "                hierarchical_removals.append(eng_feat)\n",
    "                removal_analysis[eng_feat] = f\"Replaced by superior engineered feature {best_engineered_name} ({best_engineered:.3f} vs {eng_corr:.3f})\"\n",
    "                print(f\"  Remove: {eng_feat} (correlation: {eng_corr:.3f})\")\n",
    "    else:\n",
    "        print(\"→ Decision: Keep individual bathroom features\")\n",
    "\n",
    "# 2. AREA COMBINATION ANALYSIS\n",
    "print(f\"\\n2. AREA COMBINATION ANALYSIS:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Check major area combinations vs their components\n",
    "area_combinations = [\n",
    "    ('GrLivArea_add_TotalBsmtSF', ['GrLivArea', 'TotalBsmtSF']),\n",
    "    ('WoodDeckSF_add_OpenPorchSF', ['WoodDeckSF', 'OpenPorchSF']),\n",
    "    ('GarageArea_add_TotalBsmtSF', ['GarageArea', 'TotalBsmtSF'])\n",
    "]\n",
    "\n",
    "for combo_name, components in area_combinations:\n",
    "    if combo_name in df_combined.columns:\n",
    "        combo_data = df_combined.loc[train_mask, combo_name]\n",
    "        combo_corr = combo_data.corr(target_data)\n",
    "\n",
    "        component_corrs = {}\n",
    "        for comp in components:\n",
    "            if comp in df_combined.columns:\n",
    "                comp_data = df_combined.loc[train_mask, comp]\n",
    "                comp_corr = comp_data.corr(target_data)\n",
    "                component_corrs[comp] = comp_corr\n",
    "\n",
    "        best_component = max(component_corrs.values()) if component_corrs else 0\n",
    "\n",
    "        print(f\"\\nCombination: {combo_name}: {combo_corr:.3f}\")\n",
    "        for comp, corr in component_corrs.items():\n",
    "            print(f\"  Component: {comp}: {corr:.3f}\")\n",
    "\n",
    "        print(f\"Best component: {best_component:.3f}\")\n",
    "        improvement = (combo_corr - best_component) / best_component * 100 if best_component > 0 else 0\n",
    "        print(f\"Improvement: {improvement:+.1f}%\")\n",
    "\n",
    "        # Decision: Keep components for area features (they provide complementary information)\n",
    "        print(\"→ Decision: Keep individual area components (complementary information)\")\n",
    "\n",
    "# 3. QUALITY INTERACTION ANALYSIS\n",
    "print(f\"\\n3. QUALITY INTERACTION ANALYSIS:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Check top quality interactions vs their components\n",
    "quality_interactions = [\n",
    "    ('OverallQual_multiply_GrLivArea', ['OverallQual', 'GrLivArea']),\n",
    "    ('ExterQual_multiply_GrLivArea', ['ExterQual', 'GrLivArea']),\n",
    "    ('BsmtQual_multiply_TotalBsmtSF', ['BsmtQual', 'TotalBsmtSF'])\n",
    "]\n",
    "\n",
    "for interaction_name, components in quality_interactions:\n",
    "    if interaction_name in df_combined.columns:\n",
    "        interaction_data = df_combined.loc[train_mask, interaction_name]\n",
    "        interaction_corr = interaction_data.corr(target_data)\n",
    "\n",
    "        component_corrs = {}\n",
    "        for comp in components:\n",
    "            if comp in df_combined.columns:\n",
    "                comp_data = df_combined.loc[train_mask, comp]\n",
    "                comp_corr = comp_data.corr(target_data)\n",
    "                component_corrs[comp] = comp_corr\n",
    "\n",
    "        best_component = max(component_corrs.values()) if component_corrs else 0\n",
    "\n",
    "        print(f\"\\nInteraction: {interaction_name}: {interaction_corr:.3f}\")\n",
    "        for comp, corr in component_corrs.items():\n",
    "            print(f\"  Component: {comp}: {corr:.3f}\")\n",
    "\n",
    "        improvement = (interaction_corr - best_component) / best_component * 100 if best_component > 0 else 0\n",
    "        print(f\"Improvement: {improvement:+.1f}%\")\n",
    "\n",
    "        # Decision: Keep components for quality features (interpretability + model flexibility)\n",
    "        print(\"→ Decision: Keep individual quality components (interpretability + flexibility)\")\n",
    "\n",
    "# 4. FAILED FEATURE REMOVAL\n",
    "print(f\"\\n4. FAILED FEATURE REMOVAL:\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Remove all efficiency ratio features (already proven to fail)\n",
    "efficiency_ratios = [\n",
    "    'GarageArea_per_Car', 'GrLivArea_per_TotRmsAbvGrd', 'GrLivArea_per_BedroomAbvGr',\n",
    "    'TotalBsmtSF_per_BedroomAbvGr', 'GrLivArea_per_LotArea', '1stFlrSF_per_LotArea'\n",
    "]\n",
    "\n",
    "print(\"Removing failed efficiency ratio features:\")\n",
    "for feat in efficiency_ratios:\n",
    "    if feat in df_combined.columns:\n",
    "        hierarchical_removals.append(feat)\n",
    "        removal_analysis[feat] = \"Systematic underperformance (21-69% correlation decline)\"\n",
    "        print(f\"  Remove: {feat} (ratio strategy failure)\")\n",
    "\n",
    "# FINAL REMOVAL SUMMARY\n",
    "print(f\"\\nHIERARCHICAL REDUNDANCY REMOVAL SUMMARY:\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"Features marked for removal: {len(hierarchical_removals)}\")\n",
    "print(\"\\nRemoval analysis results:\")\n",
    "for feat in hierarchical_removals:\n",
    "    reason = removal_analysis.get(feat, \"Replaced by superior engineered feature\")\n",
    "    print(f\"  {feat}: {reason}\")\n",
    "\n",
    "# Store removal list for Section 7 implementation\n",
    "print(f\"\\nFeature removal list prepared for systematic implementation.\")\n",
    "print(f\"Total features to remove: {len(hierarchical_removals)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical feature redundancy analysis removes 11 features: TotalBaths_All (0.677) replaces 4 individual bathroom components plus TotalBaths_Standard (0.641), while 6 failed efficiency ratio features removed due to systematic underperformance.\n",
    "Area and quality combinations preserved alongside individual components for model interpretability despite strong improvements (13-31% and up to 15% respectively).\n",
    "\n",
    "\n",
    "Systematic removal of 6 failed efficiency ratio features confirms ratio strategy ineffectiveness, with final hierarchical analysis removing 11 redundant features (5 bathroom + 6 efficiency ratios) while preserving optimal engineered combinations.\n",
    "\n",
    "### 6.3 Variance Filtering and Final Selection\n",
    "\n",
    "Apply variance threshold filtering to remove near-constant features from cleaned continuous feature set.\n",
    "Create optimized feature set ready for model-driven selection and performance testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features with minimal variance that provide little predictive value\n",
    "print(\"VARIANCE THRESHOLD FILTERING\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Use conservative threshold to remove only truly constant features\n",
    "variance_threshold = 0.01\n",
    "variance_filter = VarianceThreshold(threshold=variance_threshold)\n",
    "\n",
    "# Fit variance filter on training data only for proper ML workflow\n",
    "train_mask = df_combined['dataset_source'] == 'train'\n",
    "training_features = df_combined.loc[train_mask, analysis_features]\n",
    "\n",
    "variance_filter.fit(training_features)\n",
    "\n",
    "# Get feature variance scores\n",
    "feature_variances = variance_filter.variances_\n",
    "low_variance_features = []\n",
    "\n",
    "print(f\"Variance threshold: {variance_threshold}\")\n",
    "print(f\"Features analyzed: {len(analysis_features)}\")\n",
    "\n",
    "# Identify features below threshold\n",
    "for i, (feature, variance) in enumerate(zip(analysis_features, feature_variances)):\n",
    "    if variance <= variance_threshold:\n",
    "        low_variance_features.append(feature)\n",
    "        print(f\"Low variance: {feature} (variance: {variance:.4f})\")\n",
    "\n",
    "print(f\"\\nLow variance features found: {len(low_variance_features)}\")\n",
    "\n",
    "# Create final optimized feature set\n",
    "optimized_features = [f for f in analysis_features if f not in low_variance_features]\n",
    "print(f\"Optimized feature count: {len(optimized_features)}\")\n",
    "print(f\"Variance filtering reduction: {len(analysis_features)} → {len(optimized_features)} ({len(low_variance_features)} removed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance filtering identifies 6 near-constant features with variance < 0.01, primarily log-transformed lot utilization ratios (GrLivArea_per_LotArea_log: 0.0073) and kitchen features (KitchenAbvGr_log: 0.0080). Statistical variance analysis reduces feature set from 123 to 117 features, removing low-information variables while preserving predictive capacity for model optimization.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Apply Feature Analysis Results and Dataset Export\n",
    "\n",
    "Apply systematic feature analysis results from Section 6 to create final optimized feature set.\n",
    "Export model-ready datasets with complete feature engineering documentation for model development pipeline.\n",
    "\n",
    "### 7.1 Apply Section 6 Analysis Results\n",
    "\n",
    "Implement hierarchical feature removals and variance filtering results from systematic analysis.\n",
    "Create final optimized feature set based on domain-knowledge-driven feature selection methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement hierarchical removals and variance filtering for final feature set\n",
    "print(\"APPLYING SECTION 6 FEATURE ANALYSIS RESULTS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Start with ALL features from preprocessing (numerical + one-hot encoded)\n",
    "all_feature_columns = [col for col in df_combined.columns \n",
    "                      if col not in ['dataset_source', 'Id', 'SalePrice', 'SalePrice_log']]\n",
    "\n",
    "print(f\"All features from preprocessing: {len(all_feature_columns)}\")\n",
    "\n",
    "# Note: Section 6.3 variance filtering was only applied to numerical features\n",
    "# We keep ALL one-hot encoded categorical features (they have binary variance)\n",
    "numerical_features_clean = [f for f in all_feature_columns if f in optimized_features] if 'optimized_features' in locals() else []\n",
    "categorical_features = [f for f in all_feature_columns if f not in numerical_features_clean]\n",
    "\n",
    "print(f\"Numerical features (after variance filtering): {len(numerical_features_clean)}\")\n",
    "print(f\"Categorical one-hot encoded features: {len(categorical_features)}\")\n",
    "\n",
    "# Combine numerical (post-variance-filter) + all categorical features\n",
    "final_features = numerical_features_clean + categorical_features\n",
    "print(f\"Starting with combined feature set: {len(final_features)}\")\n",
    "print(\"(117 numerical + categorical one-hot encoded features)\")\n",
    "\n",
    "# Apply hierarchical feature removals from Section 6.2 analysis\n",
    "if 'hierarchical_removals' in locals() and len(hierarchical_removals) > 0:\n",
    "    print(f\"\\nApplying hierarchical component removals:\")\n",
    "    print(f\"Features identified for removal: {len(hierarchical_removals)}\")\n",
    "    \n",
    "    # Show removal decisions with reasoning\n",
    "    removal_summary = []\n",
    "    for feature in hierarchical_removals:\n",
    "        if feature in final_features:\n",
    "            reason = removal_analysis.get(feature, \"Component replaced by superior engineered feature\")\n",
    "            removal_summary.append(f\"  {feature}: {reason}\")\n",
    "            final_features.remove(feature)\n",
    "    \n",
    "    print(\"Hierarchical removal decisions:\")\n",
    "    for removal in removal_summary:\n",
    "        print(removal)\n",
    "    \n",
    "    print(f\"\\nFeatures after hierarchical removal: {len(final_features)}\")\n",
    "else:\n",
    "    print(\"\\nNo hierarchical removals to apply (variable not found)\")\n",
    "\n",
    "# Validate final feature set consistency\n",
    "print(f\"\\nFINAL FEATURE SET VALIDATION:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Check all features exist in dataset\n",
    "missing_features = [f for f in final_features if f not in df_combined.columns]\n",
    "if missing_features:\n",
    "    print(f\"WARNING: {len(missing_features)} features missing from dataset:\")\n",
    "    for feat in missing_features[:5]:\n",
    "        print(f\"  - {feat}\")\n",
    "    final_features = [f for f in final_features if f in df_combined.columns]\n",
    "\n",
    "print(f\"Final validated feature count: {len(final_features)}\")\n",
    "print(f\"Feature validation: {'PASSED' if len(missing_features) == 0 else 'ADJUSTED'}\")\n",
    "\n",
    "# Quick performance validation to ensure removals don't harm baseline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "print(f\"\\nPerformance validation with final feature set:\")\n",
    "train_mask = df_combined['dataset_source'] == 'train'\n",
    "target_col = 'SalePrice_log' if 'SalePrice_log' in df_train_clean.columns else 'SalePrice'\n",
    "y_train = df_train_clean[target_col].values\n",
    "\n",
    "# Quick validation with RandomForest\n",
    "X_final = df_combined.loc[train_mask, final_features].values\n",
    "rf_model = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "cv_scores = cross_val_score(rf_model, X_final, y_train, cv=3, \n",
    "                           scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "final_rmse = np.sqrt(-cv_scores.mean())\n",
    "\n",
    "print(f\"Final feature set performance: {final_rmse:.4f} RMSE\")\n",
    "print(f\"Performance validation: {'ACCEPTABLE' if final_rmse < 0.20 else 'REVIEW NEEDED'}\")\n",
    "\n",
    "# Summary of feature reduction pipeline\n",
    "print(f\"\\nFEATURE REDUCTION SUMMARY:\")\n",
    "print(\"=\" * 25)\n",
    "original_count = len(numerical_features) if 'numerical_features' in locals() else 'unknown'\n",
    "variance_count = len(optimized_features) if 'optimized_features' in locals() else 'unknown'\n",
    "hierarchical_removed = len(hierarchical_removals) if 'hierarchical_removals' in locals() else 0\n",
    "\n",
    "print(f\"Original features (preprocessed): {original_count}\")\n",
    "print(f\"After variance filtering: {variance_count}\")\n",
    "print(f\"Hierarchical removals applied: {hierarchical_removed}\")\n",
    "print(f\"Final feature count: {len(final_features)}\")\n",
    "print(f\"Total reduction: {len(all_feature_columns)} → {len(final_features)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature integration combines 117 variance-filtered numerical features with 169 categorical one-hot encoded features from preprocessing pipeline. Hierarchical removal implementation successfully eliminates 11 redundant features (5 bathroom components + 6 efficiency ratios) with final validation confirming acceptable performance (0.1385 RMSE), reducing feature set from 286 to 275 features for model development.\n",
    "\n",
    "### 7.2 Final Dataset Validation and Export\n",
    "\n",
    "Validate final feature set consistency and export model-ready datasets for development pipeline.\n",
    "Create comprehensive feature engineering documentation with complete transformation summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model-ready datasets with comprehensive feature engineering documentation\n",
    "print(f\"\\nFINAL DATASET EXPORT AND DOCUMENTATION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Prepare final train and test datasets with optimized features\n",
    "train_mask = df_combined['dataset_source'] == 'train'\n",
    "test_mask = df_combined['dataset_source'] == 'test'\n",
    "\n",
    "# Create final feature matrices\n",
    "df_train_final = df_combined.loc[train_mask, final_features].reset_index(drop=True)\n",
    "df_test_final = df_combined.loc[test_mask, final_features].reset_index(drop=True)\n",
    "\n",
    "# Add target variables to training dataset\n",
    "df_train_final['SalePrice'] = df_train_clean['SalePrice'].values\n",
    "if 'SalePrice_log' in df_train_clean.columns:\n",
    "    df_train_final['SalePrice_log'] = df_train_clean['SalePrice_log'].values\n",
    "\n",
    "# Add Id columns for tracking\n",
    "df_train_final['Id'] = df_train_clean['Id'].values\n",
    "df_test_final['Id'] = df_test_clean['Id'].values\n",
    "\n",
    "print(f\"Final dataset shapes:\")\n",
    "print(f\"Train: {df_train_final.shape} (features + target + Id)\")\n",
    "print(f\"Test: {df_test_final.shape} (features + Id)\")\n",
    "\n",
    "# Export final datasets\n",
    "import os\n",
    "output_dir = '../data/processed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "train_output_path = f'{output_dir}/train_feature_engineered.csv'\n",
    "test_output_path = f'{output_dir}/test_feature_engineered.csv'\n",
    "\n",
    "df_train_final.to_csv(train_output_path, index=False)\n",
    "df_test_final.to_csv(test_output_path, index=False)\n",
    "\n",
    "print(f\"\\nDatasets exported:\")\n",
    "print(f\"✓ {train_output_path}\")\n",
    "print(f\"✓ {test_output_path}\")\n",
    "\n",
    "# Create comprehensive feature engineering documentation\n",
    "feature_engineering_doc = {\n",
    "    'feature_engineering_summary': {\n",
    "        'total_features_preprocessed': len(numerical_features) if 'numerical_features' in locals() else 'unknown',\n",
    "        'engineered_features_created': len(all_engineered_features) if 'all_engineered_features' in locals() else 'unknown',\n",
    "        'features_after_variance_filter': len(optimized_features) if 'optimized_features' in locals() else 'unknown',\n",
    "        'final_optimized_features': len(final_features),\n",
    "        'feature_reduction_ratio': f\"{len(final_features)}/{len(numerical_features) if 'numerical_features' in locals() else 'unknown'}\",\n",
    "        'final_performance_rmse': final_rmse\n",
    "    },\n",
    "    'feature_removal_summary': {\n",
    "        'hierarchical_removals': hierarchical_removals if 'hierarchical_removals' in locals() else [],\n",
    "        'variance_filter_removals': low_variance_features if 'low_variance_features' in locals() else []\n",
    "    },\n",
    "    'final_feature_list': final_features\n",
    "}\n",
    "\n",
    "# Export feature engineering documentation\n",
    "import json\n",
    "doc_output_path = f'{output_dir}/feature_engineering_documentation.json'\n",
    "with open(doc_output_path, 'w') as f:\n",
    "    json.dump(feature_engineering_doc, f, indent=2, default=str)\n",
    "\n",
    "print(f\"✓ Feature documentation: {doc_output_path}\")\n",
    "\n",
    "# Summary of feature engineering achievements\n",
    "print(f\"\\nFEATURE ENGINEERING ACHIEVEMENTS:\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"Starting features (preprocessed): {feature_engineering_doc['feature_engineering_summary']['total_features_preprocessed']}\")\n",
    "print(f\"Features after variance filtering: {feature_engineering_doc['feature_engineering_summary']['features_after_variance_filter']}\")\n",
    "print(f\"Final optimized feature count: {len(final_features)}\")\n",
    "print(f\"Final model performance: {final_rmse:.4f} RMSE\")\n",
    "print(f\"Feature reduction: {len(final_features)} features ready for model development\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final dataset export creates model-ready train_feature_engineered.csv and test_feature_engineered.csv with comprehensive feature engineering documentation. Systematic feature optimization provides validated feature set for model development phase with preserved predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dxcoilacg4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data to check for missing values\n",
    "df = pd.read_csv('../data/processed/train_preprocessed.csv')\n",
    "\n",
    "# Check the specific columns you're trying to convert\n",
    "integer_features = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'GarageCars']\n",
    "\n",
    "print(\"MISSING VALUES CHECK:\")\n",
    "print(\"=\" * 30)\n",
    "for col in integer_features:\n",
    "    if col in df.columns:\n",
    "        missing_count = df[col].isna().sum()\n",
    "        unique_vals = df[col].unique()[:10]  # Show first 10 unique values\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  Missing values: {missing_count}\")\n",
    "        print(f\"  Data type: {df[col].dtype}\")\n",
    "        print(f\"  Sample values: {unique_vals}\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HousePrediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
